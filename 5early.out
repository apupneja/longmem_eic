2023-10-16 23:16:14 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:16:14 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:16:18 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13561
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13561
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13561
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13561
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 1
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 2
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 3
2023-10-16 23:16:19 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-10-16 23:16:19 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 0
2023-10-16 23:16:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13561', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/data/zyu401_data/anirudh/longmem_data/train_ckpt/5early', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 10000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_sidenet_gpt2_small', 'activation_fn': gelu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 12, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': True, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'reduction_factor': 8, 'layer_reduction_factor': 2, 'reload_ptm_layer': True, 'tune_lm_head': False, 'gpt_encoder_path': 'gpt2_bpe', 'use_external_memory': True, 'retrieval_layer_index': 17, 'chunk_size': 4, 'dstore_fp16': False, 'use_gpu_to_search': True, 'move_dstore_to_mem': False, 'dstore_size': 10000000, 'k': 64, 'probe': 32, 'dstore_filename': 'data/datastore', 'long_context_attention': False, 'memory_size': 65536, 'precompute_mem_layer': 5, 'pretrained_model_path': '/data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt', 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': '/data/zyu401_data/anirudh/longmem_data/data-bin/longmem', 'sample_break_mode': none, 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'data_iteration': False, 'data_no_shuffle': True, 'seed': 42, 'batch_size': 1, 'batch_size_valid': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 100000.0, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-10-16 23:16:21 | INFO | fairseq.tasks.language_modeling | dictionary: 51200 types
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
2023-10-16 23:16:30 | INFO | fairseq.tasks.gpt | dictionary: 51200 types
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 0
put done
2023-10-16 23:16:38 | INFO | fairseq_cli.train | TransformerLanguageModelSideNet(
  (decoder): TransformerDecoderSideNet(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 1024, padding_idx=1)
    (layers): ModuleList(
      (0-7): 8 x TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): JointMultiheadAttentionWeightedSum(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9-11): 3 x TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (pretrained_model): NewGPTLanguageModel(
      (decoder): NewGPTDecoder(
        (model): NewGPTForCausalLM(
          (transformer): NewGPTModel(
            (wte): Embedding(51200, 1024)
            (drop): Dropout(p=0.1, inplace=False)
            (h): ModuleList(
              (0-23): 24 x NewGPTBlock(
                (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): NewGPTAttention(
                  (resid_dropout): Dropout(p=0.1, inplace=False)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (mlp): NewGPTMLP(
                  (fc_in): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc_out): Linear(in_features=4096, out_features=1024, bias=True)
                  (act): NewGELUActivation()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (ln_f): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (lm_head): Linear(in_features=1024, out_features=51200, bias=True)
        )
      )
    )
    (output_projection): Linear(in_features=1024, out_features=51200, bias=False)
  )
)
2023-10-16 23:16:38 | INFO | fairseq_cli.train | task: LanguageModelingTask
2023-10-16 23:16:38 | INFO | fairseq_cli.train | model: TransformerLanguageModelSideNet
2023-10-16 23:16:38 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2023-10-16 23:16:38 | INFO | fairseq_cli.train | num. shared model params: 558,155,792 (num. trained: 151,083,024)
2023-10-16 23:16:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-10-16 23:16:39 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-10-16 23:16:39 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.pretrained_model.decoder.model.transformer.wte.weight
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.q_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.k_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.v_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.out_proj.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.output_projection.bias
2023-10-16 23:16:39 | INFO | fairseq.trainer | detected shared parameter: decoder.pretrained_model.decoder.model.lm_head.weight <- decoder.output_projection.weight
2023-10-16 23:16:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-10-16 23:16:39 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-16 23:16:39 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-16 23:16:39 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-16 23:16:39 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-16 23:16:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-10-16 23:16:39 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2023-10-16 23:16:39 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 1
2023-10-16 23:16:39 | INFO | fairseq.trainer | Preparing to load checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_last.pt
2023-10-16 23:16:39 | INFO | fairseq.trainer | No existing checkpoint found /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_last.pt
2023-10-16 23:16:39 | INFO | fairseq.trainer | loading train data for epoch 1
2023-10-16 23:24:13 | INFO | fairseq.data.data_utils | loaded 1,681,792,185 examples from: /data/zyu401_data/anirudh/longmem_data/data-bin/longmem/train
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 2
put done
[29290284]
[29290285]
2023-10-16 23:30:05 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:30:05 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
[0]
[1]
2023-10-16 23:31:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1830643
2023-10-16 23:31:09 | INFO | fairseq.trainer | begin training epoch 1
2023-10-16 23:31:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-10-16 23:31:29 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:31:29 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 3
put done
[43935426]
[43935427]
2023-10-16 23:32:49 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:32:49 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 1
put done
[14645142]
[14645143]
2023-10-16 23:32:55 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-16 23:32:55 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-10-16 23:43:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-10-16 23:43:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-10-16 23:43:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-16 23:43:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-16 23:43:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-16 23:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-16 23:45:57 | INFO | train_inner | epoch 001:    106 / 1830643 loss=4.217, ppl=18.6, wps=23373.5, ups=0.71, wpb=32768, bsz=32, num_updates=100, lr=0.0001998, gnorm=0.528, loss_scale=2, train_wall=357, gb_free=15.4, wall=1758
2023-10-16 23:48:17 | INFO | train_inner | epoch 001:    206 / 1830643 loss=4.013, ppl=16.15, wps=23349.2, ups=0.71, wpb=32768, bsz=32, num_updates=200, lr=0.0001996, gnorm=0.289, loss_scale=2, train_wall=140, gb_free=15.4, wall=1898
2023-10-16 23:50:39 | INFO | train_inner | epoch 001:    306 / 1830643 loss=3.915, ppl=15.09, wps=23116.5, ups=0.71, wpb=32768, bsz=32, num_updates=300, lr=0.0001994, gnorm=0.295, loss_scale=2, train_wall=141, gb_free=15.4, wall=2040
2023-10-16 23:53:01 | INFO | train_inner | epoch 001:    406 / 1830643 loss=4.036, ppl=16.41, wps=23043.4, ups=0.7, wpb=32768, bsz=32, num_updates=400, lr=0.0001992, gnorm=0.284, loss_scale=2, train_wall=142, gb_free=15.4, wall=2182
2023-10-16 23:55:24 | INFO | train_inner | epoch 001:    506 / 1830643 loss=3.7, ppl=13, wps=22990.8, ups=0.7, wpb=32768, bsz=32, num_updates=500, lr=0.000199, gnorm=0.271, loss_scale=2, train_wall=142, gb_free=15.4, wall=2325
2023-10-16 23:57:47 | INFO | train_inner | epoch 001:    606 / 1830643 loss=3.711, ppl=13.1, wps=22908.2, ups=0.7, wpb=32768, bsz=32, num_updates=600, lr=0.0001988, gnorm=0.285, loss_scale=4, train_wall=143, gb_free=15.4, wall=2468
2023-10-17 00:00:10 | INFO | train_inner | epoch 001:    706 / 1830643 loss=3.696, ppl=12.96, wps=22907.5, ups=0.7, wpb=32768, bsz=32, num_updates=700, lr=0.0001986, gnorm=0.31, loss_scale=4, train_wall=143, gb_free=15.4, wall=2611
2023-10-17 00:02:31 | INFO | train_inner | epoch 001:    806 / 1830643 loss=3.654, ppl=12.59, wps=23098.8, ups=0.7, wpb=32768, bsz=32, num_updates=800, lr=0.0001984, gnorm=0.253, loss_scale=4, train_wall=141, gb_free=15.4, wall=2753
2023-10-17 00:04:53 | INFO | train_inner | epoch 001:    906 / 1830643 loss=3.74, ppl=13.36, wps=23100.7, ups=0.7, wpb=32768, bsz=32, num_updates=900, lr=0.0001982, gnorm=0.263, loss_scale=4, train_wall=141, gb_free=15.4, wall=2894
2023-10-17 00:07:15 | INFO | train_inner | epoch 001:   1006 / 1830643 loss=3.733, ppl=13.3, wps=23049.5, ups=0.7, wpb=32768, bsz=32, num_updates=1000, lr=0.000198, gnorm=0.272, loss_scale=4, train_wall=142, gb_free=15.4, wall=3037
2023-10-17 00:09:36 | INFO | train_inner | epoch 001:   1106 / 1830643 loss=3.592, ppl=12.06, wps=23369.1, ups=0.71, wpb=32768, bsz=32, num_updates=1100, lr=0.0001978, gnorm=0.282, loss_scale=8, train_wall=140, gb_free=15.4, wall=3177
2023-10-17 00:11:56 | INFO | train_inner | epoch 001:   1206 / 1830643 loss=3.761, ppl=13.56, wps=23398.6, ups=0.71, wpb=32768, bsz=32, num_updates=1200, lr=0.0001976, gnorm=0.232, loss_scale=8, train_wall=140, gb_free=15.4, wall=3317
2023-10-17 00:13:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 00:14:16 | INFO | train_inner | epoch 001:   1307 / 1830643 loss=3.806, ppl=13.99, wps=23353.2, ups=0.71, wpb=32768, bsz=32, num_updates=1300, lr=0.0001974, gnorm=0.272, loss_scale=4, train_wall=140, gb_free=15.4, wall=3457
2023-10-17 00:16:35 | INFO | train_inner | epoch 001:   1407 / 1830643 loss=3.851, ppl=14.43, wps=23645.2, ups=0.72, wpb=32768, bsz=32, num_updates=1400, lr=0.0001972, gnorm=0.235, loss_scale=4, train_wall=138, gb_free=15.4, wall=3596
2023-10-17 00:18:55 | INFO | train_inner | epoch 001:   1507 / 1830643 loss=3.427, ppl=10.76, wps=23335.8, ups=0.71, wpb=32768, bsz=32, num_updates=1500, lr=0.000197, gnorm=0.202, loss_scale=4, train_wall=140, gb_free=15.4, wall=3736
2023-10-17 00:21:14 | INFO | train_inner | epoch 001:   1607 / 1830643 loss=3.842, ppl=14.34, wps=23519, ups=0.72, wpb=32768, bsz=32, num_updates=1600, lr=0.0001968, gnorm=0.276, loss_scale=4, train_wall=139, gb_free=15.4, wall=3876
2023-10-17 00:23:35 | INFO | train_inner | epoch 001:   1707 / 1830643 loss=3.628, ppl=12.37, wps=23381, ups=0.71, wpb=32768, bsz=32, num_updates=1700, lr=0.0001966, gnorm=0.268, loss_scale=4, train_wall=140, gb_free=15.4, wall=4016
2023-10-17 00:25:54 | INFO | train_inner | epoch 001:   1807 / 1830643 loss=3.821, ppl=14.13, wps=23531.3, ups=0.72, wpb=32768, bsz=32, num_updates=1800, lr=0.0001964, gnorm=0.23, loss_scale=8, train_wall=139, gb_free=15.4, wall=4155
2023-10-17 00:28:13 | INFO | train_inner | epoch 001:   1907 / 1830643 loss=3.692, ppl=12.93, wps=23608.5, ups=0.72, wpb=32768, bsz=32, num_updates=1900, lr=0.0001962, gnorm=0.225, loss_scale=8, train_wall=138, gb_free=15.4, wall=4294
2023-10-17 00:30:32 | INFO | train_inner | epoch 001:   2007 / 1830643 loss=3.338, ppl=10.11, wps=23522, ups=0.72, wpb=32768, bsz=32, num_updates=2000, lr=0.000196, gnorm=0.234, loss_scale=8, train_wall=139, gb_free=15.4, wall=4433
2023-10-17 00:32:51 | INFO | train_inner | epoch 001:   2107 / 1830643 loss=3.674, ppl=12.76, wps=23548.9, ups=0.72, wpb=32768, bsz=32, num_updates=2100, lr=0.0001958, gnorm=0.22, loss_scale=8, train_wall=139, gb_free=15.4, wall=4572
2023-10-17 00:34:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 00:34:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 00:35:12 | INFO | train_inner | epoch 001:   2209 / 1830643 loss=3.644, ppl=12.5, wps=23222.9, ups=0.71, wpb=32768, bsz=32, num_updates=2200, lr=0.0001956, gnorm=0.218, loss_scale=2, train_wall=141, gb_free=15.4, wall=4713
2023-10-17 00:37:31 | INFO | train_inner | epoch 001:   2309 / 1830643 loss=3.571, ppl=11.88, wps=23564.6, ups=0.72, wpb=32768, bsz=32, num_updates=2300, lr=0.0001954, gnorm=0.256, loss_scale=2, train_wall=139, gb_free=15.4, wall=4852
2023-10-17 00:39:51 | INFO | train_inner | epoch 001:   2409 / 1830643 loss=3.736, ppl=13.32, wps=23385.3, ups=0.71, wpb=32768, bsz=32, num_updates=2400, lr=0.0001952, gnorm=0.228, loss_scale=2, train_wall=140, gb_free=15.4, wall=4992
2023-10-17 00:42:10 | INFO | train_inner | epoch 001:   2509 / 1830643 loss=3.602, ppl=12.14, wps=23603.3, ups=0.72, wpb=32768, bsz=32, num_updates=2500, lr=0.000195, gnorm=0.212, loss_scale=2, train_wall=138, gb_free=15.4, wall=5131
2023-10-17 00:44:30 | INFO | train_inner | epoch 001:   2609 / 1830643 loss=3.646, ppl=12.52, wps=23353.6, ups=0.71, wpb=32768, bsz=32, num_updates=2600, lr=0.0001948, gnorm=0.211, loss_scale=2, train_wall=140, gb_free=15.4, wall=5272
2023-10-17 00:46:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 00:46:51 | INFO | train_inner | epoch 001:   2710 / 1830643 loss=3.716, ppl=13.14, wps=23334.5, ups=0.71, wpb=32768, bsz=32, num_updates=2700, lr=0.0001946, gnorm=0.237, loss_scale=2, train_wall=140, gb_free=15.4, wall=5412
2023-10-17 00:49:11 | INFO | train_inner | epoch 001:   2810 / 1830643 loss=3.671, ppl=12.74, wps=23319.7, ups=0.71, wpb=32768, bsz=32, num_updates=2800, lr=0.0001944, gnorm=0.22, loss_scale=2, train_wall=140, gb_free=15.4, wall=5553
2023-10-17 00:51:29 | INFO | train_inner | epoch 001:   2910 / 1830643 loss=3.6, ppl=12.12, wps=23740.6, ups=0.72, wpb=32768, bsz=32, num_updates=2900, lr=0.0001942, gnorm=0.232, loss_scale=2, train_wall=138, gb_free=15.4, wall=5691
2023-10-17 00:53:48 | INFO | train_inner | epoch 001:   3010 / 1830643 loss=3.672, ppl=12.75, wps=23731.9, ups=0.72, wpb=32768, bsz=32, num_updates=3000, lr=0.000194, gnorm=0.248, loss_scale=2, train_wall=138, gb_free=15.4, wall=5829
2023-10-17 00:56:06 | INFO | train_inner | epoch 001:   3110 / 1830643 loss=3.608, ppl=12.2, wps=23649.9, ups=0.72, wpb=32768, bsz=32, num_updates=3100, lr=0.0001938, gnorm=0.202, loss_scale=2, train_wall=138, gb_free=15.4, wall=5967
2023-10-17 00:58:24 | INFO | train_inner | epoch 001:   3210 / 1830643 loss=3.655, ppl=12.6, wps=23809.5, ups=0.73, wpb=32768, bsz=32, num_updates=3200, lr=0.0001936, gnorm=0.207, loss_scale=4, train_wall=137, gb_free=15.4, wall=6105
2023-10-17 01:00:42 | INFO | train_inner | epoch 001:   3310 / 1830643 loss=3.549, ppl=11.71, wps=23711.9, ups=0.72, wpb=32768, bsz=32, num_updates=3300, lr=0.0001934, gnorm=0.224, loss_scale=4, train_wall=138, gb_free=15.4, wall=6243
2023-10-17 01:03:00 | INFO | train_inner | epoch 001:   3410 / 1830643 loss=3.685, ppl=12.86, wps=23737, ups=0.72, wpb=32768, bsz=32, num_updates=3400, lr=0.0001932, gnorm=0.212, loss_scale=4, train_wall=138, gb_free=15.4, wall=6381
2023-10-17 01:05:18 | INFO | train_inner | epoch 001:   3510 / 1830643 loss=3.62, ppl=12.3, wps=23730.4, ups=0.72, wpb=32768, bsz=32, num_updates=3500, lr=0.000193, gnorm=0.204, loss_scale=4, train_wall=138, gb_free=15.4, wall=6519
2023-10-17 01:07:36 | INFO | train_inner | epoch 001:   3610 / 1830643 loss=3.498, ppl=11.3, wps=23680.5, ups=0.72, wpb=32768, bsz=32, num_updates=3600, lr=0.0001928, gnorm=0.201, loss_scale=4, train_wall=138, gb_free=15.4, wall=6658
2023-10-17 01:09:57 | INFO | train_inner | epoch 001:   3710 / 1830643 loss=3.526, ppl=11.52, wps=23278.2, ups=0.71, wpb=32768, bsz=32, num_updates=3700, lr=0.0001926, gnorm=0.199, loss_scale=4, train_wall=140, gb_free=15.4, wall=6798
2023-10-17 01:12:17 | INFO | train_inner | epoch 001:   3810 / 1830643 loss=3.761, ppl=13.56, wps=23414.2, ups=0.71, wpb=32768, bsz=32, num_updates=3800, lr=0.0001924, gnorm=0.232, loss_scale=8, train_wall=140, gb_free=15.4, wall=6938
2023-10-17 01:14:36 | INFO | train_inner | epoch 001:   3910 / 1830643 loss=3.57, ppl=11.87, wps=23648.3, ups=0.72, wpb=32768, bsz=32, num_updates=3900, lr=0.0001922, gnorm=0.209, loss_scale=8, train_wall=138, gb_free=15.4, wall=7077
2023-10-17 01:16:55 | INFO | train_inner | epoch 001:   4010 / 1830643 loss=3.685, ppl=12.86, wps=23571.7, ups=0.72, wpb=32768, bsz=32, num_updates=4000, lr=0.000192, gnorm=0.223, loss_scale=8, train_wall=139, gb_free=15.4, wall=7216
2023-10-17 01:19:14 | INFO | train_inner | epoch 001:   4110 / 1830643 loss=3.574, ppl=11.91, wps=23585.6, ups=0.72, wpb=32768, bsz=32, num_updates=4100, lr=0.0001918, gnorm=0.208, loss_scale=8, train_wall=139, gb_free=15.4, wall=7355
2023-10-17 01:21:32 | INFO | train_inner | epoch 001:   4210 / 1830643 loss=3.533, ppl=11.58, wps=23690.1, ups=0.72, wpb=32768, bsz=32, num_updates=4200, lr=0.0001916, gnorm=0.209, loss_scale=8, train_wall=138, gb_free=15.4, wall=7493
2023-10-17 01:23:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 01:23:52 | INFO | train_inner | epoch 001:   4311 / 1830643 loss=3.65, ppl=12.55, wps=23406, ups=0.71, wpb=32768, bsz=32, num_updates=4300, lr=0.0001914, gnorm=0.203, loss_scale=8, train_wall=140, gb_free=15.4, wall=7633
2023-10-17 01:26:11 | INFO | train_inner | epoch 001:   4411 / 1830643 loss=3.526, ppl=11.52, wps=23602.3, ups=0.72, wpb=32768, bsz=32, num_updates=4400, lr=0.0001912, gnorm=0.214, loss_scale=8, train_wall=138, gb_free=15.4, wall=7772
2023-10-17 01:28:30 | INFO | train_inner | epoch 001:   4511 / 1830643 loss=3.487, ppl=11.21, wps=23492.8, ups=0.72, wpb=32768, bsz=32, num_updates=4500, lr=0.000191, gnorm=0.201, loss_scale=8, train_wall=139, gb_free=15.4, wall=7911
2023-10-17 01:30:50 | INFO | train_inner | epoch 001:   4611 / 1830643 loss=3.625, ppl=12.34, wps=23427.5, ups=0.71, wpb=32768, bsz=32, num_updates=4600, lr=0.0001908, gnorm=0.202, loss_scale=8, train_wall=140, gb_free=15.4, wall=8051
2023-10-17 01:33:10 | INFO | train_inner | epoch 001:   4711 / 1830643 loss=3.702, ppl=13.02, wps=23459.5, ups=0.72, wpb=32768, bsz=32, num_updates=4700, lr=0.0001906, gnorm=0.21, loss_scale=8, train_wall=139, gb_free=15.4, wall=8191
2023-10-17 01:35:31 | INFO | train_inner | epoch 001:   4811 / 1830643 loss=3.704, ppl=13.03, wps=23143.4, ups=0.71, wpb=32768, bsz=32, num_updates=4800, lr=0.0001904, gnorm=0.206, loss_scale=16, train_wall=141, gb_free=15.4, wall=8333
2023-10-17 01:36:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 01:37:53 | INFO | train_inner | epoch 001:   4912 / 1830643 loss=3.625, ppl=12.33, wps=23154, ups=0.71, wpb=32768, bsz=32, num_updates=4900, lr=0.0001902, gnorm=0.195, loss_scale=8, train_wall=141, gb_free=15.4, wall=8474
2023-10-17 01:40:13 | INFO | train_inner | epoch 001:   5012 / 1830643 loss=3.53, ppl=11.55, wps=23380.6, ups=0.71, wpb=32768, bsz=32, num_updates=5000, lr=0.00019, gnorm=0.216, loss_scale=8, train_wall=140, gb_free=15.4, wall=8614
2023-10-17 01:42:31 | INFO | train_inner | epoch 001:   5112 / 1830643 loss=3.704, ppl=13.03, wps=23734.8, ups=0.72, wpb=32768, bsz=32, num_updates=5100, lr=0.0001898, gnorm=0.209, loss_scale=8, train_wall=138, gb_free=15.4, wall=8752
2023-10-17 01:44:51 | INFO | train_inner | epoch 001:   5212 / 1830643 loss=3.782, ppl=13.76, wps=23495.8, ups=0.72, wpb=32768, bsz=32, num_updates=5200, lr=0.0001896, gnorm=0.223, loss_scale=8, train_wall=139, gb_free=15.4, wall=8892
2023-10-17 01:47:10 | INFO | train_inner | epoch 001:   5312 / 1830643 loss=3.721, ppl=13.18, wps=23508.6, ups=0.72, wpb=32768, bsz=32, num_updates=5300, lr=0.0001894, gnorm=0.194, loss_scale=8, train_wall=139, gb_free=15.4, wall=9031
2023-10-17 01:49:28 | INFO | train_inner | epoch 001:   5412 / 1830643 loss=3.572, ppl=11.89, wps=23677.6, ups=0.72, wpb=32768, bsz=32, num_updates=5400, lr=0.0001892, gnorm=0.187, loss_scale=16, train_wall=138, gb_free=15.4, wall=9170
2023-10-17 01:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 01:50:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 01:51:51 | INFO | train_inner | epoch 001:   5514 / 1830643 loss=3.605, ppl=12.17, wps=23038.6, ups=0.7, wpb=32768, bsz=32, num_updates=5500, lr=0.000189, gnorm=0.207, loss_scale=4, train_wall=142, gb_free=15.4, wall=9312
2023-10-17 01:54:09 | INFO | train_inner | epoch 001:   5614 / 1830643 loss=3.572, ppl=11.89, wps=23693.7, ups=0.72, wpb=32768, bsz=32, num_updates=5600, lr=0.0001888, gnorm=0.186, loss_scale=4, train_wall=138, gb_free=15.4, wall=9450
2023-10-17 01:56:29 | INFO | train_inner | epoch 001:   5714 / 1830643 loss=3.594, ppl=12.08, wps=23433.5, ups=0.72, wpb=32768, bsz=32, num_updates=5700, lr=0.0001886, gnorm=0.19, loss_scale=4, train_wall=139, gb_free=15.4, wall=9590
2023-10-17 01:58:49 | INFO | train_inner | epoch 001:   5814 / 1830643 loss=3.695, ppl=12.95, wps=23441.4, ups=0.72, wpb=32768, bsz=32, num_updates=5800, lr=0.0001884, gnorm=0.225, loss_scale=4, train_wall=139, gb_free=15.4, wall=9730
2023-10-17 02:01:09 | INFO | train_inner | epoch 001:   5914 / 1830643 loss=3.467, ppl=11.06, wps=23310.8, ups=0.71, wpb=32768, bsz=32, num_updates=5900, lr=0.0001882, gnorm=0.194, loss_scale=4, train_wall=140, gb_free=15.4, wall=9870
2023-10-17 02:03:28 | INFO | train_inner | epoch 001:   6014 / 1830643 loss=3.451, ppl=10.94, wps=23527, ups=0.72, wpb=32768, bsz=32, num_updates=6000, lr=0.000188, gnorm=0.215, loss_scale=8, train_wall=139, gb_free=15.4, wall=10010
2023-10-17 02:05:47 | INFO | train_inner | epoch 001:   6114 / 1830643 loss=3.462, ppl=11.02, wps=23693.9, ups=0.72, wpb=32768, bsz=32, num_updates=6100, lr=0.0001878, gnorm=0.199, loss_scale=8, train_wall=138, gb_free=15.4, wall=10148
2023-10-17 02:08:06 | INFO | train_inner | epoch 001:   6214 / 1830643 loss=3.558, ppl=11.78, wps=23592.3, ups=0.72, wpb=32768, bsz=32, num_updates=6200, lr=0.0001876, gnorm=0.198, loss_scale=8, train_wall=139, gb_free=15.4, wall=10287
2023-10-17 02:10:24 | INFO | train_inner | epoch 001:   6314 / 1830643 loss=3.483, ppl=11.18, wps=23652.5, ups=0.72, wpb=32768, bsz=32, num_updates=6300, lr=0.0001874, gnorm=0.218, loss_scale=8, train_wall=138, gb_free=15.4, wall=10425
2023-10-17 02:12:43 | INFO | train_inner | epoch 001:   6414 / 1830643 loss=3.777, ppl=13.71, wps=23617.9, ups=0.72, wpb=32768, bsz=32, num_updates=6400, lr=0.0001872, gnorm=0.208, loss_scale=8, train_wall=138, gb_free=15.4, wall=10564
2023-10-17 02:15:02 | INFO | train_inner | epoch 001:   6514 / 1830643 loss=3.637, ppl=12.44, wps=23590.3, ups=0.72, wpb=32768, bsz=32, num_updates=6500, lr=0.000187, gnorm=0.203, loss_scale=16, train_wall=139, gb_free=15.4, wall=10703
2023-10-17 02:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 02:17:22 | INFO | train_inner | epoch 001:   6615 / 1830643 loss=3.706, ppl=13.05, wps=23334.5, ups=0.71, wpb=32768, bsz=32, num_updates=6600, lr=0.0001868, gnorm=0.192, loss_scale=8, train_wall=140, gb_free=15.4, wall=10843
2023-10-17 02:19:41 | INFO | train_inner | epoch 001:   6715 / 1830643 loss=3.369, ppl=10.33, wps=23673.2, ups=0.72, wpb=32768, bsz=32, num_updates=6700, lr=0.0001866, gnorm=0.178, loss_scale=8, train_wall=138, gb_free=15.4, wall=10982
2023-10-17 02:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 02:22:02 | INFO | train_inner | epoch 001:   6816 / 1830643 loss=3.584, ppl=11.99, wps=23162.1, ups=0.71, wpb=32768, bsz=32, num_updates=6800, lr=0.0001864, gnorm=0.197, loss_scale=4, train_wall=141, gb_free=15.4, wall=11123
2023-10-17 02:24:21 | INFO | train_inner | epoch 001:   6916 / 1830643 loss=3.709, ppl=13.08, wps=23651.9, ups=0.72, wpb=32768, bsz=32, num_updates=6900, lr=0.0001862, gnorm=0.196, loss_scale=4, train_wall=138, gb_free=15.4, wall=11262
2023-10-17 02:26:40 | INFO | train_inner | epoch 001:   7016 / 1830643 loss=3.605, ppl=12.16, wps=23458, ups=0.72, wpb=32768, bsz=32, num_updates=7000, lr=0.000186, gnorm=0.186, loss_scale=4, train_wall=139, gb_free=15.4, wall=11401
2023-10-17 02:28:59 | INFO | train_inner | epoch 001:   7116 / 1830643 loss=3.477, ppl=11.13, wps=23578.6, ups=0.72, wpb=32768, bsz=32, num_updates=7100, lr=0.0001858, gnorm=0.192, loss_scale=4, train_wall=139, gb_free=15.4, wall=11540
2023-10-17 02:31:18 | INFO | train_inner | epoch 001:   7216 / 1830643 loss=3.525, ppl=11.51, wps=23573.1, ups=0.72, wpb=32768, bsz=32, num_updates=7200, lr=0.0001856, gnorm=0.193, loss_scale=4, train_wall=139, gb_free=15.4, wall=11679
2023-10-17 02:33:37 | INFO | train_inner | epoch 001:   7316 / 1830643 loss=3.611, ppl=12.21, wps=23670.6, ups=0.72, wpb=32768, bsz=32, num_updates=7300, lr=0.0001854, gnorm=0.181, loss_scale=8, train_wall=138, gb_free=15.4, wall=11818
2023-10-17 02:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 02:35:58 | INFO | train_inner | epoch 001:   7417 / 1830643 loss=3.483, ppl=11.18, wps=23200.9, ups=0.71, wpb=32768, bsz=32, num_updates=7400, lr=0.0001852, gnorm=0.208, loss_scale=4, train_wall=141, gb_free=15.4, wall=11959
2023-10-17 02:37:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 02:38:19 | INFO | train_inner | epoch 001:   7518 / 1830643 loss=3.288, ppl=9.77, wps=23271.6, ups=0.71, wpb=32768, bsz=32, num_updates=7500, lr=0.000185, gnorm=0.198, loss_scale=2, train_wall=140, gb_free=15.4, wall=12100
2023-10-17 02:40:38 | INFO | train_inner | epoch 001:   7618 / 1830643 loss=3.473, ppl=11.1, wps=23470.1, ups=0.72, wpb=32768, bsz=32, num_updates=7600, lr=0.0001848, gnorm=0.201, loss_scale=2, train_wall=139, gb_free=15.4, wall=12240
2023-10-17 02:42:57 | INFO | train_inner | epoch 001:   7718 / 1830643 loss=3.496, ppl=11.29, wps=23659.3, ups=0.72, wpb=32768, bsz=32, num_updates=7700, lr=0.0001846, gnorm=0.19, loss_scale=2, train_wall=138, gb_free=15.4, wall=12378
2023-10-17 02:45:14 | INFO | train_inner | epoch 001:   7818 / 1830643 loss=3.475, ppl=11.12, wps=23861.6, ups=0.73, wpb=32768, bsz=32, num_updates=7800, lr=0.0001844, gnorm=0.179, loss_scale=2, train_wall=137, gb_free=15.4, wall=12515
2023-10-17 02:47:33 | INFO | train_inner | epoch 001:   7918 / 1830643 loss=3.586, ppl=12.01, wps=23693.7, ups=0.72, wpb=32768, bsz=32, num_updates=7900, lr=0.0001842, gnorm=0.18, loss_scale=2, train_wall=138, gb_free=15.4, wall=12654
2023-10-17 02:49:53 | INFO | train_inner | epoch 001:   8018 / 1830643 loss=3.625, ppl=12.33, wps=23388.5, ups=0.71, wpb=32768, bsz=32, num_updates=8000, lr=0.000184, gnorm=0.189, loss_scale=4, train_wall=140, gb_free=15.4, wall=12794
2023-10-17 02:52:11 | INFO | train_inner | epoch 001:   8118 / 1830643 loss=3.663, ppl=12.66, wps=23622.3, ups=0.72, wpb=32768, bsz=32, num_updates=8100, lr=0.0001838, gnorm=0.182, loss_scale=4, train_wall=138, gb_free=15.4, wall=12932
2023-10-17 02:54:31 | INFO | train_inner | epoch 001:   8218 / 1830643 loss=3.43, ppl=10.78, wps=23530.8, ups=0.72, wpb=32768, bsz=32, num_updates=8200, lr=0.0001836, gnorm=0.193, loss_scale=4, train_wall=139, gb_free=15.4, wall=13072
2023-10-17 02:56:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 02:56:51 | INFO | train_inner | epoch 001:   8319 / 1830643 loss=3.59, ppl=12.05, wps=23310, ups=0.71, wpb=32768, bsz=32, num_updates=8300, lr=0.0001834, gnorm=0.199, loss_scale=2, train_wall=140, gb_free=15.4, wall=13212
2023-10-17 02:59:09 | INFO | train_inner | epoch 001:   8419 / 1830643 loss=3.571, ppl=11.88, wps=23702, ups=0.72, wpb=32768, bsz=32, num_updates=8400, lr=0.0001832, gnorm=0.188, loss_scale=2, train_wall=138, gb_free=15.4, wall=13351
2023-10-17 03:01:29 | INFO | train_inner | epoch 001:   8519 / 1830643 loss=3.525, ppl=11.51, wps=23454, ups=0.72, wpb=32768, bsz=32, num_updates=8500, lr=0.000183, gnorm=0.185, loss_scale=2, train_wall=139, gb_free=15.4, wall=13490
2023-10-17 03:03:48 | INFO | train_inner | epoch 001:   8619 / 1830643 loss=3.582, ppl=11.98, wps=23596.4, ups=0.72, wpb=32768, bsz=32, num_updates=8600, lr=0.0001828, gnorm=0.204, loss_scale=2, train_wall=138, gb_free=15.4, wall=13629
2023-10-17 03:06:09 | INFO | train_inner | epoch 001:   8719 / 1830643 loss=3.508, ppl=11.38, wps=23300.5, ups=0.71, wpb=32768, bsz=32, num_updates=8700, lr=0.0001826, gnorm=0.207, loss_scale=2, train_wall=140, gb_free=15.4, wall=13770
2023-10-17 03:08:27 | INFO | train_inner | epoch 001:   8819 / 1830643 loss=3.611, ppl=12.22, wps=23637.1, ups=0.72, wpb=32768, bsz=32, num_updates=8800, lr=0.0001824, gnorm=0.184, loss_scale=4, train_wall=138, gb_free=15.4, wall=13908
2023-10-17 03:10:46 | INFO | train_inner | epoch 001:   8919 / 1830643 loss=3.41, ppl=10.63, wps=23585.8, ups=0.72, wpb=32768, bsz=32, num_updates=8900, lr=0.0001822, gnorm=0.192, loss_scale=4, train_wall=139, gb_free=15.4, wall=14047
2023-10-17 03:13:04 | INFO | train_inner | epoch 001:   9019 / 1830643 loss=3.614, ppl=12.24, wps=23739.7, ups=0.72, wpb=32768, bsz=32, num_updates=9000, lr=0.000182, gnorm=0.187, loss_scale=4, train_wall=138, gb_free=15.4, wall=14185
2023-10-17 03:15:23 | INFO | train_inner | epoch 001:   9119 / 1830643 loss=3.523, ppl=11.5, wps=23573.5, ups=0.72, wpb=32768, bsz=32, num_updates=9100, lr=0.0001818, gnorm=0.182, loss_scale=4, train_wall=139, gb_free=15.4, wall=14324
2023-10-17 03:17:44 | INFO | train_inner | epoch 001:   9219 / 1830643 loss=3.605, ppl=12.17, wps=23298.7, ups=0.71, wpb=32768, bsz=32, num_updates=9200, lr=0.0001816, gnorm=0.206, loss_scale=4, train_wall=140, gb_free=15.4, wall=14465
2023-10-17 03:20:05 | INFO | train_inner | epoch 001:   9319 / 1830643 loss=3.439, ppl=10.85, wps=23137.7, ups=0.71, wpb=32768, bsz=32, num_updates=9300, lr=0.0001814, gnorm=0.167, loss_scale=8, train_wall=141, gb_free=15.4, wall=14607
2023-10-17 03:22:25 | INFO | train_inner | epoch 001:   9419 / 1830643 loss=3.839, ppl=14.31, wps=23505.9, ups=0.72, wpb=32768, bsz=32, num_updates=9400, lr=0.0001812, gnorm=0.187, loss_scale=8, train_wall=139, gb_free=15.4, wall=14746
2023-10-17 03:24:44 | INFO | train_inner | epoch 001:   9519 / 1830643 loss=3.711, ppl=13.1, wps=23511.8, ups=0.72, wpb=32768, bsz=32, num_updates=9500, lr=0.000181, gnorm=0.191, loss_scale=8, train_wall=139, gb_free=15.4, wall=14885
2023-10-17 03:27:03 | INFO | train_inner | epoch 001:   9619 / 1830643 loss=3.842, ppl=14.34, wps=23536.3, ups=0.72, wpb=32768, bsz=32, num_updates=9600, lr=0.0001808, gnorm=0.186, loss_scale=8, train_wall=139, gb_free=15.4, wall=15025
2023-10-17 03:29:23 | INFO | train_inner | epoch 001:   9719 / 1830643 loss=3.901, ppl=14.94, wps=23482.5, ups=0.72, wpb=32768, bsz=32, num_updates=9700, lr=0.0001806, gnorm=0.181, loss_scale=8, train_wall=139, gb_free=15.4, wall=15164
2023-10-17 03:31:43 | INFO | train_inner | epoch 001:   9819 / 1830643 loss=3.571, ppl=11.88, wps=23341.1, ups=0.71, wpb=32768, bsz=32, num_updates=9800, lr=0.0001804, gnorm=0.181, loss_scale=8, train_wall=140, gb_free=15.4, wall=15305
2023-10-17 03:34:03 | INFO | train_inner | epoch 001:   9919 / 1830643 loss=3.557, ppl=11.77, wps=23550.1, ups=0.72, wpb=32768, bsz=32, num_updates=9900, lr=0.0001802, gnorm=0.197, loss_scale=16, train_wall=139, gb_free=15.4, wall=15444
2023-10-17 03:34:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 03:36:22 | INFO | train_inner | epoch 001:  10020 / 1830643 loss=3.575, ppl=11.92, wps=23463.9, ups=0.72, wpb=32768, bsz=32, num_updates=10000, lr=0.00018, gnorm=0.191, loss_scale=8, train_wall=139, gb_free=15.4, wall=15583
2023-10-17 03:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-10-17 03:36:22 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_10000.pt
2023-10-17 03:36:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_10000.pt
2023-10-17 03:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score None) (writing took 9.668080292991363 seconds)
2023-10-17 03:38:51 | INFO | train_inner | epoch 001:  10120 / 1830643 loss=3.628, ppl=12.36, wps=22000.8, ups=0.67, wpb=32768, bsz=32, num_updates=10100, lr=0.0001798, gnorm=0.19, loss_scale=8, train_wall=139, gb_free=15.4, wall=15732
2023-10-17 03:41:11 | INFO | train_inner | epoch 001:  10220 / 1830643 loss=3.444, ppl=10.88, wps=23513.5, ups=0.72, wpb=32768, bsz=32, num_updates=10200, lr=0.0001796, gnorm=0.189, loss_scale=8, train_wall=139, gb_free=15.4, wall=15872
2023-10-17 03:43:29 | INFO | train_inner | epoch 001:  10320 / 1830643 loss=3.595, ppl=12.08, wps=23617.1, ups=0.72, wpb=32768, bsz=32, num_updates=10300, lr=0.0001794, gnorm=0.175, loss_scale=8, train_wall=138, gb_free=15.4, wall=16010
2023-10-17 03:45:48 | INFO | train_inner | epoch 001:  10420 / 1830643 loss=3.515, ppl=11.43, wps=23588.8, ups=0.72, wpb=32768, bsz=32, num_updates=10400, lr=0.0001792, gnorm=0.18, loss_scale=8, train_wall=139, gb_free=15.4, wall=16149
2023-10-17 03:48:08 | INFO | train_inner | epoch 001:  10520 / 1830643 loss=3.853, ppl=14.45, wps=23502.6, ups=0.72, wpb=32768, bsz=32, num_updates=10500, lr=0.000179, gnorm=0.182, loss_scale=16, train_wall=139, gb_free=15.4, wall=16289
2023-10-17 03:50:26 | INFO | train_inner | epoch 001:  10620 / 1830643 loss=3.526, ppl=11.52, wps=23622.2, ups=0.72, wpb=32768, bsz=32, num_updates=10600, lr=0.0001788, gnorm=0.174, loss_scale=16, train_wall=138, gb_free=15.4, wall=16427
2023-10-17 03:52:46 | INFO | train_inner | epoch 001:  10720 / 1830643 loss=3.393, ppl=10.51, wps=23527.3, ups=0.72, wpb=32768, bsz=32, num_updates=10700, lr=0.0001786, gnorm=0.183, loss_scale=16, train_wall=139, gb_free=15.4, wall=16567
2023-10-17 03:54:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 03:55:06 | INFO | train_inner | epoch 001:  10821 / 1830643 loss=3.51, ppl=11.39, wps=23329.5, ups=0.71, wpb=32768, bsz=32, num_updates=10800, lr=0.0001784, gnorm=0.176, loss_scale=8, train_wall=140, gb_free=15.4, wall=16707
2023-10-17 03:57:25 | INFO | train_inner | epoch 001:  10921 / 1830643 loss=3.545, ppl=11.67, wps=23620.6, ups=0.72, wpb=32768, bsz=32, num_updates=10900, lr=0.0001782, gnorm=0.181, loss_scale=8, train_wall=138, gb_free=15.4, wall=16846
2023-10-17 03:59:44 | INFO | train_inner | epoch 001:  11021 / 1830643 loss=3.36, ppl=10.27, wps=23593, ups=0.72, wpb=32768, bsz=32, num_updates=11000, lr=0.000178, gnorm=0.193, loss_scale=8, train_wall=139, gb_free=15.4, wall=16985
2023-10-17 04:02:04 | INFO | train_inner | epoch 001:  11121 / 1830643 loss=3.546, ppl=11.68, wps=23300.2, ups=0.71, wpb=32768, bsz=32, num_updates=11100, lr=0.0001778, gnorm=0.19, loss_scale=8, train_wall=140, gb_free=15.4, wall=17125
2023-10-17 04:04:24 | INFO | train_inner | epoch 001:  11221 / 1830643 loss=3.495, ppl=11.28, wps=23379, ups=0.71, wpb=32768, bsz=32, num_updates=11200, lr=0.0001776, gnorm=0.192, loss_scale=8, train_wall=140, gb_free=15.4, wall=17266
2023-10-17 04:06:45 | INFO | train_inner | epoch 001:  11321 / 1830643 loss=3.477, ppl=11.14, wps=23302, ups=0.71, wpb=32768, bsz=32, num_updates=11300, lr=0.0001774, gnorm=0.168, loss_scale=16, train_wall=140, gb_free=15.4, wall=17406
2023-10-17 04:09:05 | INFO | train_inner | epoch 001:  11421 / 1830643 loss=3.556, ppl=11.76, wps=23453.8, ups=0.72, wpb=32768, bsz=32, num_updates=11400, lr=0.0001772, gnorm=0.186, loss_scale=16, train_wall=139, gb_free=15.4, wall=17546
2023-10-17 04:09:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 04:11:25 | INFO | train_inner | epoch 001:  11522 / 1830643 loss=3.566, ppl=11.85, wps=23326.3, ups=0.71, wpb=32768, bsz=32, num_updates=11500, lr=0.000177, gnorm=0.203, loss_scale=8, train_wall=140, gb_free=15.4, wall=17686
2023-10-17 04:13:45 | INFO | train_inner | epoch 001:  11622 / 1830643 loss=3.604, ppl=12.16, wps=23521.1, ups=0.72, wpb=32768, bsz=32, num_updates=11600, lr=0.0001768, gnorm=0.175, loss_scale=8, train_wall=139, gb_free=15.4, wall=17826
2023-10-17 04:16:04 | INFO | train_inner | epoch 001:  11722 / 1830643 loss=3.493, ppl=11.26, wps=23503.5, ups=0.72, wpb=32768, bsz=32, num_updates=11700, lr=0.0001766, gnorm=0.166, loss_scale=8, train_wall=139, gb_free=15.4, wall=17965
2023-10-17 04:18:24 | INFO | train_inner | epoch 001:  11822 / 1830643 loss=3.661, ppl=12.65, wps=23473.3, ups=0.72, wpb=32768, bsz=32, num_updates=11800, lr=0.0001764, gnorm=0.192, loss_scale=8, train_wall=139, gb_free=15.4, wall=18105
2023-10-17 04:20:42 | INFO | train_inner | epoch 001:  11922 / 1830643 loss=3.483, ppl=11.18, wps=23625.1, ups=0.72, wpb=32768, bsz=32, num_updates=11900, lr=0.0001762, gnorm=0.191, loss_scale=8, train_wall=138, gb_free=15.4, wall=18243
2023-10-17 04:22:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 04:22:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 04:23:04 | INFO | train_inner | epoch 001:  12024 / 1830643 loss=3.621, ppl=12.3, wps=23081.7, ups=0.7, wpb=32768, bsz=32, num_updates=12000, lr=0.000176, gnorm=0.181, loss_scale=4, train_wall=142, gb_free=15.4, wall=18385
2023-10-17 04:25:24 | INFO | train_inner | epoch 001:  12124 / 1830643 loss=3.368, ppl=10.33, wps=23455, ups=0.72, wpb=32768, bsz=32, num_updates=12100, lr=0.0001758, gnorm=0.181, loss_scale=4, train_wall=139, gb_free=15.4, wall=18525
2023-10-17 04:27:44 | INFO | train_inner | epoch 001:  12224 / 1830643 loss=3.471, ppl=11.09, wps=23478, ups=0.72, wpb=32768, bsz=32, num_updates=12200, lr=0.0001756, gnorm=0.167, loss_scale=4, train_wall=139, gb_free=15.4, wall=18665
2023-10-17 04:30:04 | INFO | train_inner | epoch 001:  12324 / 1830643 loss=3.71, ppl=13.09, wps=23325.3, ups=0.71, wpb=32768, bsz=32, num_updates=12300, lr=0.0001754, gnorm=0.169, loss_scale=4, train_wall=140, gb_free=15.4, wall=18805
2023-10-17 04:32:23 | INFO | train_inner | epoch 001:  12424 / 1830643 loss=3.512, ppl=11.41, wps=23553.8, ups=0.72, wpb=32768, bsz=32, num_updates=12400, lr=0.0001752, gnorm=0.19, loss_scale=4, train_wall=139, gb_free=15.4, wall=18944
2023-10-17 04:34:43 | INFO | train_inner | epoch 001:  12524 / 1830643 loss=3.601, ppl=12.13, wps=23453.2, ups=0.72, wpb=32768, bsz=32, num_updates=12500, lr=0.000175, gnorm=0.189, loss_scale=8, train_wall=139, gb_free=15.4, wall=19084
2023-10-17 04:37:01 | INFO | train_inner | epoch 001:  12624 / 1830643 loss=3.588, ppl=12.03, wps=23642.1, ups=0.72, wpb=32768, bsz=32, num_updates=12600, lr=0.0001748, gnorm=0.178, loss_scale=8, train_wall=138, gb_free=15.4, wall=19223
2023-10-17 04:38:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 04:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 04:39:23 | INFO | train_inner | epoch 001:  12726 / 1830643 loss=3.368, ppl=10.33, wps=23115.2, ups=0.71, wpb=32768, bsz=32, num_updates=12700, lr=0.0001746, gnorm=0.203, loss_scale=2, train_wall=141, gb_free=15.4, wall=19364
2023-10-17 04:41:43 | INFO | train_inner | epoch 001:  12826 / 1830643 loss=3.618, ppl=12.28, wps=23502.3, ups=0.72, wpb=32768, bsz=32, num_updates=12800, lr=0.0001744, gnorm=0.181, loss_scale=2, train_wall=139, gb_free=15.4, wall=19504
2023-10-17 04:44:02 | INFO | train_inner | epoch 001:  12926 / 1830643 loss=3.532, ppl=11.57, wps=23503.3, ups=0.72, wpb=32768, bsz=32, num_updates=12900, lr=0.0001742, gnorm=0.191, loss_scale=2, train_wall=139, gb_free=15.4, wall=19643
2023-10-17 04:46:21 | INFO | train_inner | epoch 001:  13026 / 1830643 loss=3.461, ppl=11.01, wps=23558.1, ups=0.72, wpb=32768, bsz=32, num_updates=13000, lr=0.000174, gnorm=0.199, loss_scale=2, train_wall=139, gb_free=15.4, wall=19782
2023-10-17 04:48:41 | INFO | train_inner | epoch 001:  13126 / 1830643 loss=3.623, ppl=12.32, wps=23510, ups=0.72, wpb=32768, bsz=32, num_updates=13100, lr=0.0001738, gnorm=0.177, loss_scale=2, train_wall=139, gb_free=15.4, wall=19922
2023-10-17 04:51:00 | INFO | train_inner | epoch 001:  13226 / 1830643 loss=3.686, ppl=12.87, wps=23517.7, ups=0.72, wpb=32768, bsz=32, num_updates=13200, lr=0.0001736, gnorm=0.182, loss_scale=4, train_wall=139, gb_free=15.4, wall=20061
2023-10-17 04:53:20 | INFO | train_inner | epoch 001:  13326 / 1830643 loss=3.154, ppl=8.9, wps=23359.2, ups=0.71, wpb=32768, bsz=32, num_updates=13300, lr=0.0001734, gnorm=0.165, loss_scale=4, train_wall=140, gb_free=15.4, wall=20201
2023-10-17 04:55:40 | INFO | train_inner | epoch 001:  13426 / 1830643 loss=3.218, ppl=9.31, wps=23506.3, ups=0.72, wpb=32768, bsz=32, num_updates=13400, lr=0.0001732, gnorm=0.173, loss_scale=4, train_wall=139, gb_free=15.4, wall=20341
2023-10-17 04:58:00 | INFO | train_inner | epoch 001:  13526 / 1830643 loss=3.08, ppl=8.45, wps=23405.6, ups=0.71, wpb=32768, bsz=32, num_updates=13500, lr=0.000173, gnorm=0.168, loss_scale=4, train_wall=140, gb_free=15.4, wall=20481
2023-10-17 05:00:18 | INFO | train_inner | epoch 001:  13626 / 1830643 loss=3.467, ppl=11.06, wps=23660.5, ups=0.72, wpb=32768, bsz=32, num_updates=13600, lr=0.0001728, gnorm=0.181, loss_scale=4, train_wall=138, gb_free=15.4, wall=20619
2023-10-17 05:02:36 | INFO | train_inner | epoch 001:  13726 / 1830643 loss=3.465, ppl=11.04, wps=23795.8, ups=0.73, wpb=32768, bsz=32, num_updates=13700, lr=0.0001726, gnorm=0.183, loss_scale=8, train_wall=137, gb_free=15.4, wall=20757
2023-10-17 05:04:54 | INFO | train_inner | epoch 001:  13826 / 1830643 loss=3.634, ppl=12.41, wps=23723.6, ups=0.72, wpb=32768, bsz=32, num_updates=13800, lr=0.0001724, gnorm=0.169, loss_scale=8, train_wall=138, gb_free=15.4, wall=20895
2023-10-17 05:05:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 05:05:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 05:07:16 | INFO | train_inner | epoch 001:  13928 / 1830643 loss=3.562, ppl=11.81, wps=23066.6, ups=0.7, wpb=32768, bsz=32, num_updates=13900, lr=0.0001722, gnorm=0.17, loss_scale=2, train_wall=142, gb_free=15.4, wall=21037
2023-10-17 05:09:35 | INFO | train_inner | epoch 001:  14028 / 1830643 loss=3.626, ppl=12.34, wps=23634.1, ups=0.72, wpb=32768, bsz=32, num_updates=14000, lr=0.000172, gnorm=0.195, loss_scale=2, train_wall=138, gb_free=15.4, wall=21176
2023-10-17 05:11:53 | INFO | train_inner | epoch 001:  14128 / 1830643 loss=3.547, ppl=11.69, wps=23641.7, ups=0.72, wpb=32768, bsz=32, num_updates=14100, lr=0.0001718, gnorm=0.182, loss_scale=2, train_wall=138, gb_free=15.4, wall=21314
2023-10-17 05:14:12 | INFO | train_inner | epoch 001:  14228 / 1830643 loss=3.608, ppl=12.2, wps=23548, ups=0.72, wpb=32768, bsz=32, num_updates=14200, lr=0.0001716, gnorm=0.174, loss_scale=2, train_wall=139, gb_free=15.4, wall=21454
2023-10-17 05:16:32 | INFO | train_inner | epoch 001:  14328 / 1830643 loss=3.589, ppl=12.03, wps=23495.4, ups=0.72, wpb=32768, bsz=32, num_updates=14300, lr=0.0001714, gnorm=0.169, loss_scale=2, train_wall=139, gb_free=15.4, wall=21593
2023-10-17 05:18:52 | INFO | train_inner | epoch 001:  14428 / 1830643 loss=3.688, ppl=12.89, wps=23353.1, ups=0.71, wpb=32768, bsz=32, num_updates=14400, lr=0.0001712, gnorm=0.189, loss_scale=4, train_wall=140, gb_free=15.4, wall=21733
2023-10-17 05:21:12 | INFO | train_inner | epoch 001:  14528 / 1830643 loss=3.496, ppl=11.28, wps=23489.6, ups=0.72, wpb=32768, bsz=32, num_updates=14500, lr=0.000171, gnorm=0.17, loss_scale=4, train_wall=139, gb_free=15.4, wall=21873
2023-10-17 05:23:30 | INFO | train_inner | epoch 001:  14628 / 1830643 loss=3.64, ppl=12.46, wps=23722.1, ups=0.72, wpb=32768, bsz=32, num_updates=14600, lr=0.0001708, gnorm=0.2, loss_scale=4, train_wall=138, gb_free=15.4, wall=22011
2023-10-17 05:25:50 | INFO | train_inner | epoch 001:  14728 / 1830643 loss=3.674, ppl=12.77, wps=23443.4, ups=0.72, wpb=32768, bsz=32, num_updates=14700, lr=0.0001706, gnorm=0.165, loss_scale=4, train_wall=139, gb_free=15.4, wall=22151
2023-10-17 05:28:07 | INFO | train_inner | epoch 001:  14828 / 1830643 loss=3.64, ppl=12.47, wps=23771.4, ups=0.73, wpb=32768, bsz=32, num_updates=14800, lr=0.0001704, gnorm=0.175, loss_scale=4, train_wall=137, gb_free=15.4, wall=22289
2023-10-17 05:30:27 | INFO | train_inner | epoch 001:  14928 / 1830643 loss=3.523, ppl=11.5, wps=23549.4, ups=0.72, wpb=32768, bsz=32, num_updates=14900, lr=0.0001702, gnorm=0.162, loss_scale=8, train_wall=139, gb_free=15.4, wall=22428
2023-10-17 05:32:46 | INFO | train_inner | epoch 001:  15028 / 1830643 loss=3.622, ppl=12.31, wps=23537.7, ups=0.72, wpb=32768, bsz=32, num_updates=15000, lr=0.00017, gnorm=0.172, loss_scale=8, train_wall=139, gb_free=15.4, wall=22567
2023-10-17 05:35:05 | INFO | train_inner | epoch 001:  15128 / 1830643 loss=3.744, ppl=13.4, wps=23477.7, ups=0.72, wpb=32768, bsz=32, num_updates=15100, lr=0.0001698, gnorm=0.175, loss_scale=8, train_wall=139, gb_free=15.4, wall=22706
2023-10-17 05:37:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 05:37:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 05:37:28 | INFO | train_inner | epoch 001:  15230 / 1830643 loss=3.708, ppl=13.07, wps=23011.5, ups=0.7, wpb=32768, bsz=32, num_updates=15200, lr=0.0001696, gnorm=0.193, loss_scale=2, train_wall=142, gb_free=15.4, wall=22849
2023-10-17 05:39:45 | INFO | train_inner | epoch 001:  15330 / 1830643 loss=4.023, ppl=16.26, wps=23838, ups=0.73, wpb=32768, bsz=32, num_updates=15300, lr=0.0001694, gnorm=0.177, loss_scale=2, train_wall=137, gb_free=15.4, wall=22986
2023-10-17 05:42:04 | INFO | train_inner | epoch 001:  15430 / 1830643 loss=3.574, ppl=11.91, wps=23525.3, ups=0.72, wpb=32768, bsz=32, num_updates=15400, lr=0.0001692, gnorm=0.161, loss_scale=2, train_wall=139, gb_free=15.4, wall=23126
2023-10-17 05:44:24 | INFO | train_inner | epoch 001:  15530 / 1830643 loss=3.668, ppl=12.71, wps=23423.2, ups=0.71, wpb=32768, bsz=32, num_updates=15500, lr=0.000169, gnorm=0.166, loss_scale=2, train_wall=140, gb_free=15.4, wall=23266
2023-10-17 05:46:42 | INFO | train_inner | epoch 001:  15630 / 1830643 loss=3.725, ppl=13.22, wps=23849.1, ups=0.73, wpb=32768, bsz=32, num_updates=15600, lr=0.0001688, gnorm=0.177, loss_scale=2, train_wall=137, gb_free=15.4, wall=23403
2023-10-17 05:49:02 | INFO | train_inner | epoch 001:  15730 / 1830643 loss=3.547, ppl=11.69, wps=23362.1, ups=0.71, wpb=32768, bsz=32, num_updates=15700, lr=0.0001686, gnorm=0.182, loss_scale=4, train_wall=140, gb_free=15.4, wall=23543
2023-10-17 05:51:20 | INFO | train_inner | epoch 001:  15830 / 1830643 loss=3.715, ppl=13.13, wps=23704.8, ups=0.72, wpb=32768, bsz=32, num_updates=15800, lr=0.0001684, gnorm=0.181, loss_scale=4, train_wall=138, gb_free=15.4, wall=23681
2023-10-17 05:53:40 | INFO | train_inner | epoch 001:  15930 / 1830643 loss=3.461, ppl=11.01, wps=23374, ups=0.71, wpb=32768, bsz=32, num_updates=15900, lr=0.0001682, gnorm=0.158, loss_scale=4, train_wall=140, gb_free=15.4, wall=23822
2023-10-17 05:55:59 | INFO | train_inner | epoch 001:  16030 / 1830643 loss=3.621, ppl=12.31, wps=23668.7, ups=0.72, wpb=32768, bsz=32, num_updates=16000, lr=0.000168, gnorm=0.196, loss_scale=4, train_wall=138, gb_free=15.4, wall=23960
2023-10-17 05:58:18 | INFO | train_inner | epoch 001:  16130 / 1830643 loss=3.703, ppl=13.02, wps=23476.5, ups=0.72, wpb=32768, bsz=32, num_updates=16100, lr=0.0001678, gnorm=0.173, loss_scale=4, train_wall=139, gb_free=15.4, wall=24100
2023-10-17 06:00:40 | INFO | train_inner | epoch 001:  16230 / 1830643 loss=3.655, ppl=12.59, wps=23232.1, ups=0.71, wpb=32768, bsz=32, num_updates=16200, lr=0.0001676, gnorm=0.178, loss_scale=4, train_wall=141, gb_free=15.4, wall=24241
2023-10-17 06:02:58 | INFO | train_inner | epoch 001:  16330 / 1830643 loss=3.562, ppl=11.81, wps=23625.5, ups=0.72, wpb=32768, bsz=32, num_updates=16300, lr=0.0001674, gnorm=0.157, loss_scale=8, train_wall=138, gb_free=15.4, wall=24379
2023-10-17 06:05:18 | INFO | train_inner | epoch 001:  16430 / 1830643 loss=3.684, ppl=12.85, wps=23451.3, ups=0.72, wpb=32768, bsz=32, num_updates=16400, lr=0.0001672, gnorm=0.17, loss_scale=8, train_wall=139, gb_free=15.4, wall=24519
2023-10-17 06:07:37 | INFO | train_inner | epoch 001:  16530 / 1830643 loss=3.595, ppl=12.09, wps=23607.2, ups=0.72, wpb=32768, bsz=32, num_updates=16500, lr=0.000167, gnorm=0.163, loss_scale=8, train_wall=138, gb_free=15.4, wall=24658
2023-10-17 06:09:56 | INFO | train_inner | epoch 001:  16630 / 1830643 loss=3.6, ppl=12.12, wps=23466.4, ups=0.72, wpb=32768, bsz=32, num_updates=16600, lr=0.0001668, gnorm=0.169, loss_scale=8, train_wall=139, gb_free=15.4, wall=24798
2023-10-17 06:12:15 | INFO | train_inner | epoch 001:  16730 / 1830643 loss=3.65, ppl=12.55, wps=23566, ups=0.72, wpb=32768, bsz=32, num_updates=16700, lr=0.0001666, gnorm=0.164, loss_scale=8, train_wall=139, gb_free=15.4, wall=24937
2023-10-17 06:14:34 | INFO | train_inner | epoch 001:  16830 / 1830643 loss=3.582, ppl=11.97, wps=23606.4, ups=0.72, wpb=32768, bsz=32, num_updates=16800, lr=0.0001664, gnorm=0.162, loss_scale=16, train_wall=138, gb_free=15.4, wall=25075
2023-10-17 06:16:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 06:16:56 | INFO | train_inner | epoch 001:  16931 / 1830643 loss=3.453, ppl=10.95, wps=23077.5, ups=0.7, wpb=32768, bsz=32, num_updates=16900, lr=0.0001662, gnorm=0.164, loss_scale=8, train_wall=142, gb_free=15.4, wall=25217
2023-10-17 06:19:14 | INFO | train_inner | epoch 001:  17031 / 1830643 loss=3.5, ppl=11.31, wps=23743.7, ups=0.72, wpb=32768, bsz=32, num_updates=17000, lr=0.000166, gnorm=0.159, loss_scale=8, train_wall=138, gb_free=15.4, wall=25355
2023-10-17 06:21:33 | INFO | train_inner | epoch 001:  17131 / 1830643 loss=3.575, ppl=11.92, wps=23550.1, ups=0.72, wpb=32768, bsz=32, num_updates=17100, lr=0.0001658, gnorm=0.185, loss_scale=8, train_wall=139, gb_free=15.4, wall=25495
2023-10-17 06:23:52 | INFO | train_inner | epoch 001:  17231 / 1830643 loss=3.503, ppl=11.33, wps=23691, ups=0.72, wpb=32768, bsz=32, num_updates=17200, lr=0.0001656, gnorm=0.165, loss_scale=8, train_wall=138, gb_free=15.4, wall=25633
2023-10-17 06:26:10 | INFO | train_inner | epoch 001:  17331 / 1830643 loss=3.532, ppl=11.57, wps=23750.3, ups=0.72, wpb=32768, bsz=32, num_updates=17300, lr=0.0001654, gnorm=0.179, loss_scale=8, train_wall=138, gb_free=15.4, wall=25771
2023-10-17 06:28:30 | INFO | train_inner | epoch 001:  17431 / 1830643 loss=3.524, ppl=11.5, wps=23416.3, ups=0.71, wpb=32768, bsz=32, num_updates=17400, lr=0.0001652, gnorm=0.186, loss_scale=8, train_wall=140, gb_free=15.4, wall=25911
2023-10-17 06:28:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 06:30:50 | INFO | train_inner | epoch 001:  17532 / 1830643 loss=3.65, ppl=12.56, wps=23295.9, ups=0.71, wpb=32768, bsz=32, num_updates=17500, lr=0.000165, gnorm=0.168, loss_scale=8, train_wall=140, gb_free=15.4, wall=26051
2023-10-17 06:33:09 | INFO | train_inner | epoch 001:  17632 / 1830643 loss=3.668, ppl=12.71, wps=23623.6, ups=0.72, wpb=32768, bsz=32, num_updates=17600, lr=0.0001648, gnorm=0.191, loss_scale=8, train_wall=138, gb_free=15.4, wall=26190
2023-10-17 06:35:28 | INFO | train_inner | epoch 001:  17732 / 1830643 loss=3.43, ppl=10.78, wps=23548.1, ups=0.72, wpb=32768, bsz=32, num_updates=17700, lr=0.0001646, gnorm=0.173, loss_scale=8, train_wall=139, gb_free=15.4, wall=26329
2023-10-17 06:37:47 | INFO | train_inner | epoch 001:  17832 / 1830643 loss=3.518, ppl=11.46, wps=23587.4, ups=0.72, wpb=32768, bsz=32, num_updates=17800, lr=0.0001644, gnorm=0.158, loss_scale=8, train_wall=139, gb_free=15.4, wall=26468
2023-10-17 06:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 06:40:07 | INFO | train_inner | epoch 001:  17933 / 1830643 loss=3.648, ppl=12.53, wps=23451.7, ups=0.72, wpb=32768, bsz=32, num_updates=17900, lr=0.0001642, gnorm=0.189, loss_scale=4, train_wall=139, gb_free=15.4, wall=26608
2023-10-17 06:42:26 | INFO | train_inner | epoch 001:  18033 / 1830643 loss=3.476, ppl=11.13, wps=23532.3, ups=0.72, wpb=32768, bsz=32, num_updates=18000, lr=0.000164, gnorm=0.172, loss_scale=4, train_wall=139, gb_free=15.4, wall=26747
2023-10-17 06:44:46 | INFO | train_inner | epoch 001:  18133 / 1830643 loss=3.535, ppl=11.6, wps=23420.7, ups=0.71, wpb=32768, bsz=32, num_updates=18100, lr=0.0001638, gnorm=0.168, loss_scale=4, train_wall=140, gb_free=15.4, wall=26887
2023-10-17 06:47:06 | INFO | train_inner | epoch 001:  18233 / 1830643 loss=3.43, ppl=10.78, wps=23408.1, ups=0.71, wpb=32768, bsz=32, num_updates=18200, lr=0.0001636, gnorm=0.174, loss_scale=4, train_wall=140, gb_free=15.4, wall=27027
2023-10-17 06:49:24 | INFO | train_inner | epoch 001:  18333 / 1830643 loss=3.498, ppl=11.3, wps=23765.7, ups=0.73, wpb=32768, bsz=32, num_updates=18300, lr=0.0001634, gnorm=0.174, loss_scale=4, train_wall=138, gb_free=15.4, wall=27165
2023-10-17 06:51:44 | INFO | train_inner | epoch 001:  18433 / 1830643 loss=3.618, ppl=12.28, wps=23339.4, ups=0.71, wpb=32768, bsz=32, num_updates=18400, lr=0.0001632, gnorm=0.173, loss_scale=8, train_wall=140, gb_free=15.4, wall=27305
2023-10-17 06:54:06 | INFO | train_inner | epoch 001:  18533 / 1830643 loss=3.525, ppl=11.51, wps=23184.7, ups=0.71, wpb=32768, bsz=32, num_updates=18500, lr=0.000163, gnorm=0.179, loss_scale=8, train_wall=141, gb_free=15.4, wall=27447
2023-10-17 06:56:25 | INFO | train_inner | epoch 001:  18633 / 1830643 loss=3.392, ppl=10.5, wps=23577.7, ups=0.72, wpb=32768, bsz=32, num_updates=18600, lr=0.0001628, gnorm=0.166, loss_scale=8, train_wall=139, gb_free=15.4, wall=27586
2023-10-17 06:58:42 | INFO | train_inner | epoch 001:  18733 / 1830643 loss=3.409, ppl=10.62, wps=23809.3, ups=0.73, wpb=32768, bsz=32, num_updates=18700, lr=0.0001626, gnorm=0.181, loss_scale=8, train_wall=137, gb_free=15.4, wall=27723
2023-10-17 06:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 07:01:03 | INFO | train_inner | epoch 001:  18834 / 1830643 loss=3.546, ppl=11.68, wps=23223.2, ups=0.71, wpb=32768, bsz=32, num_updates=18800, lr=0.0001624, gnorm=0.169, loss_scale=4, train_wall=141, gb_free=15.4, wall=27864
2023-10-17 07:03:22 | INFO | train_inner | epoch 001:  18934 / 1830643 loss=3.55, ppl=11.71, wps=23583.5, ups=0.72, wpb=32768, bsz=32, num_updates=18900, lr=0.0001622, gnorm=0.182, loss_scale=4, train_wall=139, gb_free=15.4, wall=28003
2023-10-17 07:05:43 | INFO | train_inner | epoch 001:  19034 / 1830643 loss=3.52, ppl=11.48, wps=23204.2, ups=0.71, wpb=32768, bsz=32, num_updates=19000, lr=0.000162, gnorm=0.165, loss_scale=4, train_wall=141, gb_free=15.4, wall=28145
2023-10-17 07:08:02 | INFO | train_inner | epoch 001:  19134 / 1830643 loss=3.413, ppl=10.65, wps=23622.4, ups=0.72, wpb=32768, bsz=32, num_updates=19100, lr=0.0001618, gnorm=0.184, loss_scale=4, train_wall=138, gb_free=15.4, wall=28283
2023-10-17 07:10:23 | INFO | train_inner | epoch 001:  19234 / 1830643 loss=3.503, ppl=11.34, wps=23234.7, ups=0.71, wpb=32768, bsz=32, num_updates=19200, lr=0.0001616, gnorm=0.165, loss_scale=4, train_wall=141, gb_free=15.4, wall=28424
2023-10-17 07:12:42 | INFO | train_inner | epoch 001:  19334 / 1830643 loss=3.465, ppl=11.04, wps=23516.4, ups=0.72, wpb=32768, bsz=32, num_updates=19300, lr=0.0001614, gnorm=0.173, loss_scale=8, train_wall=139, gb_free=15.4, wall=28564
2023-10-17 07:15:02 | INFO | train_inner | epoch 001:  19434 / 1830643 loss=3.465, ppl=11.04, wps=23483.9, ups=0.72, wpb=32768, bsz=32, num_updates=19400, lr=0.0001612, gnorm=0.174, loss_scale=8, train_wall=139, gb_free=15.4, wall=28703
2023-10-17 07:17:21 | INFO | train_inner | epoch 001:  19534 / 1830643 loss=3.402, ppl=10.57, wps=23510.6, ups=0.72, wpb=32768, bsz=32, num_updates=19500, lr=0.000161, gnorm=0.164, loss_scale=8, train_wall=139, gb_free=15.4, wall=28843
2023-10-17 07:19:42 | INFO | train_inner | epoch 001:  19634 / 1830643 loss=3.448, ppl=10.91, wps=23267.3, ups=0.71, wpb=32768, bsz=32, num_updates=19600, lr=0.0001608, gnorm=0.171, loss_scale=8, train_wall=140, gb_free=15.4, wall=28983
2023-10-17 07:20:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 07:20:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 07:20:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-10-17 07:22:06 | INFO | train_inner | epoch 001:  19737 / 1830643 loss=3.459, ppl=10.99, wps=22867.6, ups=0.7, wpb=32768, bsz=32, num_updates=19700, lr=0.0001606, gnorm=0.202, loss_scale=1, train_wall=143, gb_free=15.4, wall=29127
2023-10-17 07:24:28 | INFO | train_inner | epoch 001:  19837 / 1830643 loss=3.574, ppl=11.91, wps=23016.8, ups=0.7, wpb=32768, bsz=32, num_updates=19800, lr=0.0001604, gnorm=0.162, loss_scale=1, train_wall=142, gb_free=15.4, wall=29269
2023-10-17 07:26:46 | INFO | train_inner | epoch 001:  19937 / 1830643 loss=3.41, ppl=10.63, wps=23664.5, ups=0.72, wpb=32768, bsz=32, num_updates=19900, lr=0.0001602, gnorm=0.168, loss_scale=1, train_wall=138, gb_free=15.4, wall=29408
2023-10-17 07:29:07 | INFO | train_inner | epoch 001:  20037 / 1830643 loss=3.611, ppl=12.22, wps=23314.8, ups=0.71, wpb=32768, bsz=32, num_updates=20000, lr=0.00016, gnorm=0.171, loss_scale=1, train_wall=140, gb_free=15.4, wall=29548
2023-10-17 07:29:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 20000 updates
2023-10-17 07:29:07 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_20000.pt
2023-10-17 07:29:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_20000.pt
2023-10-17 07:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_20000.pt (epoch 1 @ 20000 updates, score None) (writing took 24.30312994297128 seconds)
2023-10-17 07:31:50 | INFO | train_inner | epoch 001:  20137 / 1830643 loss=3.455, ppl=10.96, wps=20065.3, ups=0.61, wpb=32768, bsz=32, num_updates=20100, lr=0.0001598, gnorm=0.199, loss_scale=1, train_wall=139, gb_free=15.4, wall=29711
2023-10-17 07:34:11 | INFO | train_inner | epoch 001:  20237 / 1830643 loss=3.402, ppl=10.57, wps=23340.5, ups=0.71, wpb=32768, bsz=32, num_updates=20200, lr=0.0001596, gnorm=0.184, loss_scale=2, train_wall=140, gb_free=15.4, wall=29852
2023-10-17 07:36:29 | INFO | train_inner | epoch 001:  20337 / 1830643 loss=3.427, ppl=10.75, wps=23671.8, ups=0.72, wpb=32768, bsz=32, num_updates=20300, lr=0.0001594, gnorm=0.181, loss_scale=2, train_wall=138, gb_free=15.4, wall=29990
2023-10-17 07:38:48 | INFO | train_inner | epoch 001:  20437 / 1830643 loss=3.604, ppl=12.16, wps=23649.2, ups=0.72, wpb=32768, bsz=32, num_updates=20400, lr=0.0001592, gnorm=0.178, loss_scale=2, train_wall=138, gb_free=15.4, wall=30129
2023-10-17 07:41:06 | INFO | train_inner | epoch 001:  20537 / 1830643 loss=3.508, ppl=11.38, wps=23597, ups=0.72, wpb=32768, bsz=32, num_updates=20500, lr=0.000159, gnorm=0.162, loss_scale=2, train_wall=139, gb_free=15.4, wall=30268
2023-10-17 07:43:24 | INFO | train_inner | epoch 001:  20637 / 1830643 loss=3.329, ppl=10.05, wps=23806.6, ups=0.73, wpb=32768, bsz=32, num_updates=20600, lr=0.0001588, gnorm=0.175, loss_scale=2, train_wall=137, gb_free=15.4, wall=30405
2023-10-17 07:45:43 | INFO | train_inner | epoch 001:  20737 / 1830643 loss=3.438, ppl=10.84, wps=23629.1, ups=0.72, wpb=32768, bsz=32, num_updates=20700, lr=0.0001586, gnorm=0.181, loss_scale=4, train_wall=138, gb_free=15.4, wall=30544
2023-10-17 07:48:02 | INFO | train_inner | epoch 001:  20837 / 1830643 loss=3.514, ppl=11.43, wps=23573.4, ups=0.72, wpb=32768, bsz=32, num_updates=20800, lr=0.0001584, gnorm=0.175, loss_scale=4, train_wall=139, gb_free=15.4, wall=30683
2023-10-17 07:50:23 | INFO | train_inner | epoch 001:  20937 / 1830643 loss=3.421, ppl=10.71, wps=23284.5, ups=0.71, wpb=32768, bsz=32, num_updates=20900, lr=0.0001582, gnorm=0.164, loss_scale=4, train_wall=140, gb_free=15.4, wall=30824
2023-10-17 07:52:41 | INFO | train_inner | epoch 001:  21037 / 1830643 loss=3.457, ppl=10.98, wps=23599.9, ups=0.72, wpb=32768, bsz=32, num_updates=21000, lr=0.000158, gnorm=0.175, loss_scale=4, train_wall=138, gb_free=15.4, wall=30963
2023-10-17 07:55:01 | INFO | train_inner | epoch 001:  21137 / 1830643 loss=3.517, ppl=11.45, wps=23441.8, ups=0.72, wpb=32768, bsz=32, num_updates=21100, lr=0.0001578, gnorm=0.169, loss_scale=4, train_wall=139, gb_free=15.4, wall=31102
2023-10-17 07:57:19 | INFO | train_inner | epoch 001:  21237 / 1830643 loss=3.484, ppl=11.19, wps=23715.1, ups=0.72, wpb=32768, bsz=32, num_updates=21200, lr=0.0001576, gnorm=0.172, loss_scale=8, train_wall=138, gb_free=15.4, wall=31240
2023-10-17 07:59:38 | INFO | train_inner | epoch 001:  21337 / 1830643 loss=3.531, ppl=11.56, wps=23660.1, ups=0.72, wpb=32768, bsz=32, num_updates=21300, lr=0.0001574, gnorm=0.176, loss_scale=8, train_wall=138, gb_free=15.4, wall=31379
2023-10-17 08:01:57 | INFO | train_inner | epoch 001:  21437 / 1830643 loss=3.54, ppl=11.63, wps=23501, ups=0.72, wpb=32768, bsz=32, num_updates=21400, lr=0.0001572, gnorm=0.161, loss_scale=8, train_wall=139, gb_free=15.4, wall=31518
2023-10-17 08:04:16 | INFO | train_inner | epoch 001:  21537 / 1830643 loss=3.388, ppl=10.47, wps=23553.1, ups=0.72, wpb=32768, bsz=32, num_updates=21500, lr=0.000157, gnorm=0.197, loss_scale=8, train_wall=139, gb_free=15.4, wall=31658
2023-10-17 08:06:35 | INFO | train_inner | epoch 001:  21637 / 1830643 loss=3.363, ppl=10.29, wps=23558.4, ups=0.72, wpb=32768, bsz=32, num_updates=21600, lr=0.0001568, gnorm=0.161, loss_scale=8, train_wall=139, gb_free=15.4, wall=31797
2023-10-17 08:08:55 | INFO | train_inner | epoch 001:  21737 / 1830643 loss=3.444, ppl=10.88, wps=23542.6, ups=0.72, wpb=32768, bsz=32, num_updates=21700, lr=0.0001566, gnorm=0.161, loss_scale=16, train_wall=139, gb_free=15.4, wall=31936
2023-10-17 08:11:14 | INFO | train_inner | epoch 001:  21837 / 1830643 loss=3.305, ppl=9.89, wps=23530.9, ups=0.72, wpb=32768, bsz=32, num_updates=21800, lr=0.0001564, gnorm=0.166, loss_scale=16, train_wall=139, gb_free=15.4, wall=32075
2023-10-17 08:13:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 08:13:35 | INFO | train_inner | epoch 001:  21938 / 1830643 loss=3.457, ppl=10.98, wps=23282.9, ups=0.71, wpb=32768, bsz=32, num_updates=21900, lr=0.0001562, gnorm=0.163, loss_scale=8, train_wall=140, gb_free=15.4, wall=32216
2023-10-17 08:15:53 | INFO | train_inner | epoch 001:  22038 / 1830643 loss=3.454, ppl=10.96, wps=23698.8, ups=0.72, wpb=32768, bsz=32, num_updates=22000, lr=0.000156, gnorm=0.161, loss_scale=8, train_wall=138, gb_free=15.4, wall=32354
2023-10-17 08:18:11 | INFO | train_inner | epoch 001:  22138 / 1830643 loss=3.276, ppl=9.68, wps=23696.6, ups=0.72, wpb=32768, bsz=32, num_updates=22100, lr=0.0001558, gnorm=0.159, loss_scale=8, train_wall=138, gb_free=15.4, wall=32492
2023-10-17 08:19:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 08:20:32 | INFO | train_inner | epoch 001:  22239 / 1830643 loss=3.448, ppl=10.91, wps=23322.9, ups=0.71, wpb=32768, bsz=32, num_updates=22200, lr=0.0001556, gnorm=0.167, loss_scale=4, train_wall=140, gb_free=15.4, wall=32633
2023-10-17 08:22:52 | INFO | train_inner | epoch 001:  22339 / 1830643 loss=3.426, ppl=10.75, wps=23275.1, ups=0.71, wpb=32768, bsz=32, num_updates=22300, lr=0.0001554, gnorm=0.164, loss_scale=4, train_wall=140, gb_free=15.4, wall=32774
2023-10-17 08:25:16 | INFO | train_inner | epoch 001:  22439 / 1830643 loss=3.728, ppl=13.25, wps=22783.1, ups=0.7, wpb=32768, bsz=32, num_updates=22400, lr=0.0001552, gnorm=0.166, loss_scale=4, train_wall=143, gb_free=15.4, wall=32917
2023-10-17 08:27:34 | INFO | train_inner | epoch 001:  22539 / 1830643 loss=3.579, ppl=11.95, wps=23751.5, ups=0.72, wpb=32768, bsz=32, num_updates=22500, lr=0.000155, gnorm=0.16, loss_scale=4, train_wall=138, gb_free=15.4, wall=33055
2023-10-17 08:29:53 | INFO | train_inner | epoch 001:  22639 / 1830643 loss=3.633, ppl=12.41, wps=23617.5, ups=0.72, wpb=32768, bsz=32, num_updates=22600, lr=0.0001548, gnorm=0.168, loss_scale=4, train_wall=138, gb_free=15.4, wall=33194
2023-10-17 08:32:13 | INFO | train_inner | epoch 001:  22739 / 1830643 loss=3.419, ppl=10.7, wps=23478.8, ups=0.72, wpb=32768, bsz=32, num_updates=22700, lr=0.0001546, gnorm=0.161, loss_scale=8, train_wall=139, gb_free=15.4, wall=33334
2023-10-17 08:34:31 | INFO | train_inner | epoch 001:  22839 / 1830643 loss=3.668, ppl=12.71, wps=23742.1, ups=0.72, wpb=32768, bsz=32, num_updates=22800, lr=0.0001544, gnorm=0.184, loss_scale=8, train_wall=138, gb_free=15.4, wall=33472
2023-10-17 08:36:48 | INFO | train_inner | epoch 001:  22939 / 1830643 loss=3.516, ppl=11.44, wps=23776.1, ups=0.73, wpb=32768, bsz=32, num_updates=22900, lr=0.0001542, gnorm=0.168, loss_scale=8, train_wall=137, gb_free=15.4, wall=33610
2023-10-17 08:39:08 | INFO | train_inner | epoch 001:  23039 / 1830643 loss=3.465, ppl=11.04, wps=23485.2, ups=0.72, wpb=32768, bsz=32, num_updates=23000, lr=0.000154, gnorm=0.193, loss_scale=8, train_wall=139, gb_free=15.4, wall=33749
2023-10-17 08:41:27 | INFO | train_inner | epoch 001:  23139 / 1830643 loss=3.5, ppl=11.31, wps=23583, ups=0.72, wpb=32768, bsz=32, num_updates=23100, lr=0.0001538, gnorm=0.181, loss_scale=8, train_wall=139, gb_free=15.4, wall=33888
2023-10-17 08:43:46 | INFO | train_inner | epoch 001:  23239 / 1830643 loss=3.335, ppl=10.09, wps=23549.7, ups=0.72, wpb=32768, bsz=32, num_updates=23200, lr=0.0001536, gnorm=0.179, loss_scale=16, train_wall=139, gb_free=15.4, wall=34027
2023-10-17 08:46:04 | INFO | train_inner | epoch 001:  23339 / 1830643 loss=3.552, ppl=11.73, wps=23764, ups=0.73, wpb=32768, bsz=32, num_updates=23300, lr=0.0001534, gnorm=0.159, loss_scale=16, train_wall=138, gb_free=15.4, wall=34165
2023-10-17 08:47:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 08:48:23 | INFO | train_inner | epoch 001:  23440 / 1830643 loss=3.574, ppl=11.91, wps=23528, ups=0.72, wpb=32768, bsz=32, num_updates=23400, lr=0.0001532, gnorm=0.168, loss_scale=8, train_wall=139, gb_free=15.4, wall=34304
2023-10-17 08:50:41 | INFO | train_inner | epoch 001:  23540 / 1830643 loss=3.422, ppl=10.71, wps=23799.4, ups=0.73, wpb=32768, bsz=32, num_updates=23500, lr=0.000153, gnorm=0.168, loss_scale=8, train_wall=137, gb_free=15.4, wall=34442
2023-10-17 08:53:00 | INFO | train_inner | epoch 001:  23640 / 1830643 loss=3.521, ppl=11.48, wps=23606.6, ups=0.72, wpb=32768, bsz=32, num_updates=23600, lr=0.0001528, gnorm=0.184, loss_scale=8, train_wall=138, gb_free=15.4, wall=34581
2023-10-17 08:55:19 | INFO | train_inner | epoch 001:  23740 / 1830643 loss=3.654, ppl=12.59, wps=23486.4, ups=0.72, wpb=32768, bsz=32, num_updates=23700, lr=0.0001526, gnorm=0.16, loss_scale=8, train_wall=139, gb_free=15.4, wall=34720
2023-10-17 08:57:38 | INFO | train_inner | epoch 001:  23840 / 1830643 loss=3.497, ppl=11.29, wps=23641.4, ups=0.72, wpb=32768, bsz=32, num_updates=23800, lr=0.0001524, gnorm=0.173, loss_scale=8, train_wall=138, gb_free=15.4, wall=34859
2023-10-17 08:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 08:59:58 | INFO | train_inner | epoch 001:  23941 / 1830643 loss=3.402, ppl=10.57, wps=23434.3, ups=0.72, wpb=32768, bsz=32, num_updates=23900, lr=0.0001522, gnorm=0.162, loss_scale=8, train_wall=139, gb_free=15.4, wall=34999
2023-10-17 09:00:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 09:02:19 | INFO | train_inner | epoch 001:  24042 / 1830643 loss=3.582, ppl=11.98, wps=23135.9, ups=0.71, wpb=32768, bsz=32, num_updates=24000, lr=0.000152, gnorm=0.203, loss_scale=4, train_wall=141, gb_free=15.4, wall=35140
2023-10-17 09:04:39 | INFO | train_inner | epoch 001:  24142 / 1830643 loss=3.467, ppl=11.05, wps=23471.6, ups=0.72, wpb=32768, bsz=32, num_updates=24100, lr=0.0001518, gnorm=0.164, loss_scale=4, train_wall=139, gb_free=15.4, wall=35280
2023-10-17 09:06:57 | INFO | train_inner | epoch 001:  24242 / 1830643 loss=3.485, ppl=11.2, wps=23652.7, ups=0.72, wpb=32768, bsz=32, num_updates=24200, lr=0.0001516, gnorm=0.159, loss_scale=4, train_wall=138, gb_free=15.4, wall=35419
2023-10-17 09:09:15 | INFO | train_inner | epoch 001:  24342 / 1830643 loss=3.37, ppl=10.34, wps=23770.7, ups=0.73, wpb=32768, bsz=32, num_updates=24300, lr=0.0001514, gnorm=0.162, loss_scale=4, train_wall=137, gb_free=15.4, wall=35556
2023-10-17 09:11:34 | INFO | train_inner | epoch 001:  24442 / 1830643 loss=3.481, ppl=11.16, wps=23597.4, ups=0.72, wpb=32768, bsz=32, num_updates=24400, lr=0.0001512, gnorm=0.182, loss_scale=4, train_wall=139, gb_free=15.4, wall=35695
2023-10-17 09:13:53 | INFO | train_inner | epoch 001:  24542 / 1830643 loss=3.445, ppl=10.89, wps=23601.5, ups=0.72, wpb=32768, bsz=32, num_updates=24500, lr=0.000151, gnorm=0.16, loss_scale=8, train_wall=138, gb_free=15.4, wall=35834
2023-10-17 09:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 09:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 09:16:17 | INFO | train_inner | epoch 001:  24644 / 1830643 loss=3.704, ppl=13.03, wps=22675.8, ups=0.69, wpb=32768, bsz=32, num_updates=24600, lr=0.0001508, gnorm=0.163, loss_scale=2, train_wall=144, gb_free=15.4, wall=35979
2023-10-17 09:18:37 | INFO | train_inner | epoch 001:  24744 / 1830643 loss=3.565, ppl=11.84, wps=23427.7, ups=0.71, wpb=32768, bsz=32, num_updates=24700, lr=0.0001506, gnorm=0.155, loss_scale=2, train_wall=139, gb_free=15.4, wall=36119
2023-10-17 09:20:55 | INFO | train_inner | epoch 001:  24844 / 1830643 loss=3.587, ppl=12.02, wps=23777.5, ups=0.73, wpb=32768, bsz=32, num_updates=24800, lr=0.0001504, gnorm=0.162, loss_scale=2, train_wall=137, gb_free=15.4, wall=36256
2023-10-17 09:23:14 | INFO | train_inner | epoch 001:  24944 / 1830643 loss=3.397, ppl=10.53, wps=23645, ups=0.72, wpb=32768, bsz=32, num_updates=24900, lr=0.0001502, gnorm=0.259, loss_scale=2, train_wall=138, gb_free=15.4, wall=36395
2023-10-17 09:25:33 | INFO | train_inner | epoch 001:  25044 / 1830643 loss=3.386, ppl=10.46, wps=23560.8, ups=0.72, wpb=32768, bsz=32, num_updates=25000, lr=0.00015, gnorm=0.159, loss_scale=2, train_wall=139, gb_free=15.4, wall=36534
2023-10-17 09:27:50 | INFO | train_inner | epoch 001:  25144 / 1830643 loss=3.41, ppl=10.63, wps=23841.6, ups=0.73, wpb=32768, bsz=32, num_updates=25100, lr=0.0001498, gnorm=0.161, loss_scale=4, train_wall=137, gb_free=15.4, wall=36671
2023-10-17 09:30:09 | INFO | train_inner | epoch 001:  25244 / 1830643 loss=3.492, ppl=11.25, wps=23657.2, ups=0.72, wpb=32768, bsz=32, num_updates=25200, lr=0.0001496, gnorm=0.165, loss_scale=4, train_wall=138, gb_free=15.4, wall=36810
2023-10-17 09:32:27 | INFO | train_inner | epoch 001:  25344 / 1830643 loss=3.549, ppl=11.7, wps=23640.2, ups=0.72, wpb=32768, bsz=32, num_updates=25300, lr=0.0001494, gnorm=0.15, loss_scale=4, train_wall=138, gb_free=15.4, wall=36949
2023-10-17 09:34:46 | INFO | train_inner | epoch 001:  25444 / 1830643 loss=3.612, ppl=12.23, wps=23671.1, ups=0.72, wpb=32768, bsz=32, num_updates=25400, lr=0.0001492, gnorm=0.164, loss_scale=4, train_wall=138, gb_free=15.4, wall=37087
2023-10-17 09:37:05 | INFO | train_inner | epoch 001:  25544 / 1830643 loss=3.416, ppl=10.68, wps=23483.2, ups=0.72, wpb=32768, bsz=32, num_updates=25500, lr=0.000149, gnorm=0.158, loss_scale=4, train_wall=139, gb_free=15.4, wall=37227
2023-10-17 09:39:24 | INFO | train_inner | epoch 001:  25644 / 1830643 loss=3.525, ppl=11.51, wps=23654, ups=0.72, wpb=32768, bsz=32, num_updates=25600, lr=0.0001488, gnorm=0.167, loss_scale=8, train_wall=138, gb_free=15.4, wall=37365
2023-10-17 09:41:42 | INFO | train_inner | epoch 001:  25744 / 1830643 loss=3.487, ppl=11.21, wps=23688.8, ups=0.72, wpb=32768, bsz=32, num_updates=25700, lr=0.0001486, gnorm=0.161, loss_scale=8, train_wall=138, gb_free=15.4, wall=37503
2023-10-17 09:44:01 | INFO | train_inner | epoch 001:  25844 / 1830643 loss=3.509, ppl=11.38, wps=23645.2, ups=0.72, wpb=32768, bsz=32, num_updates=25800, lr=0.0001484, gnorm=0.16, loss_scale=8, train_wall=138, gb_free=15.4, wall=37642
2023-10-17 09:46:18 | INFO | train_inner | epoch 001:  25944 / 1830643 loss=3.534, ppl=11.58, wps=23844, ups=0.73, wpb=32768, bsz=32, num_updates=25900, lr=0.0001482, gnorm=0.156, loss_scale=8, train_wall=137, gb_free=15.4, wall=37779
2023-10-17 09:48:38 | INFO | train_inner | epoch 001:  26044 / 1830643 loss=3.448, ppl=10.92, wps=23433.4, ups=0.72, wpb=32768, bsz=32, num_updates=26000, lr=0.000148, gnorm=0.172, loss_scale=8, train_wall=139, gb_free=15.4, wall=37919
2023-10-17 09:50:56 | INFO | train_inner | epoch 001:  26144 / 1830643 loss=3.343, ppl=10.15, wps=23689.8, ups=0.72, wpb=32768, bsz=32, num_updates=26100, lr=0.0001478, gnorm=0.159, loss_scale=16, train_wall=138, gb_free=15.4, wall=38058
2023-10-17 09:53:15 | INFO | train_inner | epoch 001:  26244 / 1830643 loss=3.163, ppl=8.96, wps=23624.9, ups=0.72, wpb=32768, bsz=32, num_updates=26200, lr=0.0001476, gnorm=0.161, loss_scale=16, train_wall=138, gb_free=15.4, wall=38196
2023-10-17 09:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 09:55:35 | INFO | train_inner | epoch 001:  26345 / 1830643 loss=3.388, ppl=10.47, wps=23419.7, ups=0.71, wpb=32768, bsz=32, num_updates=26300, lr=0.0001474, gnorm=0.177, loss_scale=8, train_wall=140, gb_free=15.4, wall=38336
2023-10-17 09:56:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 09:57:54 | INFO | train_inner | epoch 001:  26446 / 1830643 loss=3.468, ppl=11.07, wps=23560.3, ups=0.72, wpb=32768, bsz=32, num_updates=26400, lr=0.0001472, gnorm=0.163, loss_scale=4, train_wall=139, gb_free=15.4, wall=38475
2023-10-17 10:00:12 | INFO | train_inner | epoch 001:  26546 / 1830643 loss=3.47, ppl=11.08, wps=23674, ups=0.72, wpb=32768, bsz=32, num_updates=26500, lr=0.000147, gnorm=0.171, loss_scale=4, train_wall=138, gb_free=15.4, wall=38614
2023-10-17 10:02:32 | INFO | train_inner | epoch 001:  26646 / 1830643 loss=3.615, ppl=12.25, wps=23408.5, ups=0.71, wpb=32768, bsz=32, num_updates=26600, lr=0.0001468, gnorm=0.158, loss_scale=4, train_wall=140, gb_free=15.4, wall=38754
2023-10-17 10:04:51 | INFO | train_inner | epoch 001:  26746 / 1830643 loss=3.441, ppl=10.86, wps=23711.4, ups=0.72, wpb=32768, bsz=32, num_updates=26700, lr=0.0001466, gnorm=0.171, loss_scale=4, train_wall=138, gb_free=15.4, wall=38892
2023-10-17 10:07:10 | INFO | train_inner | epoch 001:  26846 / 1830643 loss=3.382, ppl=10.43, wps=23585.8, ups=0.72, wpb=32768, bsz=32, num_updates=26800, lr=0.0001464, gnorm=0.159, loss_scale=4, train_wall=139, gb_free=15.4, wall=39031
2023-10-17 10:09:30 | INFO | train_inner | epoch 001:  26946 / 1830643 loss=3.484, ppl=11.19, wps=23260.1, ups=0.71, wpb=32768, bsz=32, num_updates=26900, lr=0.0001462, gnorm=0.153, loss_scale=8, train_wall=141, gb_free=15.4, wall=39172
2023-10-17 10:11:49 | INFO | train_inner | epoch 001:  27046 / 1830643 loss=3.708, ppl=13.07, wps=23623.3, ups=0.72, wpb=32768, bsz=32, num_updates=27000, lr=0.000146, gnorm=0.163, loss_scale=8, train_wall=138, gb_free=15.4, wall=39310
2023-10-17 10:14:08 | INFO | train_inner | epoch 001:  27146 / 1830643 loss=3.328, ppl=10.04, wps=23585.7, ups=0.72, wpb=32768, bsz=32, num_updates=27100, lr=0.0001458, gnorm=0.16, loss_scale=8, train_wall=139, gb_free=15.4, wall=39449
2023-10-17 10:16:27 | INFO | train_inner | epoch 001:  27246 / 1830643 loss=3.479, ppl=11.15, wps=23671, ups=0.72, wpb=32768, bsz=32, num_updates=27200, lr=0.0001456, gnorm=0.149, loss_scale=8, train_wall=138, gb_free=15.4, wall=39588
2023-10-17 10:18:45 | INFO | train_inner | epoch 001:  27346 / 1830643 loss=3.597, ppl=12.1, wps=23743.3, ups=0.72, wpb=32768, bsz=32, num_updates=27300, lr=0.0001454, gnorm=0.173, loss_scale=8, train_wall=138, gb_free=15.4, wall=39726
2023-10-17 10:21:04 | INFO | train_inner | epoch 001:  27446 / 1830643 loss=3.501, ppl=11.32, wps=23562.7, ups=0.72, wpb=32768, bsz=32, num_updates=27400, lr=0.0001452, gnorm=0.172, loss_scale=16, train_wall=139, gb_free=15.4, wall=39865
2023-10-17 10:23:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 10:23:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 10:23:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 10:23:27 | INFO | train_inner | epoch 001:  27549 / 1830643 loss=3.486, ppl=11.2, wps=22785.7, ups=0.7, wpb=32768, bsz=32, num_updates=27500, lr=0.000145, gnorm=0.204, loss_scale=2, train_wall=143, gb_free=15.4, wall=40009
2023-10-17 10:25:47 | INFO | train_inner | epoch 001:  27649 / 1830643 loss=3.521, ppl=11.48, wps=23557.6, ups=0.72, wpb=32768, bsz=32, num_updates=27600, lr=0.0001448, gnorm=0.167, loss_scale=2, train_wall=139, gb_free=15.4, wall=40148
2023-10-17 10:28:06 | INFO | train_inner | epoch 001:  27749 / 1830643 loss=3.532, ppl=11.57, wps=23555, ups=0.72, wpb=32768, bsz=32, num_updates=27700, lr=0.0001446, gnorm=0.157, loss_scale=2, train_wall=139, gb_free=15.4, wall=40287
2023-10-17 10:30:24 | INFO | train_inner | epoch 001:  27849 / 1830643 loss=3.488, ppl=11.22, wps=23724.3, ups=0.72, wpb=32768, bsz=32, num_updates=27800, lr=0.0001444, gnorm=0.147, loss_scale=2, train_wall=138, gb_free=15.4, wall=40425
2023-10-17 10:32:45 | INFO | train_inner | epoch 001:  27949 / 1830643 loss=3.701, ppl=13.01, wps=23127.6, ups=0.71, wpb=32768, bsz=32, num_updates=27900, lr=0.0001442, gnorm=0.167, loss_scale=2, train_wall=141, gb_free=15.4, wall=40567
2023-10-17 10:35:04 | INFO | train_inner | epoch 001:  28049 / 1830643 loss=3.368, ppl=10.33, wps=23662, ups=0.72, wpb=32768, bsz=32, num_updates=28000, lr=0.000144, gnorm=0.158, loss_scale=2, train_wall=138, gb_free=15.4, wall=40705
2023-10-17 10:37:22 | INFO | train_inner | epoch 001:  28149 / 1830643 loss=3.552, ppl=11.73, wps=23720.4, ups=0.72, wpb=32768, bsz=32, num_updates=28100, lr=0.0001438, gnorm=0.169, loss_scale=4, train_wall=138, gb_free=15.4, wall=40843
2023-10-17 10:39:41 | INFO | train_inner | epoch 001:  28249 / 1830643 loss=3.428, ppl=10.76, wps=23555.5, ups=0.72, wpb=32768, bsz=32, num_updates=28200, lr=0.0001436, gnorm=0.156, loss_scale=4, train_wall=139, gb_free=15.4, wall=40982
2023-10-17 10:42:01 | INFO | train_inner | epoch 001:  28349 / 1830643 loss=3.393, ppl=10.5, wps=23505.8, ups=0.72, wpb=32768, bsz=32, num_updates=28300, lr=0.0001434, gnorm=0.165, loss_scale=4, train_wall=139, gb_free=15.4, wall=41122
2023-10-17 10:44:20 | INFO | train_inner | epoch 001:  28449 / 1830643 loss=3.593, ppl=12.07, wps=23573.5, ups=0.72, wpb=32768, bsz=32, num_updates=28400, lr=0.0001432, gnorm=0.164, loss_scale=4, train_wall=139, gb_free=15.4, wall=41261
2023-10-17 10:46:39 | INFO | train_inner | epoch 001:  28549 / 1830643 loss=3.502, ppl=11.33, wps=23562.1, ups=0.72, wpb=32768, bsz=32, num_updates=28500, lr=0.000143, gnorm=0.165, loss_scale=4, train_wall=139, gb_free=15.4, wall=41400
2023-10-17 10:48:58 | INFO | train_inner | epoch 001:  28649 / 1830643 loss=3.516, ppl=11.44, wps=23437.5, ups=0.72, wpb=32768, bsz=32, num_updates=28600, lr=0.0001428, gnorm=0.166, loss_scale=8, train_wall=139, gb_free=15.4, wall=41540
2023-10-17 10:51:17 | INFO | train_inner | epoch 001:  28749 / 1830643 loss=3.634, ppl=12.41, wps=23674.7, ups=0.72, wpb=32768, bsz=32, num_updates=28700, lr=0.0001426, gnorm=0.147, loss_scale=8, train_wall=138, gb_free=15.4, wall=41678
2023-10-17 10:53:37 | INFO | train_inner | epoch 001:  28849 / 1830643 loss=3.447, ppl=10.91, wps=23454.2, ups=0.72, wpb=32768, bsz=32, num_updates=28800, lr=0.0001424, gnorm=0.154, loss_scale=8, train_wall=139, gb_free=15.4, wall=41818
2023-10-17 10:55:56 | INFO | train_inner | epoch 001:  28949 / 1830643 loss=3.344, ppl=10.16, wps=23481.8, ups=0.72, wpb=32768, bsz=32, num_updates=28900, lr=0.0001422, gnorm=0.173, loss_scale=8, train_wall=139, gb_free=15.4, wall=41957
2023-10-17 10:58:16 | INFO | train_inner | epoch 001:  29049 / 1830643 loss=3.48, ppl=11.16, wps=23390, ups=0.71, wpb=32768, bsz=32, num_updates=29000, lr=0.000142, gnorm=0.178, loss_scale=8, train_wall=140, gb_free=15.4, wall=42097
2023-10-17 11:00:37 | INFO | train_inner | epoch 001:  29149 / 1830643 loss=3.139, ppl=8.81, wps=23323.6, ups=0.71, wpb=32768, bsz=32, num_updates=29100, lr=0.0001418, gnorm=0.171, loss_scale=16, train_wall=140, gb_free=15.4, wall=42238
2023-10-17 11:01:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 11:02:56 | INFO | train_inner | epoch 001:  29250 / 1830643 loss=3.376, ppl=10.38, wps=23500.9, ups=0.72, wpb=32768, bsz=32, num_updates=29200, lr=0.0001416, gnorm=0.159, loss_scale=8, train_wall=139, gb_free=15.4, wall=42377
2023-10-17 11:05:15 | INFO | train_inner | epoch 001:  29350 / 1830643 loss=3.438, ppl=10.84, wps=23658.6, ups=0.72, wpb=32768, bsz=32, num_updates=29300, lr=0.0001414, gnorm=0.156, loss_scale=8, train_wall=138, gb_free=15.4, wall=42516
2023-10-17 11:07:36 | INFO | train_inner | epoch 001:  29450 / 1830643 loss=3.562, ppl=11.81, wps=23254.8, ups=0.71, wpb=32768, bsz=32, num_updates=29400, lr=0.0001412, gnorm=0.161, loss_scale=8, train_wall=141, gb_free=15.4, wall=42657
2023-10-17 11:09:54 | INFO | train_inner | epoch 001:  29550 / 1830643 loss=3.441, ppl=10.86, wps=23658.7, ups=0.72, wpb=32768, bsz=32, num_updates=29500, lr=0.000141, gnorm=0.162, loss_scale=8, train_wall=138, gb_free=15.4, wall=42795
2023-10-17 11:12:14 | INFO | train_inner | epoch 001:  29650 / 1830643 loss=3.365, ppl=10.31, wps=23457.3, ups=0.72, wpb=32768, bsz=32, num_updates=29600, lr=0.0001408, gnorm=0.162, loss_scale=8, train_wall=139, gb_free=15.4, wall=42935
2023-10-17 11:14:33 | INFO | train_inner | epoch 001:  29750 / 1830643 loss=3.422, ppl=10.72, wps=23520.5, ups=0.72, wpb=32768, bsz=32, num_updates=29700, lr=0.0001406, gnorm=0.159, loss_scale=16, train_wall=139, gb_free=15.4, wall=43074
2023-10-17 11:16:55 | INFO | train_inner | epoch 001:  29850 / 1830643 loss=3.531, ppl=11.56, wps=23155.7, ups=0.71, wpb=32768, bsz=32, num_updates=29800, lr=0.0001404, gnorm=0.162, loss_scale=16, train_wall=141, gb_free=15.4, wall=43216
2023-10-17 11:19:15 | INFO | train_inner | epoch 001:  29950 / 1830643 loss=3.442, ppl=10.87, wps=23359.1, ups=0.71, wpb=32768, bsz=32, num_updates=29900, lr=0.0001402, gnorm=0.153, loss_scale=16, train_wall=140, gb_free=15.4, wall=43356
2023-10-17 11:21:38 | INFO | train_inner | epoch 001:  30050 / 1830643 loss=3.611, ppl=12.22, wps=22968.9, ups=0.7, wpb=32768, bsz=32, num_updates=30000, lr=0.00014, gnorm=0.161, loss_scale=16, train_wall=142, gb_free=15.4, wall=43499
2023-10-17 11:21:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30000 updates
2023-10-17 11:21:38 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_30000.pt
2023-10-17 11:21:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_30000.pt
2023-10-17 11:22:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_30000.pt (epoch 1 @ 30000 updates, score None) (writing took 22.882056961068884 seconds)
2023-10-17 11:24:42 | INFO | train_inner | epoch 001:  30150 / 1830643 loss=3.486, ppl=11.21, wps=17727.4, ups=0.54, wpb=32768, bsz=32, num_updates=30100, lr=0.0001398, gnorm=0.16, loss_scale=16, train_wall=162, gb_free=15.4, wall=43684
2023-10-17 11:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 11:27:14 | INFO | train_inner | epoch 001:  30251 / 1830643 loss=3.469, ppl=11.07, wps=21635.6, ups=0.66, wpb=32768, bsz=32, num_updates=30200, lr=0.0001396, gnorm=0.166, loss_scale=16, train_wall=151, gb_free=15.4, wall=43835
2023-10-17 11:29:36 | INFO | train_inner | epoch 001:  30351 / 1830643 loss=3.618, ppl=12.28, wps=22981.4, ups=0.7, wpb=32768, bsz=32, num_updates=30300, lr=0.0001394, gnorm=0.148, loss_scale=16, train_wall=142, gb_free=15.4, wall=43978
2023-10-17 11:32:00 | INFO | train_inner | epoch 001:  30451 / 1830643 loss=3.334, ppl=10.08, wps=22893.7, ups=0.7, wpb=32768, bsz=32, num_updates=30400, lr=0.0001392, gnorm=0.155, loss_scale=16, train_wall=143, gb_free=15.4, wall=44121
2023-10-17 11:34:25 | INFO | train_inner | epoch 001:  30551 / 1830643 loss=3.415, ppl=10.67, wps=22568, ups=0.69, wpb=32768, bsz=32, num_updates=30500, lr=0.000139, gnorm=0.162, loss_scale=16, train_wall=145, gb_free=15.4, wall=44266
2023-10-17 11:36:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 11:36:50 | INFO | train_inner | epoch 001:  30652 / 1830643 loss=3.496, ppl=11.28, wps=22588.9, ups=0.69, wpb=32768, bsz=32, num_updates=30600, lr=0.0001388, gnorm=0.16, loss_scale=8, train_wall=145, gb_free=15.4, wall=44411
2023-10-17 11:39:14 | INFO | train_inner | epoch 001:  30752 / 1830643 loss=3.432, ppl=10.79, wps=22758.7, ups=0.69, wpb=32768, bsz=32, num_updates=30700, lr=0.0001386, gnorm=0.152, loss_scale=8, train_wall=144, gb_free=15.4, wall=44555
2023-10-17 11:41:38 | INFO | train_inner | epoch 001:  30852 / 1830643 loss=3.463, ppl=11.03, wps=22671.8, ups=0.69, wpb=32768, bsz=32, num_updates=30800, lr=0.0001384, gnorm=0.172, loss_scale=8, train_wall=144, gb_free=15.4, wall=44700
2023-10-17 11:44:03 | INFO | train_inner | epoch 001:  30952 / 1830643 loss=3.335, ppl=10.09, wps=22629.5, ups=0.69, wpb=32768, bsz=32, num_updates=30900, lr=0.0001382, gnorm=0.164, loss_scale=8, train_wall=144, gb_free=15.4, wall=44844
2023-10-17 11:46:27 | INFO | train_inner | epoch 001:  31052 / 1830643 loss=3.168, ppl=8.99, wps=22746.3, ups=0.69, wpb=32768, bsz=32, num_updates=31000, lr=0.000138, gnorm=0.158, loss_scale=8, train_wall=144, gb_free=15.4, wall=44988
2023-10-17 11:48:52 | INFO | train_inner | epoch 001:  31152 / 1830643 loss=3.322, ppl=10, wps=22655.6, ups=0.69, wpb=32768, bsz=32, num_updates=31100, lr=0.0001378, gnorm=0.159, loss_scale=8, train_wall=144, gb_free=15.4, wall=45133
2023-10-17 11:51:17 | INFO | train_inner | epoch 001:  31252 / 1830643 loss=3.611, ppl=12.22, wps=22561.6, ups=0.69, wpb=32768, bsz=32, num_updates=31200, lr=0.0001376, gnorm=0.161, loss_scale=16, train_wall=145, gb_free=15.4, wall=45278
2023-10-17 11:53:43 | INFO | train_inner | epoch 001:  31352 / 1830643 loss=3.291, ppl=9.79, wps=22508.4, ups=0.69, wpb=32768, bsz=32, num_updates=31300, lr=0.0001374, gnorm=0.157, loss_scale=16, train_wall=145, gb_free=15.4, wall=45424
2023-10-17 11:54:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 11:56:08 | INFO | train_inner | epoch 001:  31453 / 1830643 loss=3.548, ppl=11.69, wps=22471.9, ups=0.69, wpb=32768, bsz=32, num_updates=31400, lr=0.0001372, gnorm=0.164, loss_scale=8, train_wall=145, gb_free=15.4, wall=45570
2023-10-17 11:58:33 | INFO | train_inner | epoch 001:  31553 / 1830643 loss=3.53, ppl=11.55, wps=22618, ups=0.69, wpb=32768, bsz=32, num_updates=31500, lr=0.000137, gnorm=0.16, loss_scale=8, train_wall=145, gb_free=15.4, wall=45715
2023-10-17 12:00:58 | INFO | train_inner | epoch 001:  31653 / 1830643 loss=3.542, ppl=11.65, wps=22684.4, ups=0.69, wpb=32768, bsz=32, num_updates=31600, lr=0.0001368, gnorm=0.158, loss_scale=8, train_wall=144, gb_free=15.4, wall=45859
2023-10-17 12:03:22 | INFO | train_inner | epoch 001:  31753 / 1830643 loss=3.636, ppl=12.43, wps=22737.3, ups=0.69, wpb=32768, bsz=32, num_updates=31700, lr=0.0001366, gnorm=0.171, loss_scale=8, train_wall=144, gb_free=15.4, wall=46003
2023-10-17 12:05:46 | INFO | train_inner | epoch 001:  31853 / 1830643 loss=3.47, ppl=11.08, wps=22687.7, ups=0.69, wpb=32768, bsz=32, num_updates=31800, lr=0.0001364, gnorm=0.154, loss_scale=8, train_wall=144, gb_free=15.4, wall=46148
2023-10-17 12:08:11 | INFO | train_inner | epoch 001:  31953 / 1830643 loss=3.49, ppl=11.23, wps=22629.3, ups=0.69, wpb=32768, bsz=32, num_updates=31900, lr=0.0001362, gnorm=0.166, loss_scale=16, train_wall=144, gb_free=15.4, wall=46292
2023-10-17 12:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 12:10:37 | INFO | train_inner | epoch 001:  32054 / 1830643 loss=3.45, ppl=10.93, wps=22478.9, ups=0.69, wpb=32768, bsz=32, num_updates=32000, lr=0.000136, gnorm=0.165, loss_scale=8, train_wall=145, gb_free=15.4, wall=46438
2023-10-17 12:13:01 | INFO | train_inner | epoch 001:  32154 / 1830643 loss=3.581, ppl=11.97, wps=22703.8, ups=0.69, wpb=32768, bsz=32, num_updates=32100, lr=0.0001358, gnorm=0.156, loss_scale=8, train_wall=144, gb_free=15.4, wall=46582
2023-10-17 12:15:26 | INFO | train_inner | epoch 001:  32254 / 1830643 loss=3.528, ppl=11.54, wps=22659.9, ups=0.69, wpb=32768, bsz=32, num_updates=32200, lr=0.0001356, gnorm=0.159, loss_scale=8, train_wall=144, gb_free=15.4, wall=46727
2023-10-17 12:17:50 | INFO | train_inner | epoch 001:  32354 / 1830643 loss=3.454, ppl=10.96, wps=22700.8, ups=0.69, wpb=32768, bsz=32, num_updates=32300, lr=0.0001354, gnorm=0.158, loss_scale=8, train_wall=144, gb_free=15.4, wall=46871
2023-10-17 12:20:15 | INFO | train_inner | epoch 001:  32454 / 1830643 loss=3.563, ppl=11.82, wps=22661.5, ups=0.69, wpb=32768, bsz=32, num_updates=32400, lr=0.0001352, gnorm=0.156, loss_scale=8, train_wall=144, gb_free=15.4, wall=47016
2023-10-17 12:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 12:22:41 | INFO | train_inner | epoch 001:  32555 / 1830643 loss=3.444, ppl=10.88, wps=22391.9, ups=0.68, wpb=32768, bsz=32, num_updates=32500, lr=0.000135, gnorm=0.162, loss_scale=8, train_wall=146, gb_free=15.4, wall=47162
2023-10-17 12:25:06 | INFO | train_inner | epoch 001:  32655 / 1830643 loss=3.447, ppl=10.91, wps=22682.1, ups=0.69, wpb=32768, bsz=32, num_updates=32600, lr=0.0001348, gnorm=0.165, loss_scale=8, train_wall=144, gb_free=15.4, wall=47307
2023-10-17 12:27:31 | INFO | train_inner | epoch 001:  32755 / 1830643 loss=3.304, ppl=9.88, wps=22543.2, ups=0.69, wpb=32768, bsz=32, num_updates=32700, lr=0.0001346, gnorm=0.151, loss_scale=8, train_wall=145, gb_free=15.4, wall=47452
2023-10-17 12:29:56 | INFO | train_inner | epoch 001:  32855 / 1830643 loss=3.459, ppl=11, wps=22560.2, ups=0.69, wpb=32768, bsz=32, num_updates=32800, lr=0.0001344, gnorm=0.158, loss_scale=8, train_wall=145, gb_free=15.4, wall=47597
2023-10-17 12:32:21 | INFO | train_inner | epoch 001:  32955 / 1830643 loss=3.516, ppl=11.44, wps=22658.5, ups=0.69, wpb=32768, bsz=32, num_updates=32900, lr=0.0001342, gnorm=0.173, loss_scale=8, train_wall=144, gb_free=15.4, wall=47742
2023-10-17 12:34:46 | INFO | train_inner | epoch 001:  33055 / 1830643 loss=3.408, ppl=10.62, wps=22633, ups=0.69, wpb=32768, bsz=32, num_updates=33000, lr=0.000134, gnorm=0.158, loss_scale=16, train_wall=144, gb_free=15.4, wall=47887
2023-10-17 12:37:10 | INFO | train_inner | epoch 001:  33155 / 1830643 loss=3.548, ppl=11.7, wps=22669.1, ups=0.69, wpb=32768, bsz=32, num_updates=33100, lr=0.0001338, gnorm=0.157, loss_scale=16, train_wall=144, gb_free=15.4, wall=48031
2023-10-17 12:39:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 12:39:36 | INFO | train_inner | epoch 001:  33256 / 1830643 loss=3.469, ppl=11.08, wps=22409.9, ups=0.68, wpb=32768, bsz=32, num_updates=33200, lr=0.0001336, gnorm=0.165, loss_scale=8, train_wall=146, gb_free=15.4, wall=48178
2023-10-17 12:42:01 | INFO | train_inner | epoch 001:  33356 / 1830643 loss=3.397, ppl=10.53, wps=22724.4, ups=0.69, wpb=32768, bsz=32, num_updates=33300, lr=0.0001334, gnorm=0.155, loss_scale=8, train_wall=144, gb_free=15.4, wall=48322
2023-10-17 12:44:25 | INFO | train_inner | epoch 001:  33456 / 1830643 loss=3.357, ppl=10.25, wps=22713.2, ups=0.69, wpb=32768, bsz=32, num_updates=33400, lr=0.0001332, gnorm=0.154, loss_scale=8, train_wall=144, gb_free=15.4, wall=48466
2023-10-17 12:45:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 12:45:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 12:46:52 | INFO | train_inner | epoch 001:  33558 / 1830643 loss=3.614, ppl=12.24, wps=22308.8, ups=0.68, wpb=32768, bsz=32, num_updates=33500, lr=0.000133, gnorm=0.183, loss_scale=2, train_wall=147, gb_free=15.4, wall=48613
2023-10-17 12:49:16 | INFO | train_inner | epoch 001:  33658 / 1830643 loss=3.537, ppl=11.6, wps=22653.1, ups=0.69, wpb=32768, bsz=32, num_updates=33600, lr=0.0001328, gnorm=0.173, loss_scale=2, train_wall=144, gb_free=15.4, wall=48758
2023-10-17 12:51:41 | INFO | train_inner | epoch 001:  33758 / 1830643 loss=3.46, ppl=11.01, wps=22633.6, ups=0.69, wpb=32768, bsz=32, num_updates=33700, lr=0.0001326, gnorm=0.154, loss_scale=2, train_wall=144, gb_free=15.4, wall=48902
2023-10-17 12:54:06 | INFO | train_inner | epoch 001:  33858 / 1830643 loss=3.39, ppl=10.48, wps=22653.5, ups=0.69, wpb=32768, bsz=32, num_updates=33800, lr=0.0001324, gnorm=0.16, loss_scale=2, train_wall=144, gb_free=15.4, wall=49047
2023-10-17 12:56:28 | INFO | train_inner | epoch 001:  33958 / 1830643 loss=3.349, ppl=10.19, wps=23069.6, ups=0.7, wpb=32768, bsz=32, num_updates=33900, lr=0.0001322, gnorm=0.152, loss_scale=2, train_wall=142, gb_free=15.4, wall=49189
2023-10-17 12:58:51 | INFO | train_inner | epoch 001:  34058 / 1830643 loss=3.396, ppl=10.53, wps=22938.1, ups=0.7, wpb=32768, bsz=32, num_updates=34000, lr=0.000132, gnorm=0.17, loss_scale=4, train_wall=142, gb_free=15.4, wall=49332
2023-10-17 13:01:13 | INFO | train_inner | epoch 001:  34158 / 1830643 loss=3.334, ppl=10.09, wps=23033.1, ups=0.7, wpb=32768, bsz=32, num_updates=34100, lr=0.0001318, gnorm=0.159, loss_scale=4, train_wall=142, gb_free=15.4, wall=49474
2023-10-17 13:03:36 | INFO | train_inner | epoch 001:  34258 / 1830643 loss=3.523, ppl=11.49, wps=22908.5, ups=0.7, wpb=32768, bsz=32, num_updates=34200, lr=0.0001316, gnorm=0.164, loss_scale=4, train_wall=143, gb_free=15.4, wall=49617
2023-10-17 13:06:00 | INFO | train_inner | epoch 001:  34358 / 1830643 loss=3.47, ppl=11.08, wps=22827, ups=0.7, wpb=32768, bsz=32, num_updates=34300, lr=0.0001314, gnorm=0.166, loss_scale=4, train_wall=143, gb_free=15.4, wall=49761
2023-10-17 13:08:23 | INFO | train_inner | epoch 001:  34458 / 1830643 loss=3.5, ppl=11.32, wps=22882.2, ups=0.7, wpb=32768, bsz=32, num_updates=34400, lr=0.0001312, gnorm=0.159, loss_scale=4, train_wall=143, gb_free=15.4, wall=49904
2023-10-17 13:10:46 | INFO | train_inner | epoch 001:  34558 / 1830643 loss=3.5, ppl=11.31, wps=22820.2, ups=0.7, wpb=32768, bsz=32, num_updates=34500, lr=0.000131, gnorm=0.17, loss_scale=8, train_wall=143, gb_free=15.4, wall=50048
2023-10-17 13:13:10 | INFO | train_inner | epoch 001:  34658 / 1830643 loss=3.483, ppl=11.18, wps=22818.9, ups=0.7, wpb=32768, bsz=32, num_updates=34600, lr=0.0001308, gnorm=0.16, loss_scale=8, train_wall=143, gb_free=15.4, wall=50191
2023-10-17 13:15:34 | INFO | train_inner | epoch 001:  34758 / 1830643 loss=3.56, ppl=11.79, wps=22830, ups=0.7, wpb=32768, bsz=32, num_updates=34700, lr=0.0001306, gnorm=0.156, loss_scale=8, train_wall=143, gb_free=15.4, wall=50335
2023-10-17 13:17:57 | INFO | train_inner | epoch 001:  34858 / 1830643 loss=3.737, ppl=13.34, wps=22889.9, ups=0.7, wpb=32768, bsz=32, num_updates=34800, lr=0.0001304, gnorm=0.165, loss_scale=8, train_wall=143, gb_free=15.4, wall=50478
2023-10-17 13:20:20 | INFO | train_inner | epoch 001:  34958 / 1830643 loss=3.445, ppl=10.89, wps=22924.5, ups=0.7, wpb=32768, bsz=32, num_updates=34900, lr=0.0001302, gnorm=0.151, loss_scale=8, train_wall=143, gb_free=15.4, wall=50621
2023-10-17 13:22:43 | INFO | train_inner | epoch 001:  35058 / 1830643 loss=3.446, ppl=10.9, wps=22798.9, ups=0.7, wpb=32768, bsz=32, num_updates=35000, lr=0.00013, gnorm=0.149, loss_scale=16, train_wall=143, gb_free=15.4, wall=50764
2023-10-17 13:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 13:25:09 | INFO | train_inner | epoch 001:  35159 / 1830643 loss=3.435, ppl=10.81, wps=22553.3, ups=0.69, wpb=32768, bsz=32, num_updates=35100, lr=0.0001298, gnorm=0.156, loss_scale=8, train_wall=145, gb_free=15.4, wall=50910
2023-10-17 13:27:33 | INFO | train_inner | epoch 001:  35259 / 1830643 loss=3.26, ppl=9.58, wps=22775, ups=0.7, wpb=32768, bsz=32, num_updates=35200, lr=0.0001296, gnorm=0.16, loss_scale=8, train_wall=144, gb_free=15.4, wall=51054
2023-10-17 13:29:57 | INFO | train_inner | epoch 001:  35359 / 1830643 loss=3.465, ppl=11.04, wps=22644.8, ups=0.69, wpb=32768, bsz=32, num_updates=35300, lr=0.0001294, gnorm=0.158, loss_scale=8, train_wall=144, gb_free=15.4, wall=51198
2023-10-17 13:32:21 | INFO | train_inner | epoch 001:  35459 / 1830643 loss=3.246, ppl=9.49, wps=22710.7, ups=0.69, wpb=32768, bsz=32, num_updates=35400, lr=0.0001292, gnorm=0.168, loss_scale=8, train_wall=144, gb_free=15.4, wall=51343
2023-10-17 13:34:45 | INFO | train_inner | epoch 001:  35559 / 1830643 loss=3.472, ppl=11.09, wps=22814.7, ups=0.7, wpb=32768, bsz=32, num_updates=35500, lr=0.000129, gnorm=0.157, loss_scale=8, train_wall=143, gb_free=15.4, wall=51486
2023-10-17 13:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 13:37:10 | INFO | train_inner | epoch 001:  35660 / 1830643 loss=3.38, ppl=10.41, wps=22582.7, ups=0.69, wpb=32768, bsz=32, num_updates=35600, lr=0.0001288, gnorm=0.165, loss_scale=4, train_wall=145, gb_free=15.4, wall=51631
2023-10-17 13:39:34 | INFO | train_inner | epoch 001:  35760 / 1830643 loss=3.394, ppl=10.51, wps=22791, ups=0.7, wpb=32768, bsz=32, num_updates=35700, lr=0.0001286, gnorm=0.145, loss_scale=4, train_wall=143, gb_free=15.4, wall=51775
2023-10-17 13:41:59 | INFO | train_inner | epoch 001:  35860 / 1830643 loss=3.626, ppl=12.35, wps=22584.9, ups=0.69, wpb=32768, bsz=32, num_updates=35800, lr=0.0001284, gnorm=0.164, loss_scale=4, train_wall=145, gb_free=15.4, wall=51920
2023-10-17 13:44:23 | INFO | train_inner | epoch 001:  35960 / 1830643 loss=3.551, ppl=11.72, wps=22696.1, ups=0.69, wpb=32768, bsz=32, num_updates=35900, lr=0.0001282, gnorm=0.16, loss_scale=4, train_wall=144, gb_free=15.4, wall=52065
2023-10-17 13:46:48 | INFO | train_inner | epoch 001:  36060 / 1830643 loss=3.276, ppl=9.68, wps=22707, ups=0.69, wpb=32768, bsz=32, num_updates=36000, lr=0.000128, gnorm=0.144, loss_scale=4, train_wall=144, gb_free=15.4, wall=52209
2023-10-17 13:49:12 | INFO | train_inner | epoch 001:  36160 / 1830643 loss=3.572, ppl=11.89, wps=22702.8, ups=0.69, wpb=32768, bsz=32, num_updates=36100, lr=0.0001278, gnorm=0.169, loss_scale=8, train_wall=144, gb_free=15.4, wall=52353
2023-10-17 13:51:37 | INFO | train_inner | epoch 001:  36260 / 1830643 loss=3.432, ppl=10.79, wps=22666.3, ups=0.69, wpb=32768, bsz=32, num_updates=36200, lr=0.0001276, gnorm=0.166, loss_scale=8, train_wall=144, gb_free=15.4, wall=52498
2023-10-17 13:54:02 | INFO | train_inner | epoch 001:  36360 / 1830643 loss=3.553, ppl=11.74, wps=22606.4, ups=0.69, wpb=32768, bsz=32, num_updates=36300, lr=0.0001274, gnorm=0.163, loss_scale=8, train_wall=145, gb_free=15.4, wall=52643
2023-10-17 13:56:26 | INFO | train_inner | epoch 001:  36460 / 1830643 loss=3.27, ppl=9.65, wps=22698, ups=0.69, wpb=32768, bsz=32, num_updates=36400, lr=0.0001272, gnorm=0.161, loss_scale=8, train_wall=144, gb_free=15.4, wall=52787
2023-10-17 13:58:50 | INFO | train_inner | epoch 001:  36560 / 1830643 loss=3.515, ppl=11.43, wps=22712.2, ups=0.69, wpb=32768, bsz=32, num_updates=36500, lr=0.000127, gnorm=0.149, loss_scale=8, train_wall=144, gb_free=15.4, wall=52931
2023-10-17 14:01:15 | INFO | train_inner | epoch 001:  36660 / 1830643 loss=3.539, ppl=11.62, wps=22679, ups=0.69, wpb=32768, bsz=32, num_updates=36600, lr=0.0001268, gnorm=0.159, loss_scale=16, train_wall=144, gb_free=15.4, wall=53076
2023-10-17 14:03:39 | INFO | train_inner | epoch 001:  36760 / 1830643 loss=3.507, ppl=11.37, wps=22735.4, ups=0.69, wpb=32768, bsz=32, num_updates=36700, lr=0.0001266, gnorm=0.151, loss_scale=16, train_wall=144, gb_free=15.4, wall=53220
2023-10-17 14:06:03 | INFO | train_inner | epoch 001:  36860 / 1830643 loss=3.437, ppl=10.83, wps=22770.8, ups=0.69, wpb=32768, bsz=32, num_updates=36800, lr=0.0001264, gnorm=0.15, loss_scale=16, train_wall=144, gb_free=15.4, wall=53364
2023-10-17 14:06:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 14:08:28 | INFO | train_inner | epoch 001:  36961 / 1830643 loss=3.385, ppl=10.45, wps=22526.2, ups=0.69, wpb=32768, bsz=32, num_updates=36900, lr=0.0001262, gnorm=0.17, loss_scale=8, train_wall=145, gb_free=15.4, wall=53509
2023-10-17 14:10:53 | INFO | train_inner | epoch 001:  37061 / 1830643 loss=3.274, ppl=9.67, wps=22655.1, ups=0.69, wpb=32768, bsz=32, num_updates=37000, lr=0.000126, gnorm=0.162, loss_scale=8, train_wall=144, gb_free=15.4, wall=53654
2023-10-17 14:13:16 | INFO | train_inner | epoch 001:  37161 / 1830643 loss=3.407, ppl=10.61, wps=22837.5, ups=0.7, wpb=32768, bsz=32, num_updates=37100, lr=0.0001258, gnorm=0.154, loss_scale=8, train_wall=143, gb_free=15.4, wall=53798
2023-10-17 14:15:40 | INFO | train_inner | epoch 001:  37261 / 1830643 loss=3.568, ppl=11.86, wps=22834.3, ups=0.7, wpb=32768, bsz=32, num_updates=37200, lr=0.0001256, gnorm=0.154, loss_scale=8, train_wall=143, gb_free=15.4, wall=53941
2023-10-17 14:18:05 | INFO | train_inner | epoch 001:  37361 / 1830643 loss=3.359, ppl=10.26, wps=22572.6, ups=0.69, wpb=32768, bsz=32, num_updates=37300, lr=0.0001254, gnorm=0.155, loss_scale=8, train_wall=145, gb_free=15.4, wall=54086
2023-10-17 14:20:29 | INFO | train_inner | epoch 001:  37461 / 1830643 loss=3.493, ppl=11.26, wps=22784, ups=0.7, wpb=32768, bsz=32, num_updates=37400, lr=0.0001252, gnorm=0.155, loss_scale=16, train_wall=143, gb_free=15.4, wall=54230
2023-10-17 14:22:53 | INFO | train_inner | epoch 001:  37561 / 1830643 loss=3.346, ppl=10.17, wps=22671.6, ups=0.69, wpb=32768, bsz=32, num_updates=37500, lr=0.000125, gnorm=0.156, loss_scale=16, train_wall=144, gb_free=15.4, wall=54375
2023-10-17 14:25:18 | INFO | train_inner | epoch 001:  37661 / 1830643 loss=3.575, ppl=11.91, wps=22706.7, ups=0.69, wpb=32768, bsz=32, num_updates=37600, lr=0.0001248, gnorm=0.154, loss_scale=16, train_wall=144, gb_free=15.4, wall=54519
2023-10-17 14:27:42 | INFO | train_inner | epoch 001:  37761 / 1830643 loss=3.704, ppl=13.03, wps=22763.3, ups=0.69, wpb=32768, bsz=32, num_updates=37700, lr=0.0001246, gnorm=0.166, loss_scale=16, train_wall=144, gb_free=15.4, wall=54663
2023-10-17 14:30:06 | INFO | train_inner | epoch 001:  37861 / 1830643 loss=3.355, ppl=10.23, wps=22729.2, ups=0.69, wpb=32768, bsz=32, num_updates=37800, lr=0.0001244, gnorm=0.154, loss_scale=16, train_wall=144, gb_free=15.4, wall=54807
2023-10-17 14:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 14:32:34 | INFO | train_inner | epoch 001:  37962 / 1830643 loss=3.395, ppl=10.52, wps=22171.6, ups=0.68, wpb=32768, bsz=32, num_updates=37900, lr=0.0001242, gnorm=0.153, loss_scale=16, train_wall=147, gb_free=15.4, wall=54955
2023-10-17 14:34:57 | INFO | train_inner | epoch 001:  38062 / 1830643 loss=3.536, ppl=11.6, wps=22910.5, ups=0.7, wpb=32768, bsz=32, num_updates=38000, lr=0.000124, gnorm=0.15, loss_scale=16, train_wall=143, gb_free=15.4, wall=55098
2023-10-17 14:37:20 | INFO | train_inner | epoch 001:  38162 / 1830643 loss=3.536, ppl=11.6, wps=22793.9, ups=0.7, wpb=32768, bsz=32, num_updates=38100, lr=0.0001238, gnorm=0.156, loss_scale=16, train_wall=143, gb_free=15.4, wall=55242
2023-10-17 14:39:45 | INFO | train_inner | epoch 001:  38262 / 1830643 loss=3.498, ppl=11.3, wps=22737.4, ups=0.69, wpb=32768, bsz=32, num_updates=38200, lr=0.0001236, gnorm=0.151, loss_scale=16, train_wall=144, gb_free=15.4, wall=55386
2023-10-17 14:42:08 | INFO | train_inner | epoch 001:  38362 / 1830643 loss=3.55, ppl=11.71, wps=22775.9, ups=0.7, wpb=32768, bsz=32, num_updates=38300, lr=0.0001234, gnorm=0.162, loss_scale=16, train_wall=144, gb_free=15.4, wall=55530
2023-10-17 14:43:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 14:44:34 | INFO | train_inner | epoch 001:  38463 / 1830643 loss=3.285, ppl=9.75, wps=22515.8, ups=0.69, wpb=32768, bsz=32, num_updates=38400, lr=0.0001232, gnorm=0.147, loss_scale=16, train_wall=145, gb_free=15.4, wall=55675
2023-10-17 14:44:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 14:44:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 14:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-17 14:47:03 | INFO | train_inner | epoch 001:  38566 / 1830643 loss=3.469, ppl=11.07, wps=22011.2, ups=0.67, wpb=32768, bsz=32, num_updates=38500, lr=0.000123, gnorm=0.152, loss_scale=2, train_wall=149, gb_free=15.4, wall=55824
2023-10-17 14:49:27 | INFO | train_inner | epoch 001:  38666 / 1830643 loss=3.664, ppl=12.68, wps=22743.8, ups=0.69, wpb=32768, bsz=32, num_updates=38600, lr=0.0001228, gnorm=0.153, loss_scale=2, train_wall=144, gb_free=15.4, wall=55968
2023-10-17 14:51:51 | INFO | train_inner | epoch 001:  38766 / 1830643 loss=3.381, ppl=10.41, wps=22718.1, ups=0.69, wpb=32768, bsz=32, num_updates=38700, lr=0.0001226, gnorm=0.161, loss_scale=2, train_wall=144, gb_free=15.4, wall=56112
2023-10-17 14:54:15 | INFO | train_inner | epoch 001:  38866 / 1830643 loss=3.192, ppl=9.14, wps=22710.6, ups=0.69, wpb=32768, bsz=32, num_updates=38800, lr=0.0001224, gnorm=0.151, loss_scale=2, train_wall=144, gb_free=15.4, wall=56257
2023-10-17 14:56:39 | INFO | train_inner | epoch 001:  38966 / 1830643 loss=3.551, ppl=11.72, wps=22763.5, ups=0.69, wpb=32768, bsz=32, num_updates=38900, lr=0.0001222, gnorm=0.163, loss_scale=2, train_wall=144, gb_free=15.4, wall=56401
2023-10-17 14:59:04 | INFO | train_inner | epoch 001:  39066 / 1830643 loss=3.895, ppl=14.87, wps=22731.4, ups=0.69, wpb=32768, bsz=32, num_updates=39000, lr=0.000122, gnorm=0.163, loss_scale=4, train_wall=144, gb_free=15.4, wall=56545
2023-10-17 15:01:28 | INFO | train_inner | epoch 001:  39166 / 1830643 loss=3.53, ppl=11.55, wps=22671.2, ups=0.69, wpb=32768, bsz=32, num_updates=39100, lr=0.0001218, gnorm=0.163, loss_scale=4, train_wall=144, gb_free=15.4, wall=56689
2023-10-17 15:03:52 | INFO | train_inner | epoch 001:  39266 / 1830643 loss=3.573, ppl=11.9, wps=22704.8, ups=0.69, wpb=32768, bsz=32, num_updates=39200, lr=0.0001216, gnorm=0.164, loss_scale=4, train_wall=144, gb_free=15.4, wall=56834
2023-10-17 15:06:16 | INFO | train_inner | epoch 001:  39366 / 1830643 loss=3.329, ppl=10.05, wps=22750.3, ups=0.69, wpb=32768, bsz=32, num_updates=39300, lr=0.0001214, gnorm=0.151, loss_scale=4, train_wall=144, gb_free=15.4, wall=56978
2023-10-17 15:08:41 | INFO | train_inner | epoch 001:  39466 / 1830643 loss=3.439, ppl=10.85, wps=22728.9, ups=0.69, wpb=32768, bsz=32, num_updates=39400, lr=0.0001212, gnorm=0.15, loss_scale=4, train_wall=144, gb_free=15.4, wall=57122
2023-10-17 15:11:04 | INFO | train_inner | epoch 001:  39566 / 1830643 loss=3.498, ppl=11.3, wps=22785.7, ups=0.7, wpb=32768, bsz=32, num_updates=39500, lr=0.000121, gnorm=0.154, loss_scale=8, train_wall=143, gb_free=15.4, wall=57266
2023-10-17 15:13:28 | INFO | train_inner | epoch 001:  39666 / 1830643 loss=3.395, ppl=10.52, wps=22779.8, ups=0.7, wpb=32768, bsz=32, num_updates=39600, lr=0.0001208, gnorm=0.148, loss_scale=8, train_wall=143, gb_free=15.4, wall=57409
2023-10-17 15:15:53 | INFO | train_inner | epoch 001:  39766 / 1830643 loss=3.559, ppl=11.79, wps=22635, ups=0.69, wpb=32768, bsz=32, num_updates=39700, lr=0.0001206, gnorm=0.156, loss_scale=8, train_wall=144, gb_free=15.4, wall=57554
2023-10-17 15:18:17 | INFO | train_inner | epoch 001:  39866 / 1830643 loss=3.415, ppl=10.67, wps=22760.5, ups=0.69, wpb=32768, bsz=32, num_updates=39800, lr=0.0001204, gnorm=0.159, loss_scale=8, train_wall=144, gb_free=15.4, wall=57698
2023-10-17 15:20:41 | INFO | train_inner | epoch 001:  39966 / 1830643 loss=3.51, ppl=11.4, wps=22705.2, ups=0.69, wpb=32768, bsz=32, num_updates=39900, lr=0.0001202, gnorm=0.166, loss_scale=8, train_wall=144, gb_free=15.4, wall=57842
2023-10-17 15:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 15:23:07 | INFO | train_inner | epoch 001:  40067 / 1830643 loss=3.334, ppl=10.08, wps=22422.4, ups=0.68, wpb=32768, bsz=32, num_updates=40000, lr=0.00012, gnorm=0.155, loss_scale=8, train_wall=146, gb_free=15.4, wall=57989
2023-10-17 15:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 40000 updates
2023-10-17 15:23:07 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_40000.pt
2023-10-17 15:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_40000.pt
2023-10-17 15:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_40000.pt (epoch 1 @ 40000 updates, score None) (writing took 15.659605517052114 seconds)
2023-10-17 15:25:47 | INFO | train_inner | epoch 001:  40167 / 1830643 loss=3.361, ppl=10.28, wps=20519.9, ups=0.63, wpb=32768, bsz=32, num_updates=40100, lr=0.0001198, gnorm=0.162, loss_scale=8, train_wall=144, gb_free=15.4, wall=58148
2023-10-17 15:28:11 | INFO | train_inner | epoch 001:  40267 / 1830643 loss=3.549, ppl=11.71, wps=22740.2, ups=0.69, wpb=32768, bsz=32, num_updates=40200, lr=0.0001196, gnorm=0.152, loss_scale=8, train_wall=144, gb_free=15.4, wall=58292
2023-10-17 15:30:37 | INFO | train_inner | epoch 001:  40367 / 1830643 loss=3.362, ppl=10.28, wps=22510.6, ups=0.69, wpb=32768, bsz=32, num_updates=40300, lr=0.0001194, gnorm=0.16, loss_scale=8, train_wall=145, gb_free=15.4, wall=58438
2023-10-17 15:33:01 | INFO | train_inner | epoch 001:  40467 / 1830643 loss=3.432, ppl=10.79, wps=22725.1, ups=0.69, wpb=32768, bsz=32, num_updates=40400, lr=0.0001192, gnorm=0.154, loss_scale=8, train_wall=144, gb_free=15.4, wall=58582
2023-10-17 15:35:25 | INFO | train_inner | epoch 001:  40567 / 1830643 loss=3.557, ppl=11.77, wps=22719.2, ups=0.69, wpb=32768, bsz=32, num_updates=40500, lr=0.000119, gnorm=0.156, loss_scale=16, train_wall=144, gb_free=15.4, wall=58726
2023-10-17 15:37:50 | INFO | train_inner | epoch 001:  40667 / 1830643 loss=3.404, ppl=10.59, wps=22585.7, ups=0.69, wpb=32768, bsz=32, num_updates=40600, lr=0.0001188, gnorm=0.145, loss_scale=16, train_wall=145, gb_free=15.4, wall=58871
2023-10-17 15:40:15 | INFO | train_inner | epoch 001:  40767 / 1830643 loss=3.469, ppl=11.08, wps=22583.9, ups=0.69, wpb=32768, bsz=32, num_updates=40700, lr=0.0001186, gnorm=0.154, loss_scale=16, train_wall=145, gb_free=15.4, wall=59017
2023-10-17 15:42:40 | INFO | train_inner | epoch 001:  40867 / 1830643 loss=3.497, ppl=11.29, wps=22705.3, ups=0.69, wpb=32768, bsz=32, num_updates=40800, lr=0.0001184, gnorm=0.154, loss_scale=16, train_wall=144, gb_free=15.4, wall=59161
2023-10-17 15:45:04 | INFO | train_inner | epoch 001:  40967 / 1830643 loss=3.583, ppl=11.98, wps=22763.4, ups=0.69, wpb=32768, bsz=32, num_updates=40900, lr=0.0001182, gnorm=0.175, loss_scale=16, train_wall=144, gb_free=15.4, wall=59305
2023-10-17 15:46:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 15:47:29 | INFO | train_inner | epoch 001:  41068 / 1830643 loss=3.196, ppl=9.16, wps=22541.4, ups=0.69, wpb=32768, bsz=32, num_updates=41000, lr=0.000118, gnorm=0.161, loss_scale=8, train_wall=145, gb_free=15.4, wall=59450
2023-10-17 15:49:53 | INFO | train_inner | epoch 001:  41168 / 1830643 loss=3.468, ppl=11.06, wps=22758.2, ups=0.69, wpb=32768, bsz=32, num_updates=41100, lr=0.0001178, gnorm=0.152, loss_scale=8, train_wall=144, gb_free=15.4, wall=59594
2023-10-17 15:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 15:52:19 | INFO | train_inner | epoch 001:  41269 / 1830643 loss=3.368, ppl=10.33, wps=22508.1, ups=0.69, wpb=32768, bsz=32, num_updates=41200, lr=0.0001176, gnorm=0.15, loss_scale=4, train_wall=145, gb_free=15.4, wall=59740
2023-10-17 15:54:43 | INFO | train_inner | epoch 001:  41369 / 1830643 loss=3.616, ppl=12.26, wps=22700.7, ups=0.69, wpb=32768, bsz=32, num_updates=41300, lr=0.0001174, gnorm=0.159, loss_scale=4, train_wall=144, gb_free=15.4, wall=59884
2023-10-17 15:57:08 | INFO | train_inner | epoch 001:  41469 / 1830643 loss=3.332, ppl=10.07, wps=22631.4, ups=0.69, wpb=32768, bsz=32, num_updates=41400, lr=0.0001172, gnorm=0.159, loss_scale=4, train_wall=144, gb_free=15.4, wall=60029
2023-10-17 15:59:32 | INFO | train_inner | epoch 001:  41569 / 1830643 loss=3.479, ppl=11.15, wps=22670.1, ups=0.69, wpb=32768, bsz=32, num_updates=41500, lr=0.000117, gnorm=0.154, loss_scale=4, train_wall=144, gb_free=15.4, wall=60173
2023-10-17 16:01:56 | INFO | train_inner | epoch 001:  41669 / 1830643 loss=3.321, ppl=9.99, wps=22720, ups=0.69, wpb=32768, bsz=32, num_updates=41600, lr=0.0001168, gnorm=0.154, loss_scale=4, train_wall=144, gb_free=15.4, wall=60318
2023-10-17 16:04:20 | INFO | train_inner | epoch 001:  41769 / 1830643 loss=3.564, ppl=11.83, wps=22765.5, ups=0.69, wpb=32768, bsz=32, num_updates=41700, lr=0.0001166, gnorm=0.153, loss_scale=8, train_wall=144, gb_free=15.4, wall=60462
2023-10-17 16:06:44 | INFO | train_inner | epoch 001:  41869 / 1830643 loss=3.308, ppl=9.9, wps=22746.6, ups=0.69, wpb=32768, bsz=32, num_updates=41800, lr=0.0001164, gnorm=0.165, loss_scale=8, train_wall=144, gb_free=15.4, wall=60606
2023-10-17 16:09:07 | INFO | train_inner | epoch 001:  41969 / 1830643 loss=3.178, ppl=9.05, wps=23064.8, ups=0.7, wpb=32768, bsz=32, num_updates=41900, lr=0.0001162, gnorm=0.159, loss_scale=8, train_wall=142, gb_free=15.4, wall=60748
2023-10-17 16:11:34 | INFO | train_inner | epoch 001:  42069 / 1830643 loss=2.989, ppl=7.94, wps=22195, ups=0.68, wpb=32768, bsz=32, num_updates=42000, lr=0.000116, gnorm=0.155, loss_scale=8, train_wall=147, gb_free=15.4, wall=60895
2023-10-17 16:14:00 | INFO | train_inner | epoch 001:  42169 / 1830643 loss=3.074, ppl=8.42, wps=22488.9, ups=0.69, wpb=32768, bsz=32, num_updates=42100, lr=0.0001158, gnorm=0.153, loss_scale=8, train_wall=145, gb_free=15.4, wall=61041
2023-10-17 16:16:24 | INFO | train_inner | epoch 001:  42269 / 1830643 loss=3.072, ppl=8.41, wps=22812.6, ups=0.7, wpb=32768, bsz=32, num_updates=42200, lr=0.0001156, gnorm=0.158, loss_scale=16, train_wall=143, gb_free=15.4, wall=61185
2023-10-17 16:18:47 | INFO | train_inner | epoch 001:  42369 / 1830643 loss=3.295, ppl=9.81, wps=22773.2, ups=0.69, wpb=32768, bsz=32, num_updates=42300, lr=0.0001154, gnorm=0.162, loss_scale=16, train_wall=144, gb_free=15.4, wall=61329
2023-10-17 16:21:12 | INFO | train_inner | epoch 001:  42469 / 1830643 loss=3.596, ppl=12.09, wps=22715.3, ups=0.69, wpb=32768, bsz=32, num_updates=42400, lr=0.0001152, gnorm=0.157, loss_scale=16, train_wall=144, gb_free=15.4, wall=61473
2023-10-17 16:23:37 | INFO | train_inner | epoch 001:  42569 / 1830643 loss=3.383, ppl=10.43, wps=22626.9, ups=0.69, wpb=32768, bsz=32, num_updates=42500, lr=0.000115, gnorm=0.152, loss_scale=16, train_wall=144, gb_free=15.4, wall=61618
2023-10-17 16:26:01 | INFO | train_inner | epoch 001:  42669 / 1830643 loss=3.31, ppl=9.92, wps=22667.1, ups=0.69, wpb=32768, bsz=32, num_updates=42600, lr=0.0001148, gnorm=0.16, loss_scale=16, train_wall=144, gb_free=15.4, wall=61762
2023-10-17 16:27:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 16:28:27 | INFO | train_inner | epoch 001:  42770 / 1830643 loss=3.365, ppl=10.3, wps=22517.6, ups=0.69, wpb=32768, bsz=32, num_updates=42700, lr=0.0001146, gnorm=0.146, loss_scale=16, train_wall=145, gb_free=15.4, wall=61908
2023-10-17 16:30:51 | INFO | train_inner | epoch 001:  42870 / 1830643 loss=3.461, ppl=11.01, wps=22760, ups=0.69, wpb=32768, bsz=32, num_updates=42800, lr=0.0001144, gnorm=0.145, loss_scale=16, train_wall=144, gb_free=15.4, wall=62052
2023-10-17 16:33:15 | INFO | train_inner | epoch 001:  42970 / 1830643 loss=3.587, ppl=12.02, wps=22736.9, ups=0.69, wpb=32768, bsz=32, num_updates=42900, lr=0.0001142, gnorm=0.152, loss_scale=16, train_wall=144, gb_free=15.4, wall=62196
2023-10-17 16:35:39 | INFO | train_inner | epoch 001:  43070 / 1830643 loss=3.946, ppl=15.42, wps=22737.7, ups=0.69, wpb=32768, bsz=32, num_updates=43000, lr=0.000114, gnorm=0.148, loss_scale=16, train_wall=144, gb_free=15.4, wall=62340
2023-10-17 16:38:03 | INFO | train_inner | epoch 001:  43170 / 1830643 loss=3.349, ppl=10.19, wps=22737, ups=0.69, wpb=32768, bsz=32, num_updates=43100, lr=0.0001138, gnorm=0.147, loss_scale=16, train_wall=144, gb_free=15.4, wall=62484
2023-10-17 16:40:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 16:40:28 | INFO | train_inner | epoch 001:  43271 / 1830643 loss=3.442, ppl=10.86, wps=22593.5, ups=0.69, wpb=32768, bsz=32, num_updates=43200, lr=0.0001136, gnorm=0.154, loss_scale=16, train_wall=145, gb_free=15.4, wall=62629
2023-10-17 16:42:52 | INFO | train_inner | epoch 001:  43371 / 1830643 loss=3.378, ppl=10.4, wps=22799.2, ups=0.7, wpb=32768, bsz=32, num_updates=43300, lr=0.0001134, gnorm=0.152, loss_scale=16, train_wall=143, gb_free=15.4, wall=62773
2023-10-17 16:44:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 16:45:17 | INFO | train_inner | epoch 001:  43472 / 1830643 loss=3.473, ppl=11.1, wps=22552.3, ups=0.69, wpb=32768, bsz=32, num_updates=43400, lr=0.0001132, gnorm=0.173, loss_scale=8, train_wall=145, gb_free=15.4, wall=62918
2023-10-17 16:47:41 | INFO | train_inner | epoch 001:  43572 / 1830643 loss=3.488, ppl=11.22, wps=22701.7, ups=0.69, wpb=32768, bsz=32, num_updates=43500, lr=0.000113, gnorm=0.15, loss_scale=8, train_wall=144, gb_free=15.4, wall=63062
2023-10-17 16:50:05 | INFO | train_inner | epoch 001:  43672 / 1830643 loss=3.408, ppl=10.61, wps=22758.5, ups=0.69, wpb=32768, bsz=32, num_updates=43600, lr=0.0001128, gnorm=0.163, loss_scale=8, train_wall=144, gb_free=15.4, wall=63206
2023-10-17 16:52:30 | INFO | train_inner | epoch 001:  43772 / 1830643 loss=3.355, ppl=10.23, wps=22717, ups=0.69, wpb=32768, bsz=32, num_updates=43700, lr=0.0001126, gnorm=0.161, loss_scale=8, train_wall=144, gb_free=15.4, wall=63351
2023-10-17 16:54:54 | INFO | train_inner | epoch 001:  43872 / 1830643 loss=3.38, ppl=10.41, wps=22710.7, ups=0.69, wpb=32768, bsz=32, num_updates=43800, lr=0.0001124, gnorm=0.142, loss_scale=8, train_wall=144, gb_free=15.4, wall=63495
2023-10-17 16:57:17 | INFO | train_inner | epoch 001:  43972 / 1830643 loss=3.47, ppl=11.08, wps=22809.8, ups=0.7, wpb=32768, bsz=32, num_updates=43900, lr=0.0001122, gnorm=0.164, loss_scale=16, train_wall=143, gb_free=15.4, wall=63639
2023-10-17 16:59:42 | INFO | train_inner | epoch 001:  44072 / 1830643 loss=3.507, ppl=11.37, wps=22750.9, ups=0.69, wpb=32768, bsz=32, num_updates=44000, lr=0.000112, gnorm=0.148, loss_scale=16, train_wall=144, gb_free=15.4, wall=63783
2023-10-17 17:01:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 17:02:07 | INFO | train_inner | epoch 001:  44173 / 1830643 loss=3.474, ppl=11.11, wps=22463.8, ups=0.69, wpb=32768, bsz=32, num_updates=44100, lr=0.0001118, gnorm=0.156, loss_scale=8, train_wall=145, gb_free=15.4, wall=63929
2023-10-17 17:04:32 | INFO | train_inner | epoch 001:  44273 / 1830643 loss=3.463, ppl=11.03, wps=22671.6, ups=0.69, wpb=32768, bsz=32, num_updates=44200, lr=0.0001116, gnorm=0.17, loss_scale=8, train_wall=144, gb_free=15.4, wall=64073
2023-10-17 17:06:56 | INFO | train_inner | epoch 001:  44373 / 1830643 loss=3.405, ppl=10.59, wps=22704.6, ups=0.69, wpb=32768, bsz=32, num_updates=44300, lr=0.0001114, gnorm=0.148, loss_scale=8, train_wall=144, gb_free=15.4, wall=64217
2023-10-17 17:09:21 | INFO | train_inner | epoch 001:  44473 / 1830643 loss=3.341, ppl=10.13, wps=22661.3, ups=0.69, wpb=32768, bsz=32, num_updates=44400, lr=0.0001112, gnorm=0.149, loss_scale=8, train_wall=144, gb_free=15.4, wall=64362
2023-10-17 17:11:45 | INFO | train_inner | epoch 001:  44573 / 1830643 loss=3.41, ppl=10.63, wps=22654.8, ups=0.69, wpb=32768, bsz=32, num_updates=44500, lr=0.000111, gnorm=0.158, loss_scale=8, train_wall=144, gb_free=15.4, wall=64507
2023-10-17 17:14:09 | INFO | train_inner | epoch 001:  44673 / 1830643 loss=3.312, ppl=9.93, wps=22775.9, ups=0.7, wpb=32768, bsz=32, num_updates=44600, lr=0.0001108, gnorm=0.147, loss_scale=16, train_wall=144, gb_free=15.4, wall=64651
2023-10-17 17:16:34 | INFO | train_inner | epoch 001:  44773 / 1830643 loss=3.456, ppl=10.97, wps=22719.8, ups=0.69, wpb=32768, bsz=32, num_updates=44700, lr=0.0001106, gnorm=0.15, loss_scale=16, train_wall=144, gb_free=15.4, wall=64795
2023-10-17 17:18:58 | INFO | train_inner | epoch 001:  44873 / 1830643 loss=3.339, ppl=10.12, wps=22750.8, ups=0.69, wpb=32768, bsz=32, num_updates=44800, lr=0.0001104, gnorm=0.162, loss_scale=16, train_wall=144, gb_free=15.4, wall=64939
2023-10-17 17:21:21 | INFO | train_inner | epoch 001:  44973 / 1830643 loss=3.485, ppl=11.19, wps=22833.4, ups=0.7, wpb=32768, bsz=32, num_updates=44900, lr=0.0001102, gnorm=0.144, loss_scale=16, train_wall=143, gb_free=15.4, wall=65082
2023-10-17 17:23:45 | INFO | train_inner | epoch 001:  45073 / 1830643 loss=3.475, ppl=11.12, wps=22793.8, ups=0.7, wpb=32768, bsz=32, num_updates=45000, lr=0.00011, gnorm=0.169, loss_scale=16, train_wall=143, gb_free=15.4, wall=65226
2023-10-17 17:26:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 17:26:11 | INFO | train_inner | epoch 001:  45174 / 1830643 loss=3.514, ppl=11.42, wps=22404.1, ups=0.68, wpb=32768, bsz=32, num_updates=45100, lr=0.0001098, gnorm=0.158, loss_scale=16, train_wall=146, gb_free=15.4, wall=65372
2023-10-17 17:28:37 | INFO | train_inner | epoch 001:  45274 / 1830643 loss=3.526, ppl=11.52, wps=22391.6, ups=0.68, wpb=32768, bsz=32, num_updates=45200, lr=0.0001096, gnorm=0.161, loss_scale=16, train_wall=146, gb_free=15.4, wall=65519
2023-10-17 17:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 17:31:03 | INFO | train_inner | epoch 001:  45375 / 1830643 loss=3.232, ppl=9.39, wps=22486.1, ups=0.69, wpb=32768, bsz=32, num_updates=45300, lr=0.0001094, gnorm=0.153, loss_scale=8, train_wall=145, gb_free=15.4, wall=65664
2023-10-17 17:33:28 | INFO | train_inner | epoch 001:  45475 / 1830643 loss=2.827, ppl=7.1, wps=22689.3, ups=0.69, wpb=32768, bsz=32, num_updates=45400, lr=0.0001092, gnorm=0.162, loss_scale=8, train_wall=144, gb_free=15.4, wall=65809
2023-10-17 17:35:52 | INFO | train_inner | epoch 001:  45575 / 1830643 loss=3.318, ppl=9.97, wps=22705.7, ups=0.69, wpb=32768, bsz=32, num_updates=45500, lr=0.000109, gnorm=0.159, loss_scale=8, train_wall=144, gb_free=15.4, wall=65953
2023-10-17 17:38:17 | INFO | train_inner | epoch 001:  45675 / 1830643 loss=3.672, ppl=12.75, wps=22648.5, ups=0.69, wpb=32768, bsz=32, num_updates=45600, lr=0.0001088, gnorm=0.162, loss_scale=8, train_wall=144, gb_free=15.4, wall=66098
2023-10-17 17:40:41 | INFO | train_inner | epoch 001:  45775 / 1830643 loss=3.712, ppl=13.11, wps=22721.3, ups=0.69, wpb=32768, bsz=32, num_updates=45700, lr=0.0001086, gnorm=0.152, loss_scale=8, train_wall=144, gb_free=15.4, wall=66242
2023-10-17 17:43:01 | INFO | train_inner | epoch 001:  45875 / 1830643 loss=3.442, ppl=10.87, wps=23327.2, ups=0.71, wpb=32768, bsz=32, num_updates=45800, lr=0.0001084, gnorm=0.148, loss_scale=16, train_wall=140, gb_free=15.4, wall=66382
2023-10-17 17:45:17 | INFO | train_inner | epoch 001:  45975 / 1830643 loss=3.355, ppl=10.23, wps=24102.3, ups=0.74, wpb=32768, bsz=32, num_updates=45900, lr=0.0001082, gnorm=0.156, loss_scale=16, train_wall=136, gb_free=15.4, wall=66518
2023-10-17 17:47:33 | INFO | train_inner | epoch 001:  46075 / 1830643 loss=3.302, ppl=9.86, wps=24166.2, ups=0.74, wpb=32768, bsz=32, num_updates=46000, lr=0.000108, gnorm=0.152, loss_scale=16, train_wall=135, gb_free=15.4, wall=66654
2023-10-17 17:49:49 | INFO | train_inner | epoch 001:  46175 / 1830643 loss=3.35, ppl=10.2, wps=24083.8, ups=0.73, wpb=32768, bsz=32, num_updates=46100, lr=0.0001078, gnorm=0.156, loss_scale=16, train_wall=136, gb_free=15.4, wall=66790
2023-10-17 17:52:05 | INFO | train_inner | epoch 001:  46275 / 1830643 loss=3.271, ppl=9.65, wps=24070.7, ups=0.73, wpb=32768, bsz=32, num_updates=46200, lr=0.0001076, gnorm=0.156, loss_scale=16, train_wall=136, gb_free=15.4, wall=66926
2023-10-17 17:52:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 17:54:22 | INFO | train_inner | epoch 001:  46376 / 1830643 loss=3.509, ppl=11.38, wps=23918.8, ups=0.73, wpb=32768, bsz=32, num_updates=46300, lr=0.0001074, gnorm=0.15, loss_scale=8, train_wall=137, gb_free=15.4, wall=67063
2023-10-17 17:56:38 | INFO | train_inner | epoch 001:  46476 / 1830643 loss=3.57, ppl=11.88, wps=24178, ups=0.74, wpb=32768, bsz=32, num_updates=46400, lr=0.0001072, gnorm=0.182, loss_scale=8, train_wall=135, gb_free=15.4, wall=67199
2023-10-17 17:58:53 | INFO | train_inner | epoch 001:  46576 / 1830643 loss=3.456, ppl=10.97, wps=24123.9, ups=0.74, wpb=32768, bsz=32, num_updates=46500, lr=0.000107, gnorm=0.173, loss_scale=8, train_wall=135, gb_free=15.4, wall=67335
2023-10-17 18:01:09 | INFO | train_inner | epoch 001:  46676 / 1830643 loss=3.346, ppl=10.17, wps=24079.9, ups=0.73, wpb=32768, bsz=32, num_updates=46600, lr=0.0001068, gnorm=0.15, loss_scale=8, train_wall=136, gb_free=15.4, wall=67471
2023-10-17 18:03:25 | INFO | train_inner | epoch 001:  46776 / 1830643 loss=3.382, ppl=10.42, wps=24121.7, ups=0.74, wpb=32768, bsz=32, num_updates=46700, lr=0.0001066, gnorm=0.152, loss_scale=8, train_wall=135, gb_free=15.4, wall=67606
2023-10-17 18:05:41 | INFO | train_inner | epoch 001:  46876 / 1830643 loss=3.173, ppl=9.02, wps=24072, ups=0.73, wpb=32768, bsz=32, num_updates=46800, lr=0.0001064, gnorm=0.141, loss_scale=16, train_wall=136, gb_free=15.4, wall=67743
2023-10-17 18:07:58 | INFO | train_inner | epoch 001:  46976 / 1830643 loss=3.521, ppl=11.48, wps=24049.6, ups=0.73, wpb=32768, bsz=32, num_updates=46900, lr=0.0001062, gnorm=0.153, loss_scale=16, train_wall=136, gb_free=15.4, wall=67879
2023-10-17 18:10:13 | INFO | train_inner | epoch 001:  47076 / 1830643 loss=3.341, ppl=10.13, wps=24157.3, ups=0.74, wpb=32768, bsz=32, num_updates=47000, lr=0.000106, gnorm=0.154, loss_scale=16, train_wall=135, gb_free=15.4, wall=68015
2023-10-17 18:12:29 | INFO | train_inner | epoch 001:  47176 / 1830643 loss=3.449, ppl=10.92, wps=24081.2, ups=0.73, wpb=32768, bsz=32, num_updates=47100, lr=0.0001058, gnorm=0.149, loss_scale=16, train_wall=136, gb_free=15.4, wall=68151
2023-10-17 18:14:45 | INFO | train_inner | epoch 001:  47276 / 1830643 loss=3.499, ppl=11.31, wps=24101.1, ups=0.74, wpb=32768, bsz=32, num_updates=47200, lr=0.0001056, gnorm=0.145, loss_scale=16, train_wall=136, gb_free=15.4, wall=68287
2023-10-17 18:16:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 18:17:02 | INFO | train_inner | epoch 001:  47377 / 1830643 loss=3.414, ppl=10.66, wps=23937.3, ups=0.73, wpb=32768, bsz=32, num_updates=47300, lr=0.0001054, gnorm=0.144, loss_scale=16, train_wall=137, gb_free=15.4, wall=68423
2023-10-17 18:19:18 | INFO | train_inner | epoch 001:  47477 / 1830643 loss=3.402, ppl=10.57, wps=24087.4, ups=0.74, wpb=32768, bsz=32, num_updates=47400, lr=0.0001052, gnorm=0.148, loss_scale=16, train_wall=136, gb_free=15.4, wall=68559
2023-10-17 18:21:34 | INFO | train_inner | epoch 001:  47577 / 1830643 loss=3.528, ppl=11.54, wps=24090.5, ups=0.74, wpb=32768, bsz=32, num_updates=47500, lr=0.000105, gnorm=0.168, loss_scale=16, train_wall=136, gb_free=15.4, wall=68696
2023-10-17 18:23:50 | INFO | train_inner | epoch 001:  47677 / 1830643 loss=3.374, ppl=10.37, wps=24111.7, ups=0.74, wpb=32768, bsz=32, num_updates=47600, lr=0.0001048, gnorm=0.158, loss_scale=16, train_wall=136, gb_free=15.4, wall=68831
2023-10-17 18:24:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 18:26:07 | INFO | train_inner | epoch 001:  47778 / 1830643 loss=3.397, ppl=10.54, wps=23877.6, ups=0.73, wpb=32768, bsz=32, num_updates=47700, lr=0.0001046, gnorm=0.165, loss_scale=8, train_wall=137, gb_free=15.4, wall=68969
2023-10-17 18:28:23 | INFO | train_inner | epoch 001:  47878 / 1830643 loss=3.44, ppl=10.85, wps=24171.3, ups=0.74, wpb=32768, bsz=32, num_updates=47800, lr=0.0001044, gnorm=0.154, loss_scale=8, train_wall=135, gb_free=15.4, wall=69104
2023-10-17 18:30:40 | INFO | train_inner | epoch 001:  47978 / 1830643 loss=3.4, ppl=10.55, wps=23980.6, ups=0.73, wpb=32768, bsz=32, num_updates=47900, lr=0.0001042, gnorm=0.15, loss_scale=8, train_wall=136, gb_free=15.4, wall=69241
2023-10-17 18:32:55 | INFO | train_inner | epoch 001:  48078 / 1830643 loss=3.383, ppl=10.43, wps=24169.9, ups=0.74, wpb=32768, bsz=32, num_updates=48000, lr=0.000104, gnorm=0.152, loss_scale=8, train_wall=135, gb_free=15.4, wall=69376
2023-10-17 18:35:11 | INFO | train_inner | epoch 001:  48178 / 1830643 loss=3.607, ppl=12.18, wps=24099.8, ups=0.74, wpb=32768, bsz=32, num_updates=48100, lr=0.0001038, gnorm=0.157, loss_scale=8, train_wall=136, gb_free=15.4, wall=69512
2023-10-17 18:37:27 | INFO | train_inner | epoch 001:  48278 / 1830643 loss=3.377, ppl=10.39, wps=24127, ups=0.74, wpb=32768, bsz=32, num_updates=48200, lr=0.0001036, gnorm=0.157, loss_scale=16, train_wall=135, gb_free=15.4, wall=69648
2023-10-17 18:39:43 | INFO | train_inner | epoch 001:  48378 / 1830643 loss=3.452, ppl=10.95, wps=24123, ups=0.74, wpb=32768, bsz=32, num_updates=48300, lr=0.0001034, gnorm=0.153, loss_scale=16, train_wall=135, gb_free=15.4, wall=69784
2023-10-17 18:41:59 | INFO | train_inner | epoch 001:  48478 / 1830643 loss=3.525, ppl=11.51, wps=24094.3, ups=0.74, wpb=32768, bsz=32, num_updates=48400, lr=0.0001032, gnorm=0.157, loss_scale=16, train_wall=136, gb_free=15.4, wall=69920
2023-10-17 18:44:15 | INFO | train_inner | epoch 001:  48578 / 1830643 loss=3.646, ppl=12.52, wps=24130.2, ups=0.74, wpb=32768, bsz=32, num_updates=48500, lr=0.000103, gnorm=0.154, loss_scale=16, train_wall=135, gb_free=15.4, wall=70056
2023-10-17 18:46:30 | INFO | train_inner | epoch 001:  48678 / 1830643 loss=3.474, ppl=11.11, wps=24132.8, ups=0.74, wpb=32768, bsz=32, num_updates=48600, lr=0.0001028, gnorm=0.169, loss_scale=16, train_wall=135, gb_free=15.4, wall=70192
2023-10-17 18:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 18:48:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 18:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-17 18:48:50 | INFO | train_inner | epoch 001:  48781 / 1830643 loss=3.459, ppl=11, wps=23465.9, ups=0.72, wpb=32768, bsz=32, num_updates=48700, lr=0.0001026, gnorm=0.163, loss_scale=4, train_wall=139, gb_free=15.4, wall=70331
2023-10-17 18:51:06 | INFO | train_inner | epoch 001:  48881 / 1830643 loss=3.433, ppl=10.8, wps=24185.8, ups=0.74, wpb=32768, bsz=32, num_updates=48800, lr=0.0001024, gnorm=0.163, loss_scale=4, train_wall=135, gb_free=15.4, wall=70467
2023-10-17 18:53:21 | INFO | train_inner | epoch 001:  48981 / 1830643 loss=3.377, ppl=10.39, wps=24196.3, ups=0.74, wpb=32768, bsz=32, num_updates=48900, lr=0.0001022, gnorm=0.149, loss_scale=4, train_wall=135, gb_free=15.4, wall=70602
2023-10-17 18:55:37 | INFO | train_inner | epoch 001:  49081 / 1830643 loss=3.465, ppl=11.04, wps=24098.2, ups=0.74, wpb=32768, bsz=32, num_updates=49000, lr=0.000102, gnorm=0.159, loss_scale=4, train_wall=136, gb_free=15.4, wall=70738
2023-10-17 18:57:53 | INFO | train_inner | epoch 001:  49181 / 1830643 loss=3.162, ppl=8.95, wps=24142.7, ups=0.74, wpb=32768, bsz=32, num_updates=49100, lr=0.0001018, gnorm=0.15, loss_scale=4, train_wall=135, gb_free=15.4, wall=70874
2023-10-17 19:00:08 | INFO | train_inner | epoch 001:  49281 / 1830643 loss=3.379, ppl=10.4, wps=24142, ups=0.74, wpb=32768, bsz=32, num_updates=49200, lr=0.0001016, gnorm=0.154, loss_scale=8, train_wall=135, gb_free=15.4, wall=71010
2023-10-17 19:02:24 | INFO | train_inner | epoch 001:  49381 / 1830643 loss=3.457, ppl=10.98, wps=24131.7, ups=0.74, wpb=32768, bsz=32, num_updates=49300, lr=0.0001014, gnorm=0.147, loss_scale=8, train_wall=135, gb_free=15.4, wall=71145
2023-10-17 19:04:40 | INFO | train_inner | epoch 001:  49481 / 1830643 loss=3.429, ppl=10.77, wps=24112.8, ups=0.74, wpb=32768, bsz=32, num_updates=49400, lr=0.0001012, gnorm=0.15, loss_scale=8, train_wall=136, gb_free=15.4, wall=71281
2023-10-17 19:06:56 | INFO | train_inner | epoch 001:  49581 / 1830643 loss=3.404, ppl=10.58, wps=24110.4, ups=0.74, wpb=32768, bsz=32, num_updates=49500, lr=0.000101, gnorm=0.151, loss_scale=8, train_wall=136, gb_free=15.4, wall=71417
2023-10-17 19:09:12 | INFO | train_inner | epoch 001:  49681 / 1830643 loss=3.382, ppl=10.43, wps=24055.2, ups=0.73, wpb=32768, bsz=32, num_updates=49600, lr=0.0001008, gnorm=0.158, loss_scale=8, train_wall=136, gb_free=15.4, wall=71553
2023-10-17 19:11:28 | INFO | train_inner | epoch 001:  49781 / 1830643 loss=3.318, ppl=9.97, wps=24100.2, ups=0.74, wpb=32768, bsz=32, num_updates=49700, lr=0.0001006, gnorm=0.148, loss_scale=8, train_wall=136, gb_free=15.4, wall=71689
2023-10-17 19:13:44 | INFO | train_inner | epoch 001:  49881 / 1830643 loss=3.389, ppl=10.48, wps=24148.7, ups=0.74, wpb=32768, bsz=32, num_updates=49800, lr=0.0001004, gnorm=0.152, loss_scale=16, train_wall=135, gb_free=15.4, wall=71825
2023-10-17 19:16:00 | INFO | train_inner | epoch 001:  49981 / 1830643 loss=3.431, ppl=10.79, wps=24139.6, ups=0.74, wpb=32768, bsz=32, num_updates=49900, lr=0.0001002, gnorm=0.148, loss_scale=16, train_wall=135, gb_free=15.4, wall=71961
2023-10-17 19:18:16 | INFO | train_inner | epoch 001:  50081 / 1830643 loss=3.193, ppl=9.15, wps=24123.8, ups=0.74, wpb=32768, bsz=32, num_updates=50000, lr=0.0001, gnorm=0.146, loss_scale=16, train_wall=135, gb_free=15.4, wall=72097
2023-10-17 19:18:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 50000 updates
2023-10-17 19:18:16 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_50000.pt
2023-10-17 19:18:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_50000.pt
2023-10-17 19:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_50000.pt (epoch 1 @ 50000 updates, score None) (writing took 15.294625686015934 seconds)
2023-10-17 19:20:47 | INFO | train_inner | epoch 001:  50181 / 1830643 loss=3.385, ppl=10.45, wps=21697.2, ups=0.66, wpb=32768, bsz=32, num_updates=50100, lr=9.98e-05, gnorm=0.156, loss_scale=16, train_wall=135, gb_free=15.4, wall=72248
2023-10-17 19:23:02 | INFO | train_inner | epoch 001:  50281 / 1830643 loss=3.285, ppl=9.75, wps=24179.5, ups=0.74, wpb=32768, bsz=32, num_updates=50200, lr=9.96e-05, gnorm=0.147, loss_scale=16, train_wall=135, gb_free=15.4, wall=72383
2023-10-17 19:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 19:23:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 19:25:20 | INFO | train_inner | epoch 001:  50383 / 1830643 loss=3.503, ppl=11.33, wps=23717.3, ups=0.72, wpb=32768, bsz=32, num_updates=50300, lr=9.94e-05, gnorm=0.15, loss_scale=8, train_wall=138, gb_free=15.4, wall=72521
2023-10-17 19:27:36 | INFO | train_inner | epoch 001:  50483 / 1830643 loss=3.509, ppl=11.39, wps=24146.7, ups=0.74, wpb=32768, bsz=32, num_updates=50400, lr=9.92e-05, gnorm=0.157, loss_scale=8, train_wall=135, gb_free=15.4, wall=72657
2023-10-17 19:29:52 | INFO | train_inner | epoch 001:  50583 / 1830643 loss=3.404, ppl=10.59, wps=24076.7, ups=0.73, wpb=32768, bsz=32, num_updates=50500, lr=9.9e-05, gnorm=0.15, loss_scale=8, train_wall=136, gb_free=15.4, wall=72793
2023-10-17 19:32:08 | INFO | train_inner | epoch 001:  50683 / 1830643 loss=3.388, ppl=10.47, wps=24134.1, ups=0.74, wpb=32768, bsz=32, num_updates=50600, lr=9.88e-05, gnorm=0.144, loss_scale=8, train_wall=135, gb_free=15.4, wall=72929
2023-10-17 19:34:24 | INFO | train_inner | epoch 001:  50783 / 1830643 loss=3.426, ppl=10.75, wps=24064.2, ups=0.73, wpb=32768, bsz=32, num_updates=50700, lr=9.86e-05, gnorm=0.144, loss_scale=8, train_wall=136, gb_free=15.4, wall=73065
2023-10-17 19:36:40 | INFO | train_inner | epoch 001:  50883 / 1830643 loss=3.585, ppl=12, wps=24092.6, ups=0.74, wpb=32768, bsz=32, num_updates=50800, lr=9.84e-05, gnorm=0.15, loss_scale=16, train_wall=136, gb_free=15.4, wall=73201
2023-10-17 19:38:56 | INFO | train_inner | epoch 001:  50983 / 1830643 loss=3.387, ppl=10.46, wps=24145.4, ups=0.74, wpb=32768, bsz=32, num_updates=50900, lr=9.82e-05, gnorm=0.15, loss_scale=16, train_wall=135, gb_free=15.4, wall=73337
2023-10-17 19:41:12 | INFO | train_inner | epoch 001:  51083 / 1830643 loss=3.43, ppl=10.78, wps=24072.3, ups=0.73, wpb=32768, bsz=32, num_updates=51000, lr=9.8e-05, gnorm=0.147, loss_scale=16, train_wall=136, gb_free=15.4, wall=73473
2023-10-17 19:43:28 | INFO | train_inner | epoch 001:  51183 / 1830643 loss=3.421, ppl=10.71, wps=24007.1, ups=0.73, wpb=32768, bsz=32, num_updates=51100, lr=9.78e-05, gnorm=0.144, loss_scale=16, train_wall=136, gb_free=15.4, wall=73609
2023-10-17 19:45:44 | INFO | train_inner | epoch 001:  51283 / 1830643 loss=3.381, ppl=10.42, wps=24126.3, ups=0.74, wpb=32768, bsz=32, num_updates=51200, lr=9.76e-05, gnorm=0.152, loss_scale=16, train_wall=135, gb_free=15.4, wall=73745
2023-10-17 19:47:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 19:48:02 | INFO | train_inner | epoch 001:  51384 / 1830643 loss=3.475, ppl=11.12, wps=23778.3, ups=0.73, wpb=32768, bsz=32, num_updates=51300, lr=9.74e-05, gnorm=0.147, loss_scale=16, train_wall=137, gb_free=15.4, wall=73883
2023-10-17 19:50:18 | INFO | train_inner | epoch 001:  51484 / 1830643 loss=3.389, ppl=10.48, wps=24078.4, ups=0.73, wpb=32768, bsz=32, num_updates=51400, lr=9.72e-05, gnorm=0.148, loss_scale=16, train_wall=136, gb_free=15.4, wall=74019
2023-10-17 19:52:35 | INFO | train_inner | epoch 001:  51584 / 1830643 loss=3.441, ppl=10.86, wps=24002.1, ups=0.73, wpb=32768, bsz=32, num_updates=51500, lr=9.7e-05, gnorm=0.141, loss_scale=16, train_wall=136, gb_free=15.4, wall=74156
2023-10-17 19:54:50 | INFO | train_inner | epoch 001:  51684 / 1830643 loss=3.396, ppl=10.52, wps=24139.2, ups=0.74, wpb=32768, bsz=32, num_updates=51600, lr=9.68e-05, gnorm=0.15, loss_scale=16, train_wall=135, gb_free=15.4, wall=74291
2023-10-17 19:57:06 | INFO | train_inner | epoch 001:  51784 / 1830643 loss=3.302, ppl=9.86, wps=24070.9, ups=0.73, wpb=32768, bsz=32, num_updates=51700, lr=9.66e-05, gnorm=0.144, loss_scale=16, train_wall=136, gb_free=15.4, wall=74428
2023-10-17 19:58:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 19:59:24 | INFO | train_inner | epoch 001:  51885 / 1830643 loss=3.41, ppl=10.63, wps=23854.4, ups=0.73, wpb=32768, bsz=32, num_updates=51800, lr=9.64e-05, gnorm=0.157, loss_scale=8, train_wall=137, gb_free=15.4, wall=74565
2023-10-17 20:01:40 | INFO | train_inner | epoch 001:  51985 / 1830643 loss=3.516, ppl=11.44, wps=24060.9, ups=0.73, wpb=32768, bsz=32, num_updates=51900, lr=9.62e-05, gnorm=0.154, loss_scale=8, train_wall=136, gb_free=15.4, wall=74701
2023-10-17 20:03:57 | INFO | train_inner | epoch 001:  52085 / 1830643 loss=3.41, ppl=10.63, wps=23935.9, ups=0.73, wpb=32768, bsz=32, num_updates=52000, lr=9.6e-05, gnorm=0.142, loss_scale=8, train_wall=137, gb_free=15.4, wall=74838
2023-10-17 20:06:13 | INFO | train_inner | epoch 001:  52185 / 1830643 loss=3.234, ppl=9.41, wps=24097.3, ups=0.74, wpb=32768, bsz=32, num_updates=52100, lr=9.58e-05, gnorm=0.146, loss_scale=8, train_wall=136, gb_free=15.4, wall=74974
2023-10-17 20:08:29 | INFO | train_inner | epoch 001:  52285 / 1830643 loss=3.456, ppl=10.97, wps=24111.7, ups=0.74, wpb=32768, bsz=32, num_updates=52200, lr=9.56e-05, gnorm=0.147, loss_scale=8, train_wall=135, gb_free=15.4, wall=75110
2023-10-17 20:10:45 | INFO | train_inner | epoch 001:  52385 / 1830643 loss=3.529, ppl=11.55, wps=24121.5, ups=0.74, wpb=32768, bsz=32, num_updates=52300, lr=9.54e-05, gnorm=0.155, loss_scale=16, train_wall=135, gb_free=15.4, wall=75246
2023-10-17 20:12:59 | INFO | train_inner | epoch 001:  52485 / 1830643 loss=3.421, ppl=10.71, wps=24302.7, ups=0.74, wpb=32768, bsz=32, num_updates=52400, lr=9.52e-05, gnorm=0.148, loss_scale=16, train_wall=134, gb_free=15.4, wall=75381
2023-10-17 20:15:15 | INFO | train_inner | epoch 001:  52585 / 1830643 loss=3.435, ppl=10.82, wps=24141.4, ups=0.74, wpb=32768, bsz=32, num_updates=52500, lr=9.5e-05, gnorm=0.157, loss_scale=16, train_wall=135, gb_free=15.4, wall=75516
2023-10-17 20:17:31 | INFO | train_inner | epoch 001:  52685 / 1830643 loss=3.373, ppl=10.36, wps=24211.6, ups=0.74, wpb=32768, bsz=32, num_updates=52600, lr=9.48e-05, gnorm=0.149, loss_scale=16, train_wall=135, gb_free=15.4, wall=75652
2023-10-17 20:19:46 | INFO | train_inner | epoch 001:  52785 / 1830643 loss=3.347, ppl=10.18, wps=24206.9, ups=0.74, wpb=32768, bsz=32, num_updates=52700, lr=9.46e-05, gnorm=0.148, loss_scale=16, train_wall=135, gb_free=15.4, wall=75787
2023-10-17 20:22:02 | INFO | train_inner | epoch 001:  52885 / 1830643 loss=3.424, ppl=10.73, wps=24138.3, ups=0.74, wpb=32768, bsz=32, num_updates=52800, lr=9.44e-05, gnorm=0.144, loss_scale=32, train_wall=135, gb_free=15.4, wall=75923
2023-10-17 20:23:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 20:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 20:24:20 | INFO | train_inner | epoch 001:  52987 / 1830643 loss=3.329, ppl=10.05, wps=23601.1, ups=0.72, wpb=32768, bsz=32, num_updates=52900, lr=9.42e-05, gnorm=0.156, loss_scale=8, train_wall=138, gb_free=15.4, wall=76062
2023-10-17 20:26:38 | INFO | train_inner | epoch 001:  53087 / 1830643 loss=3.212, ppl=9.27, wps=23907.1, ups=0.73, wpb=32768, bsz=32, num_updates=53000, lr=9.4e-05, gnorm=0.154, loss_scale=8, train_wall=137, gb_free=15.4, wall=76199
2023-10-17 20:28:53 | INFO | train_inner | epoch 001:  53187 / 1830643 loss=3.311, ppl=9.92, wps=24129.8, ups=0.74, wpb=32768, bsz=32, num_updates=53100, lr=9.38e-05, gnorm=0.154, loss_scale=8, train_wall=135, gb_free=15.4, wall=76334
2023-10-17 20:31:09 | INFO | train_inner | epoch 001:  53287 / 1830643 loss=3.573, ppl=11.9, wps=24091.2, ups=0.74, wpb=32768, bsz=32, num_updates=53200, lr=9.36e-05, gnorm=0.152, loss_scale=8, train_wall=136, gb_free=15.4, wall=76471
2023-10-17 20:33:26 | INFO | train_inner | epoch 001:  53387 / 1830643 loss=3.426, ppl=10.74, wps=23952.4, ups=0.73, wpb=32768, bsz=32, num_updates=53300, lr=9.34e-05, gnorm=0.151, loss_scale=8, train_wall=136, gb_free=15.4, wall=76607
2023-10-17 20:35:42 | INFO | train_inner | epoch 001:  53487 / 1830643 loss=3.42, ppl=10.7, wps=24137.9, ups=0.74, wpb=32768, bsz=32, num_updates=53400, lr=9.32e-05, gnorm=0.147, loss_scale=16, train_wall=135, gb_free=15.4, wall=76743
2023-10-17 20:37:58 | INFO | train_inner | epoch 001:  53587 / 1830643 loss=3.631, ppl=12.39, wps=24142.1, ups=0.74, wpb=32768, bsz=32, num_updates=53500, lr=9.3e-05, gnorm=0.145, loss_scale=16, train_wall=135, gb_free=15.4, wall=76879
2023-10-17 20:40:14 | INFO | train_inner | epoch 001:  53687 / 1830643 loss=3.706, ppl=13.05, wps=24108.2, ups=0.74, wpb=32768, bsz=32, num_updates=53600, lr=9.28e-05, gnorm=0.146, loss_scale=16, train_wall=136, gb_free=15.4, wall=77015
2023-10-17 20:42:30 | INFO | train_inner | epoch 001:  53787 / 1830643 loss=3.609, ppl=12.2, wps=24042.2, ups=0.73, wpb=32768, bsz=32, num_updates=53700, lr=9.26e-05, gnorm=0.154, loss_scale=16, train_wall=136, gb_free=15.4, wall=77151
2023-10-17 20:44:46 | INFO | train_inner | epoch 001:  53887 / 1830643 loss=3.423, ppl=10.73, wps=24041.4, ups=0.73, wpb=32768, bsz=32, num_updates=53800, lr=9.24e-05, gnorm=0.153, loss_scale=16, train_wall=136, gb_free=15.4, wall=77287
2023-10-17 20:44:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 20:47:04 | INFO | train_inner | epoch 001:  53988 / 1830643 loss=3.592, ppl=12.06, wps=23821.4, ups=0.73, wpb=32768, bsz=32, num_updates=53900, lr=9.22e-05, gnorm=0.171, loss_scale=8, train_wall=137, gb_free=15.4, wall=77425
2023-10-17 20:49:19 | INFO | train_inner | epoch 001:  54088 / 1830643 loss=3.569, ppl=11.87, wps=24188.7, ups=0.74, wpb=32768, bsz=32, num_updates=54000, lr=9.2e-05, gnorm=0.149, loss_scale=8, train_wall=135, gb_free=15.4, wall=77560
2023-10-17 20:51:35 | INFO | train_inner | epoch 001:  54188 / 1830643 loss=3.407, ppl=10.61, wps=24040.1, ups=0.73, wpb=32768, bsz=32, num_updates=54100, lr=9.18e-05, gnorm=0.148, loss_scale=8, train_wall=136, gb_free=15.4, wall=77697
2023-10-17 20:53:51 | INFO | train_inner | epoch 001:  54288 / 1830643 loss=3.533, ppl=11.57, wps=24113.8, ups=0.74, wpb=32768, bsz=32, num_updates=54200, lr=9.16e-05, gnorm=0.15, loss_scale=8, train_wall=136, gb_free=15.4, wall=77833
2023-10-17 20:56:07 | INFO | train_inner | epoch 001:  54388 / 1830643 loss=3.527, ppl=11.53, wps=24075, ups=0.73, wpb=32768, bsz=32, num_updates=54300, lr=9.14e-05, gnorm=0.147, loss_scale=8, train_wall=136, gb_free=15.4, wall=77969
2023-10-17 20:58:24 | INFO | train_inner | epoch 001:  54488 / 1830643 loss=3.624, ppl=12.33, wps=24067.4, ups=0.73, wpb=32768, bsz=32, num_updates=54400, lr=9.12e-05, gnorm=0.157, loss_scale=16, train_wall=136, gb_free=15.4, wall=78105
2023-10-17 21:00:40 | INFO | train_inner | epoch 001:  54588 / 1830643 loss=3.472, ppl=11.1, wps=24057.6, ups=0.73, wpb=32768, bsz=32, num_updates=54500, lr=9.1e-05, gnorm=0.142, loss_scale=16, train_wall=136, gb_free=15.4, wall=78241
2023-10-17 21:02:55 | INFO | train_inner | epoch 001:  54688 / 1830643 loss=3.546, ppl=11.68, wps=24199.8, ups=0.74, wpb=32768, bsz=32, num_updates=54600, lr=9.08e-05, gnorm=0.152, loss_scale=16, train_wall=135, gb_free=15.4, wall=78376
2023-10-17 21:05:11 | INFO | train_inner | epoch 001:  54788 / 1830643 loss=3.539, ppl=11.62, wps=24095.6, ups=0.74, wpb=32768, bsz=32, num_updates=54700, lr=9.06e-05, gnorm=0.152, loss_scale=16, train_wall=136, gb_free=15.4, wall=78512
2023-10-17 21:07:27 | INFO | train_inner | epoch 001:  54888 / 1830643 loss=3.331, ppl=10.06, wps=24161.1, ups=0.74, wpb=32768, bsz=32, num_updates=54800, lr=9.04e-05, gnorm=0.145, loss_scale=16, train_wall=135, gb_free=15.4, wall=78648
2023-10-17 21:08:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 21:08:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 21:09:45 | INFO | train_inner | epoch 001:  54990 / 1830643 loss=3.395, ppl=10.52, wps=23734.5, ups=0.72, wpb=32768, bsz=32, num_updates=54900, lr=9.02e-05, gnorm=0.154, loss_scale=8, train_wall=138, gb_free=15.4, wall=78786
2023-10-17 21:12:01 | INFO | train_inner | epoch 001:  55090 / 1830643 loss=3.357, ppl=10.24, wps=24004.6, ups=0.73, wpb=32768, bsz=32, num_updates=55000, lr=9e-05, gnorm=0.143, loss_scale=8, train_wall=136, gb_free=15.4, wall=78923
2023-10-17 21:14:17 | INFO | train_inner | epoch 001:  55190 / 1830643 loss=3.44, ppl=10.85, wps=24127.1, ups=0.74, wpb=32768, bsz=32, num_updates=55100, lr=8.98e-05, gnorm=0.149, loss_scale=8, train_wall=135, gb_free=15.4, wall=79058
2023-10-17 21:16:33 | INFO | train_inner | epoch 001:  55290 / 1830643 loss=3.512, ppl=11.41, wps=24098.5, ups=0.74, wpb=32768, bsz=32, num_updates=55200, lr=8.96e-05, gnorm=0.14, loss_scale=8, train_wall=136, gb_free=15.4, wall=79194
2023-10-17 21:18:49 | INFO | train_inner | epoch 001:  55390 / 1830643 loss=3.46, ppl=11.01, wps=24156, ups=0.74, wpb=32768, bsz=32, num_updates=55300, lr=8.94e-05, gnorm=0.156, loss_scale=8, train_wall=135, gb_free=15.4, wall=79330
2023-10-17 21:21:05 | INFO | train_inner | epoch 001:  55490 / 1830643 loss=3.293, ppl=9.8, wps=24148.2, ups=0.74, wpb=32768, bsz=32, num_updates=55400, lr=8.92e-05, gnorm=0.164, loss_scale=16, train_wall=135, gb_free=15.4, wall=79466
2023-10-17 21:23:20 | INFO | train_inner | epoch 001:  55590 / 1830643 loss=3.386, ppl=10.45, wps=24144.1, ups=0.74, wpb=32768, bsz=32, num_updates=55500, lr=8.9e-05, gnorm=0.155, loss_scale=16, train_wall=135, gb_free=15.4, wall=79601
2023-10-17 21:25:36 | INFO | train_inner | epoch 001:  55690 / 1830643 loss=3.467, ppl=11.05, wps=24128, ups=0.74, wpb=32768, bsz=32, num_updates=55600, lr=8.88e-05, gnorm=0.148, loss_scale=16, train_wall=135, gb_free=15.4, wall=79737
2023-10-17 21:27:52 | INFO | train_inner | epoch 001:  55790 / 1830643 loss=3.378, ppl=10.4, wps=24143.8, ups=0.74, wpb=32768, bsz=32, num_updates=55700, lr=8.86e-05, gnorm=0.149, loss_scale=16, train_wall=135, gb_free=15.4, wall=79873
2023-10-17 21:30:07 | INFO | train_inner | epoch 001:  55890 / 1830643 loss=3.329, ppl=10.05, wps=24197.2, ups=0.74, wpb=32768, bsz=32, num_updates=55800, lr=8.84e-05, gnorm=0.151, loss_scale=16, train_wall=135, gb_free=15.4, wall=80008
2023-10-17 21:32:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 21:32:24 | INFO | train_inner | epoch 001:  55991 / 1830643 loss=3.669, ppl=12.72, wps=23918.6, ups=0.73, wpb=32768, bsz=32, num_updates=55900, lr=8.82e-05, gnorm=0.153, loss_scale=16, train_wall=137, gb_free=15.4, wall=80145
2023-10-17 21:34:40 | INFO | train_inner | epoch 001:  56091 / 1830643 loss=3.482, ppl=11.18, wps=24194.3, ups=0.74, wpb=32768, bsz=32, num_updates=56000, lr=8.8e-05, gnorm=0.148, loss_scale=16, train_wall=135, gb_free=15.4, wall=80281
2023-10-17 21:36:55 | INFO | train_inner | epoch 001:  56191 / 1830643 loss=3.659, ppl=12.63, wps=24183.3, ups=0.74, wpb=32768, bsz=32, num_updates=56100, lr=8.78e-05, gnorm=0.151, loss_scale=16, train_wall=135, gb_free=15.4, wall=80416
2023-10-17 21:39:11 | INFO | train_inner | epoch 001:  56291 / 1830643 loss=3.73, ppl=13.27, wps=24161.8, ups=0.74, wpb=32768, bsz=32, num_updates=56200, lr=8.76e-05, gnorm=0.16, loss_scale=16, train_wall=135, gb_free=15.4, wall=80552
2023-10-17 21:41:27 | INFO | train_inner | epoch 001:  56391 / 1830643 loss=3.476, ppl=11.13, wps=24078.2, ups=0.73, wpb=32768, bsz=32, num_updates=56300, lr=8.74e-05, gnorm=0.153, loss_scale=16, train_wall=136, gb_free=15.4, wall=80688
2023-10-17 21:43:43 | INFO | train_inner | epoch 001:  56491 / 1830643 loss=3.335, ppl=10.09, wps=24037.6, ups=0.73, wpb=32768, bsz=32, num_updates=56400, lr=8.72e-05, gnorm=0.151, loss_scale=16, train_wall=136, gb_free=15.4, wall=80824
2023-10-17 21:45:59 | INFO | train_inner | epoch 001:  56591 / 1830643 loss=3.331, ppl=10.06, wps=24202.7, ups=0.74, wpb=32768, bsz=32, num_updates=56500, lr=8.7e-05, gnorm=0.144, loss_scale=32, train_wall=135, gb_free=15.4, wall=80960
2023-10-17 21:47:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 21:48:16 | INFO | train_inner | epoch 001:  56692 / 1830643 loss=3.389, ppl=10.48, wps=23892.4, ups=0.73, wpb=32768, bsz=32, num_updates=56600, lr=8.68e-05, gnorm=0.145, loss_scale=16, train_wall=137, gb_free=15.4, wall=81097
2023-10-17 21:50:32 | INFO | train_inner | epoch 001:  56792 / 1830643 loss=3.622, ppl=12.31, wps=24085.7, ups=0.74, wpb=32768, bsz=32, num_updates=56700, lr=8.66e-05, gnorm=0.162, loss_scale=16, train_wall=136, gb_free=15.4, wall=81233
2023-10-17 21:52:47 | INFO | train_inner | epoch 001:  56892 / 1830643 loss=3.547, ppl=11.69, wps=24224.1, ups=0.74, wpb=32768, bsz=32, num_updates=56800, lr=8.64e-05, gnorm=0.148, loss_scale=16, train_wall=135, gb_free=15.4, wall=81368
2023-10-17 21:54:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 21:55:04 | INFO | train_inner | epoch 001:  56993 / 1830643 loss=3.351, ppl=10.2, wps=23882.2, ups=0.73, wpb=32768, bsz=32, num_updates=56900, lr=8.62e-05, gnorm=0.147, loss_scale=8, train_wall=137, gb_free=15.4, wall=81505
2023-10-17 21:57:20 | INFO | train_inner | epoch 001:  57093 / 1830643 loss=3.325, ppl=10.02, wps=24073.9, ups=0.73, wpb=32768, bsz=32, num_updates=57000, lr=8.6e-05, gnorm=0.156, loss_scale=8, train_wall=136, gb_free=15.4, wall=81642
2023-10-17 21:59:36 | INFO | train_inner | epoch 001:  57193 / 1830643 loss=3.308, ppl=9.91, wps=24132.8, ups=0.74, wpb=32768, bsz=32, num_updates=57100, lr=8.58e-05, gnorm=0.156, loss_scale=8, train_wall=135, gb_free=15.4, wall=81777
2023-10-17 22:01:52 | INFO | train_inner | epoch 001:  57293 / 1830643 loss=3.266, ppl=9.62, wps=24100, ups=0.74, wpb=32768, bsz=32, num_updates=57200, lr=8.56e-05, gnorm=0.137, loss_scale=8, train_wall=136, gb_free=15.4, wall=81913
2023-10-17 22:04:08 | INFO | train_inner | epoch 001:  57393 / 1830643 loss=3.419, ppl=10.7, wps=24121.9, ups=0.74, wpb=32768, bsz=32, num_updates=57300, lr=8.54e-05, gnorm=0.153, loss_scale=8, train_wall=135, gb_free=15.4, wall=82049
2023-10-17 22:06:24 | INFO | train_inner | epoch 001:  57493 / 1830643 loss=3.34, ppl=10.13, wps=24141.8, ups=0.74, wpb=32768, bsz=32, num_updates=57400, lr=8.52e-05, gnorm=0.158, loss_scale=8, train_wall=135, gb_free=15.4, wall=82185
2023-10-17 22:08:39 | INFO | train_inner | epoch 001:  57593 / 1830643 loss=3.442, ppl=10.86, wps=24214, ups=0.74, wpb=32768, bsz=32, num_updates=57500, lr=8.5e-05, gnorm=0.14, loss_scale=16, train_wall=135, gb_free=15.4, wall=82320
2023-10-17 22:10:55 | INFO | train_inner | epoch 001:  57693 / 1830643 loss=3.433, ppl=10.8, wps=24115.5, ups=0.74, wpb=32768, bsz=32, num_updates=57600, lr=8.48e-05, gnorm=0.148, loss_scale=16, train_wall=136, gb_free=15.4, wall=82456
2023-10-17 22:13:12 | INFO | train_inner | epoch 001:  57793 / 1830643 loss=3.22, ppl=9.32, wps=23871.1, ups=0.73, wpb=32768, bsz=32, num_updates=57700, lr=8.46e-05, gnorm=0.15, loss_scale=16, train_wall=137, gb_free=15.4, wall=82593
2023-10-17 22:15:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 22:15:29 | INFO | train_inner | epoch 001:  57894 / 1830643 loss=3.538, ppl=11.61, wps=23887.8, ups=0.73, wpb=32768, bsz=32, num_updates=57800, lr=8.44e-05, gnorm=0.149, loss_scale=8, train_wall=137, gb_free=15.4, wall=82731
2023-10-17 22:17:45 | INFO | train_inner | epoch 001:  57994 / 1830643 loss=3.498, ppl=11.3, wps=24107.4, ups=0.74, wpb=32768, bsz=32, num_updates=57900, lr=8.42e-05, gnorm=0.145, loss_scale=8, train_wall=136, gb_free=15.4, wall=82866
2023-10-17 22:20:01 | INFO | train_inner | epoch 001:  58094 / 1830643 loss=3.336, ppl=10.1, wps=24167.2, ups=0.74, wpb=32768, bsz=32, num_updates=58000, lr=8.4e-05, gnorm=0.138, loss_scale=8, train_wall=135, gb_free=15.4, wall=83002
2023-10-17 22:22:16 | INFO | train_inner | epoch 001:  58194 / 1830643 loss=3.515, ppl=11.43, wps=24229, ups=0.74, wpb=32768, bsz=32, num_updates=58100, lr=8.38e-05, gnorm=0.153, loss_scale=8, train_wall=135, gb_free=15.4, wall=83137
2023-10-17 22:24:34 | INFO | train_inner | epoch 001:  58294 / 1830643 loss=3.514, ppl=11.42, wps=23806.2, ups=0.73, wpb=32768, bsz=32, num_updates=58200, lr=8.36e-05, gnorm=0.153, loss_scale=8, train_wall=137, gb_free=15.4, wall=83275
2023-10-17 22:27:01 | INFO | train_inner | epoch 001:  58394 / 1830643 loss=3.42, ppl=10.7, wps=22197.8, ups=0.68, wpb=32768, bsz=32, num_updates=58300, lr=8.34e-05, gnorm=0.15, loss_scale=8, train_wall=147, gb_free=15.4, wall=83423
2023-10-17 22:29:22 | INFO | train_inner | epoch 001:  58494 / 1830643 loss=3.164, ppl=8.96, wps=23300.7, ups=0.71, wpb=32768, bsz=32, num_updates=58400, lr=8.32e-05, gnorm=0.162, loss_scale=16, train_wall=140, gb_free=15.4, wall=83563
2023-10-17 22:31:37 | INFO | train_inner | epoch 001:  58594 / 1830643 loss=3.205, ppl=9.22, wps=24261.8, ups=0.74, wpb=32768, bsz=32, num_updates=58500, lr=8.3e-05, gnorm=0.151, loss_scale=16, train_wall=135, gb_free=15.4, wall=83698
2023-10-17 22:33:55 | INFO | train_inner | epoch 001:  58694 / 1830643 loss=3.243, ppl=9.47, wps=23698.7, ups=0.72, wpb=32768, bsz=32, num_updates=58600, lr=8.28e-05, gnorm=0.16, loss_scale=16, train_wall=138, gb_free=15.4, wall=83837
2023-10-17 22:36:11 | INFO | train_inner | epoch 001:  58794 / 1830643 loss=3.095, ppl=8.54, wps=24118.1, ups=0.74, wpb=32768, bsz=32, num_updates=58700, lr=8.26e-05, gnorm=0.15, loss_scale=16, train_wall=135, gb_free=15.4, wall=83972
2023-10-17 22:38:53 | INFO | train_inner | epoch 001:  58894 / 1830643 loss=3.228, ppl=9.37, wps=20206.1, ups=0.62, wpb=32768, bsz=32, num_updates=58800, lr=8.24e-05, gnorm=0.149, loss_scale=16, train_wall=162, gb_free=15.4, wall=84135
2023-10-17 22:40:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 22:41:22 | INFO | train_inner | epoch 001:  58995 / 1830643 loss=3.667, ppl=12.7, wps=22051, ups=0.67, wpb=32768, bsz=32, num_updates=58900, lr=8.22e-05, gnorm=0.147, loss_scale=16, train_wall=148, gb_free=15.4, wall=84283
2023-10-17 22:43:46 | INFO | train_inner | epoch 001:  59095 / 1830643 loss=3.485, ppl=11.2, wps=22814.5, ups=0.7, wpb=32768, bsz=32, num_updates=59000, lr=8.2e-05, gnorm=0.142, loss_scale=16, train_wall=143, gb_free=15.4, wall=84427
2023-10-17 22:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 22:46:11 | INFO | train_inner | epoch 001:  59196 / 1830643 loss=3.648, ppl=12.53, wps=22603.5, ups=0.69, wpb=32768, bsz=32, num_updates=59100, lr=8.18e-05, gnorm=0.148, loss_scale=8, train_wall=145, gb_free=15.4, wall=84572
2023-10-17 22:48:36 | INFO | train_inner | epoch 001:  59296 / 1830643 loss=3.414, ppl=10.66, wps=22483.8, ups=0.69, wpb=32768, bsz=32, num_updates=59200, lr=8.16e-05, gnorm=0.144, loss_scale=8, train_wall=145, gb_free=15.4, wall=84717
2023-10-17 22:51:00 | INFO | train_inner | epoch 001:  59396 / 1830643 loss=3.163, ppl=8.96, wps=22731.6, ups=0.69, wpb=32768, bsz=32, num_updates=59300, lr=8.14e-05, gnorm=0.146, loss_scale=8, train_wall=144, gb_free=15.4, wall=84862
2023-10-17 22:53:25 | INFO | train_inner | epoch 001:  59496 / 1830643 loss=3.526, ppl=11.52, wps=22633.7, ups=0.69, wpb=32768, bsz=32, num_updates=59400, lr=8.12e-05, gnorm=0.15, loss_scale=8, train_wall=144, gb_free=15.4, wall=85006
2023-10-17 22:55:59 | INFO | train_inner | epoch 001:  59596 / 1830643 loss=3.464, ppl=11.04, wps=21274.2, ups=0.65, wpb=32768, bsz=32, num_updates=59500, lr=8.1e-05, gnorm=0.146, loss_scale=8, train_wall=154, gb_free=15.4, wall=85160
2023-10-17 22:58:25 | INFO | train_inner | epoch 001:  59696 / 1830643 loss=3.376, ppl=10.38, wps=22543.4, ups=0.69, wpb=32768, bsz=32, num_updates=59600, lr=8.08e-05, gnorm=0.152, loss_scale=16, train_wall=145, gb_free=15.4, wall=85306
2023-10-17 23:00:49 | INFO | train_inner | epoch 001:  59796 / 1830643 loss=3.446, ppl=10.9, wps=22651.5, ups=0.69, wpb=32768, bsz=32, num_updates=59700, lr=8.06e-05, gnorm=0.156, loss_scale=16, train_wall=144, gb_free=15.4, wall=85450
2023-10-17 23:03:22 | INFO | train_inner | epoch 001:  59896 / 1830643 loss=3.319, ppl=9.98, wps=21509.1, ups=0.66, wpb=32768, bsz=32, num_updates=59800, lr=8.04e-05, gnorm=0.146, loss_scale=16, train_wall=152, gb_free=15.4, wall=85603
2023-10-17 23:05:46 | INFO | train_inner | epoch 001:  59996 / 1830643 loss=3.294, ppl=9.81, wps=22711.4, ups=0.69, wpb=32768, bsz=32, num_updates=59900, lr=8.02e-05, gnorm=0.143, loss_scale=16, train_wall=144, gb_free=15.4, wall=85747
2023-10-17 23:08:10 | INFO | train_inner | epoch 001:  60096 / 1830643 loss=3.506, ppl=11.36, wps=22790.4, ups=0.7, wpb=32768, bsz=32, num_updates=60000, lr=8e-05, gnorm=0.151, loss_scale=16, train_wall=143, gb_free=15.4, wall=85891
2023-10-17 23:08:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60000 updates
2023-10-17 23:08:10 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_60000.pt
2023-10-17 23:08:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_60000.pt
2023-10-17 23:08:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/5early/checkpoint_1_60000.pt (epoch 1 @ 60000 updates, score None) (writing took 19.312406779965386 seconds)
2023-10-17 23:10:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 23:10:55 | INFO | train_inner | epoch 001:  60197 / 1830643 loss=3.29, ppl=9.78, wps=19857.8, ups=0.61, wpb=32768, bsz=32, num_updates=60100, lr=7.98e-05, gnorm=0.145, loss_scale=16, train_wall=145, gb_free=15.4, wall=86056
2023-10-17 23:13:43 | INFO | train_inner | epoch 001:  60297 / 1830643 loss=3.513, ppl=11.42, wps=19525.6, ups=0.6, wpb=32768, bsz=32, num_updates=60200, lr=7.96e-05, gnorm=0.148, loss_scale=16, train_wall=167, gb_free=15.4, wall=86224
2023-10-17 23:16:35 | INFO | train_inner | epoch 001:  60397 / 1830643 loss=3.336, ppl=10.1, wps=18975.6, ups=0.58, wpb=32768, bsz=32, num_updates=60300, lr=7.94e-05, gnorm=0.14, loss_scale=16, train_wall=172, gb_free=15.4, wall=86396
2023-10-17 23:19:16 | INFO | train_inner | epoch 001:  60497 / 1830643 loss=3.359, ppl=10.26, wps=20433.6, ups=0.62, wpb=32768, bsz=32, num_updates=60400, lr=7.92e-05, gnorm=0.144, loss_scale=16, train_wall=160, gb_free=15.4, wall=86557
2023-10-17 23:21:40 | INFO | train_inner | epoch 001:  60597 / 1830643 loss=2.738, ppl=6.67, wps=22720.3, ups=0.69, wpb=32768, bsz=32, num_updates=60500, lr=7.9e-05, gnorm=0.149, loss_scale=16, train_wall=144, gb_free=15.4, wall=86701
2023-10-17 23:24:05 | INFO | train_inner | epoch 001:  60697 / 1830643 loss=2.908, ppl=7.51, wps=22642.2, ups=0.69, wpb=32768, bsz=32, num_updates=60600, lr=7.88e-05, gnorm=0.158, loss_scale=32, train_wall=144, gb_free=15.4, wall=86846
2023-10-17 23:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 23:26:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-17 23:26:31 | INFO | train_inner | epoch 001:  60799 / 1830643 loss=3.326, ppl=10.03, wps=22350.5, ups=0.68, wpb=32768, bsz=32, num_updates=60700, lr=7.86e-05, gnorm=0.141, loss_scale=8, train_wall=146, gb_free=15.4, wall=86992
2023-10-17 23:28:55 | INFO | train_inner | epoch 001:  60899 / 1830643 loss=3.445, ppl=10.89, wps=22719.6, ups=0.69, wpb=32768, bsz=32, num_updates=60800, lr=7.84e-05, gnorm=0.15, loss_scale=8, train_wall=144, gb_free=15.4, wall=87137
2023-10-17 23:31:18 | INFO | train_inner | epoch 001:  60999 / 1830643 loss=3.407, ppl=10.61, wps=22978.3, ups=0.7, wpb=32768, bsz=32, num_updates=60900, lr=7.82e-05, gnorm=0.156, loss_scale=8, train_wall=142, gb_free=15.4, wall=87279
2023-10-17 23:33:42 | INFO | train_inner | epoch 001:  61099 / 1830643 loss=3.455, ppl=10.97, wps=22742.5, ups=0.69, wpb=32768, bsz=32, num_updates=61000, lr=7.8e-05, gnorm=0.147, loss_scale=8, train_wall=144, gb_free=15.4, wall=87423
2023-10-17 23:36:05 | INFO | train_inner | epoch 001:  61199 / 1830643 loss=3.233, ppl=9.4, wps=22849.2, ups=0.7, wpb=32768, bsz=32, num_updates=61100, lr=7.78e-05, gnorm=0.159, loss_scale=8, train_wall=143, gb_free=15.4, wall=87567
2023-10-17 23:38:30 | INFO | train_inner | epoch 001:  61299 / 1830643 loss=2.765, ppl=6.8, wps=22727.8, ups=0.69, wpb=32768, bsz=32, num_updates=61200, lr=7.76e-05, gnorm=0.148, loss_scale=8, train_wall=144, gb_free=15.4, wall=87711
2023-10-17 23:40:54 | INFO | train_inner | epoch 001:  61399 / 1830643 loss=2.788, ppl=6.91, wps=22750.9, ups=0.69, wpb=32768, bsz=32, num_updates=61300, lr=7.74e-05, gnorm=0.148, loss_scale=16, train_wall=144, gb_free=15.4, wall=87855
2023-10-17 23:43:18 | INFO | train_inner | epoch 001:  61499 / 1830643 loss=3.316, ppl=9.96, wps=22708.7, ups=0.69, wpb=32768, bsz=32, num_updates=61400, lr=7.72e-05, gnorm=0.151, loss_scale=16, train_wall=144, gb_free=15.4, wall=87999
2023-10-17 23:45:43 | INFO | train_inner | epoch 001:  61599 / 1830643 loss=3.454, ppl=10.96, wps=22592.8, ups=0.69, wpb=32768, bsz=32, num_updates=61500, lr=7.7e-05, gnorm=0.147, loss_scale=16, train_wall=145, gb_free=15.4, wall=88144
2023-10-17 23:48:08 | INFO | train_inner | epoch 001:  61699 / 1830643 loss=3.163, ppl=8.96, wps=22674.6, ups=0.69, wpb=32768, bsz=32, num_updates=61600, lr=7.68e-05, gnorm=0.142, loss_scale=16, train_wall=144, gb_free=15.4, wall=88289
2023-10-17 23:50:33 | INFO | train_inner | epoch 001:  61799 / 1830643 loss=3.372, ppl=10.35, wps=22594.8, ups=0.69, wpb=32768, bsz=32, num_updates=61700, lr=7.66e-05, gnorm=0.15, loss_scale=16, train_wall=145, gb_free=15.4, wall=88434
2023-10-17 23:52:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-17 23:52:58 | INFO | train_inner | epoch 001:  61900 / 1830643 loss=3.491, ppl=11.24, wps=22460.8, ups=0.69, wpb=32768, bsz=32, num_updates=61800, lr=7.64e-05, gnorm=0.16, loss_scale=16, train_wall=146, gb_free=15.4, wall=88580
2023-10-17 23:55:23 | INFO | train_inner | epoch 001:  62000 / 1830643 loss=3.529, ppl=11.55, wps=22701.4, ups=0.69, wpb=32768, bsz=32, num_updates=61900, lr=7.62e-05, gnorm=0.152, loss_scale=16, train_wall=144, gb_free=15.4, wall=88724
2023-10-17 23:57:47 | INFO | train_inner | epoch 001:  62100 / 1830643 loss=3.345, ppl=10.16, wps=22746.3, ups=0.69, wpb=32768, bsz=32, num_updates=62000, lr=7.6e-05, gnorm=0.149, loss_scale=16, train_wall=144, gb_free=15.4, wall=88868
2023-10-18 00:00:12 | INFO | train_inner | epoch 001:  62200 / 1830643 loss=3.247, ppl=9.5, wps=22607.6, ups=0.69, wpb=32768, bsz=32, num_updates=62100, lr=7.58e-05, gnorm=0.153, loss_scale=16, train_wall=145, gb_free=15.4, wall=89013
2023-10-18 00:02:36 | INFO | train_inner | epoch 001:  62300 / 1830643 loss=3.41, ppl=10.63, wps=22709, ups=0.69, wpb=32768, bsz=32, num_updates=62200, lr=7.56e-05, gnorm=0.147, loss_scale=16, train_wall=144, gb_free=15.4, wall=89157
2023-10-18 00:03:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-18 00:05:02 | INFO | train_inner | epoch 001:  62401 / 1830643 loss=3.473, ppl=11.1, wps=22459.9, ups=0.69, wpb=32768, bsz=32, num_updates=62300, lr=7.54e-05, gnorm=0.145, loss_scale=8, train_wall=146, gb_free=15.4, wall=89303
2023-10-18 00:07:26 | INFO | train_inner | epoch 001:  62501 / 1830643 loss=3.393, ppl=10.5, wps=22749.1, ups=0.69, wpb=32768, bsz=32, num_updates=62400, lr=7.52e-05, gnorm=0.155, loss_scale=8, train_wall=144, gb_free=15.4, wall=89447
2023-10-18 00:09:51 | INFO | train_inner | epoch 001:  62601 / 1830643 loss=3.438, ppl=10.84, wps=22631.8, ups=0.69, wpb=32768, bsz=32, num_updates=62500, lr=7.5e-05, gnorm=0.149, loss_scale=8, train_wall=144, gb_free=15.4, wall=89592
2023-10-18 00:12:14 | INFO | train_inner | epoch 001:  62701 / 1830643 loss=3.419, ppl=10.7, wps=22946.3, ups=0.7, wpb=32768, bsz=32, num_updates=62600, lr=7.48e-05, gnorm=0.157, loss_scale=8, train_wall=142, gb_free=15.4, wall=89735
2023-10-18 00:14:37 | INFO | train_inner | epoch 001:  62801 / 1830643 loss=3.374, ppl=10.37, wps=22797.7, ups=0.7, wpb=32768, bsz=32, num_updates=62700, lr=7.46e-05, gnorm=0.15, loss_scale=8, train_wall=143, gb_free=15.4, wall=89879
2023-10-18 00:17:02 | INFO | train_inner | epoch 001:  62901 / 1830643 loss=3.446, ppl=10.9, wps=22726.2, ups=0.69, wpb=32768, bsz=32, num_updates=62800, lr=7.44e-05, gnorm=0.14, loss_scale=16, train_wall=144, gb_free=15.4, wall=90023
2023-10-18 00:18:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-18 00:19:27 | INFO | train_inner | epoch 001:  63002 / 1830643 loss=3.395, ppl=10.52, wps=22567.7, ups=0.69, wpb=32768, bsz=32, num_updates=62900, lr=7.42e-05, gnorm=0.16, loss_scale=8, train_wall=145, gb_free=15.4, wall=90168
2023-10-18 00:21:51 | INFO | train_inner | epoch 001:  63102 / 1830643 loss=3.321, ppl=9.99, wps=22761, ups=0.69, wpb=32768, bsz=32, num_updates=63000, lr=7.4e-05, gnorm=0.151, loss_scale=8, train_wall=144, gb_free=15.4, wall=90312
2023-10-18 00:24:15 | INFO | train_inner | epoch 001:  63202 / 1830643 loss=3.285, ppl=9.75, wps=22716.9, ups=0.69, wpb=32768, bsz=32, num_updates=63100, lr=7.38e-05, gnorm=0.148, loss_scale=8, train_wall=144, gb_free=15.4, wall=90456
2023-10-18 00:26:39 | INFO | train_inner | epoch 001:  63302 / 1830643 loss=3.417, ppl=10.68, wps=22670.6, ups=0.69, wpb=32768, bsz=32, num_updates=63200, lr=7.36e-05, gnorm=0.16, loss_scale=8, train_wall=144, gb_free=15.4, wall=90601
2023-10-18 00:29:04 | INFO | train_inner | epoch 001:  63402 / 1830643 loss=3.499, ppl=11.31, wps=22642.6, ups=0.69, wpb=32768, bsz=32, num_updates=63300, lr=7.34e-05, gnorm=0.155, loss_scale=8, train_wall=144, gb_free=15.4, wall=90745
2023-10-18 00:31:28 | INFO | train_inner | epoch 001:  63502 / 1830643 loss=3.338, ppl=10.11, wps=22744.2, ups=0.69, wpb=32768, bsz=32, num_updates=63400, lr=7.32e-05, gnorm=0.14, loss_scale=16, train_wall=144, gb_free=15.4, wall=90889
2023-10-18 00:33:53 | INFO | train_inner | epoch 001:  63602 / 1830643 loss=3.311, ppl=9.92, wps=22621.3, ups=0.69, wpb=32768, bsz=32, num_updates=63500, lr=7.3e-05, gnorm=0.148, loss_scale=16, train_wall=144, gb_free=15.4, wall=91034
2023-10-18 00:36:17 | INFO | train_inner | epoch 001:  63702 / 1830643 loss=3.323, ppl=10.01, wps=22748.9, ups=0.69, wpb=32768, bsz=32, num_updates=63600, lr=7.28e-05, gnorm=0.143, loss_scale=16, train_wall=144, gb_free=15.4, wall=91178
2023-10-18 00:38:41 | INFO | train_inner | epoch 001:  63802 / 1830643 loss=3.364, ppl=10.29, wps=22729.3, ups=0.69, wpb=32768, bsz=32, num_updates=63700, lr=7.26e-05, gnorm=0.147, loss_scale=16, train_wall=144, gb_free=15.4, wall=91323
2023-10-18 00:41:05 | INFO | train_inner | epoch 001:  63902 / 1830643 loss=3.548, ppl=11.7, wps=22764.1, ups=0.69, wpb=32768, bsz=32, num_updates=63800, lr=7.24e-05, gnorm=0.158, loss_scale=16, train_wall=144, gb_free=15.4, wall=91466
2023-10-18 00:43:29 | INFO | train_inner | epoch 001:  64002 / 1830643 loss=3.447, ppl=10.91, wps=22770.3, ups=0.69, wpb=32768, bsz=32, num_updates=63900, lr=7.22e-05, gnorm=0.145, loss_scale=32, train_wall=144, gb_free=15.4, wall=91610
2023-10-18 00:45:53 | INFO | train_inner | epoch 001:  64102 / 1830643 loss=3.315, ppl=9.95, wps=22770.3, ups=0.69, wpb=32768, bsz=32, num_updates=64000, lr=7.2e-05, gnorm=0.139, loss_scale=32, train_wall=144, gb_free=15.4, wall=91754
2023-10-18 00:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-18 00:48:18 | INFO | train_inner | epoch 001:  64203 / 1830643 loss=3.373, ppl=10.36, wps=22603, ups=0.69, wpb=32768, bsz=32, num_updates=64100, lr=7.18e-05, gnorm=0.139, loss_scale=16, train_wall=145, gb_free=15.4, wall=91899
2023-10-18 00:50:41 | INFO | train_inner | epoch 001:  64303 / 1830643 loss=3.596, ppl=12.09, wps=22858.6, ups=0.7, wpb=32768, bsz=32, num_updates=64200, lr=7.16e-05, gnorm=0.148, loss_scale=16, train_wall=143, gb_free=15.4, wall=92043
2023-10-18 00:53:05 | INFO | train_inner | epoch 001:  64403 / 1830643 loss=3.391, ppl=10.49, wps=22825.5, ups=0.7, wpb=32768, bsz=32, num_updates=64300, lr=7.14e-05, gnorm=0.145, loss_scale=16, train_wall=143, gb_free=15.4, wall=92186
2023-10-18 00:55:29 | INFO | train_inner | epoch 001:  64503 / 1830643 loss=3.448, ppl=10.91, wps=22806.2, ups=0.7, wpb=32768, bsz=32, num_updates=64400, lr=7.12e-05, gnorm=0.149, loss_scale=16, train_wall=143, gb_free=15.4, wall=92330
2023-10-18 00:57:52 | INFO | train_inner | epoch 001:  64603 / 1830643 loss=3.693, ppl=12.93, wps=22873.3, ups=0.7, wpb=32768, bsz=32, num_updates=64500, lr=7.1e-05, gnorm=0.161, loss_scale=16, train_wall=143, gb_free=15.4, wall=92473
2023-10-18 00:58:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-18 01:00:17 | INFO | train_inner | epoch 001:  64704 / 1830643 loss=3.401, ppl=10.57, wps=22601.6, ups=0.69, wpb=32768, bsz=32, num_updates=64600, lr=7.08e-05, gnorm=0.152, loss_scale=16, train_wall=145, gb_free=15.4, wall=92618
2023-10-18 01:01:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-18 01:02:42 | INFO | train_inner | epoch 001:  64805 / 1830643 loss=3.261, ppl=9.58, wps=22605.7, ups=0.69, wpb=32768, bsz=32, num_updates=64700, lr=7.06e-05, gnorm=0.162, loss_scale=8, train_wall=145, gb_free=15.4, wall=92763
2023-10-18 01:05:06 | INFO | train_inner | epoch 001:  64905 / 1830643 loss=3.394, ppl=10.51, wps=22779.4, ups=0.7, wpb=32768, bsz=32, num_updates=64800, lr=7.04e-05, gnorm=0.164, loss_scale=8, train_wall=143, gb_free=15.4, wall=92907
2023-10-18 01:07:29 | INFO | train_inner | epoch 001:  65005 / 1830643 loss=3.35, ppl=10.2, wps=22805.9, ups=0.7, wpb=32768, bsz=32, num_updates=64900, lr=7.02e-05, gnorm=0.158, loss_scale=8, train_wall=143, gb_free=15.4, wall=93051
2023-10-18 01:09:53 | INFO | train_inner | epoch 001:  65105 / 1830643 loss=3.232, ppl=9.4, wps=22817.4, ups=0.7, wpb=32768, bsz=32, num_updates=65000, lr=7e-05, gnorm=0.149, loss_scale=8, train_wall=143, gb_free=15.4, wall=93194
2023-10-18 01:09:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-18 01:12:19 | INFO | train_inner | epoch 001:  65206 / 1830643 loss=3.243, ppl=9.47, wps=22515.5, ups=0.69, wpb=32768, bsz=32, num_updates=65100, lr=6.98e-05, gnorm=0.154, loss_scale=4, train_wall=145, gb_free=15.4, wall=93340
2023-10-18 01:14:42 | INFO | train_inner | epoch 001:  65306 / 1830643 loss=3.365, ppl=10.31, wps=22806.7, ups=0.7, wpb=32768, bsz=32, num_updates=65200, lr=6.96e-05, gnorm=0.157, loss_scale=4, train_wall=143, gb_free=15.4, wall=93483
2023-10-18 01:17:06 | INFO | train_inner | epoch 001:  65406 / 1830643 loss=3.229, ppl=9.38, wps=22823.6, ups=0.7, wpb=32768, bsz=32, num_updates=65300, lr=6.94e-05, gnorm=0.158, loss_scale=4, train_wall=143, gb_free=15.4, wall=93627
2023-10-18 01:19:29 | INFO | train_inner | epoch 001:  65506 / 1830643 loss=3.276, ppl=9.69, wps=22849.1, ups=0.7, wpb=32768, bsz=32, num_updates=65400, lr=6.92e-05, gnorm=0.156, loss_scale=4, train_wall=143, gb_free=15.4, wall=93770
2023-10-18 01:21:53 | INFO | train_inner | epoch 001:  65606 / 1830643 loss=3.417, ppl=10.68, wps=22795.7, ups=0.7, wpb=32768, bsz=32, num_updates=65500, lr=6.9e-05, gnorm=0.153, loss_scale=4, train_wall=143, gb_free=15.4, wall=93914
2023-10-18 01:24:16 | INFO | train_inner | epoch 001:  65706 / 1830643 loss=3.486, ppl=11.2, wps=22895.1, ups=0.7, wpb=32768, bsz=32, num_updates=65600, lr=6.88e-05, gnorm=0.144, loss_scale=8, train_wall=143, gb_free=15.4, wall=94057
2023-10-18 01:26:40 | INFO | train_inner | epoch 001:  65806 / 1830643 loss=3.438, ppl=10.84, wps=22803.6, ups=0.7, wpb=32768, bsz=32, num_updates=65700, lr=6.86e-05, gnorm=0.141, loss_scale=8, train_wall=143, gb_free=15.4, wall=94201
2023-10-18 01:29:03 | INFO | train_inner | epoch 001:  65906 / 1830643 loss=3.564, ppl=11.82, wps=22817, ups=0.7, wpb=32768, bsz=32, num_updates=65800, lr=6.84e-05, gnorm=0.155, loss_scale=8, train_wall=143, gb_free=15.4, wall=94345
2023-10-18 01:29:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-18 01:31:29 | INFO | train_inner | epoch 001:  66007 / 1830643 loss=3.92, ppl=15.14, wps=22558.9, ups=0.69, wpb=32768, bsz=32, num_updates=65900, lr=6.82e-05, gnorm=0.159, loss_scale=4, train_wall=145, gb_free=15.4, wall=94490
2023-10-18 01:33:52 | INFO | train_inner | epoch 001:  66107 / 1830643 loss=3.713, ppl=13.11, wps=22863.5, ups=0.7, wpb=32768, bsz=32, num_updates=66000, lr=6.8e-05, gnorm=0.16, loss_scale=4, train_wall=143, gb_free=15.4, wall=94633
2023-10-18 01:36:16 | INFO | train_inner | epoch 001:  66207 / 1830643 loss=3.358, ppl=10.25, wps=22755.5, ups=0.69, wpb=32768, bsz=32, num_updates=66100, lr=6.78e-05, gnorm=0.144, loss_scale=4, train_wall=144, gb_free=15.4, wall=94777
2023-10-18 01:38:40 | INFO | train_inner | epoch 001:  66307 / 1830643 loss=3.532, ppl=11.57, wps=22789.8, ups=0.7, wpb=32768, bsz=32, num_updates=66200, lr=6.76e-05, gnorm=0.158, loss_scale=4, train_wall=143, gb_free=15.4, wall=94921
2023-10-18 01:41:03 | INFO | train_inner | epoch 001:  66407 / 1830643 loss=3.411, ppl=10.64, wps=22799.7, ups=0.7, wpb=32768, bsz=32, num_updates=66300, lr=6.74e-05, gnorm=0.156, loss_scale=4, train_wall=143, gb_free=15.4, wall=95065
2023-10-18 01:43:27 | INFO | train_inner | epoch 001:  66507 / 1830643 loss=3.242, ppl=9.46, wps=22769.4, ups=0.69, wpb=32768, bsz=32, num_updates=66400, lr=6.72e-05, gnorm=0.152, loss_scale=8, train_wall=144, gb_free=15.4, wall=95209
2023-10-18 01:45:53 | INFO | train_inner | epoch 001:  66607 / 1830643 loss=3.67, ppl=12.73, wps=22568.8, ups=0.69, wpb=32768, bsz=32, num_updates=66500, lr=6.7e-05, gnorm=0.162, loss_scale=8, train_wall=145, gb_free=15.4, wall=95354
2023-10-18 01:48:16 | INFO | train_inner | epoch 001:  66707 / 1830643 loss=3.675, ppl=12.77, wps=22820.6, ups=0.7, wpb=32768, bsz=32, num_updates=66600, lr=6.68e-05, gnorm=0.152, loss_scale=8, train_wall=143, gb_free=15.4, wall=95497
2023-10-18 01:50:40 | INFO | train_inner | epoch 001:  66807 / 1830643 loss=3.562, ppl=11.81, wps=22767.3, ups=0.69, wpb=32768, bsz=32, num_updates=66700, lr=6.66e-05, gnorm=0.147, loss_scale=8, train_wall=144, gb_free=15.4, wall=95641
2023-10-18 01:53:24 | INFO | train_inner | epoch 001:  66907 / 1830643 loss=3.359, ppl=10.26, wps=19946.3, ups=0.61, wpb=32768, bsz=32, num_updates=66800, lr=6.64e-05, gnorm=0.144, loss_scale=8, train_wall=164, gb_free=15.4, wall=95806
2023-10-18 01:53:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: Error in virtual void* faiss::gpu::StandardGpuResourcesImpl::allocMemory(const faiss::gpu::AllocRequest&) at /root/miniconda3/conda-bld/faiss-pkg_1623030479928/work/faiss/gpu/StandardGpuResources.cpp:452: Error: 'err == cudaSuccess' failed: StandardGpuResources: alloc fail type FlatData dev 0 space Device stream 0x617e98f0 size 3997696 bytes (cudaMalloc error out of memory [2])

2023-10-18 01:53:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6213 MiB |   6998 MiB |  10141 TiB |  10141 TiB |
|       from large pool |   6201 MiB |   6983 MiB |  10088 TiB |  10088 TiB |
|       from small pool |     11 MiB |     16 MiB |     52 TiB |     52 TiB |
|---------------------------------------------------------------------------|
| Active memory         |   6213 MiB |   6998 MiB |  10141 TiB |  10141 TiB |
|       from large pool |   6201 MiB |   6983 MiB |  10088 TiB |  10088 TiB |
|       from small pool |     11 MiB |     16 MiB |     52 TiB |     52 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6213 MiB |   6998 MiB |  10141 TiB |  10141 TiB |
|       from large pool |   6201 MiB |   6983 MiB |  10088 TiB |  10088 TiB |
|       from small pool |     11 MiB |     16 MiB |     52 TiB |     52 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   7916 MiB |   7916 MiB | 112642 MiB | 104726 MiB |
|       from large pool |   7896 MiB |   7896 MiB | 112076 MiB | 104180 MiB |
|       from small pool |     20 MiB |     20 MiB |    566 MiB |    546 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 231701 KiB |   1467 MiB |   6828 TiB |   6828 TiB |
|       from large pool | 227569 KiB |   1460 MiB |   6775 TiB |   6775 TiB |
|       from small pool |   4131 KiB |      7 MiB |     53 TiB |     53 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     764    |     809    |     963 M  |     963 M  |
|       from large pool |     477    |     514    |     757 M  |     757 M  |
|       from small pool |     287    |     312    |     206 M  |     206 M  |
|---------------------------------------------------------------------------|
| Active allocs         |     764    |     809    |     963 M  |     963 M  |
|       from large pool |     477    |     514    |     757 M  |     757 M  |
|       from small pool |     287    |     312    |     206 M  |     206 M  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     107    |    2107    |    2000    |
|       from large pool |      97    |      97    |    1824    |    1727    |
|       from small pool |      10    |      10    |     283    |     273    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      75    |  622977 K  |  622977 K  |
|       from large pool |      13    |      52    |  505439 K  |  505439 K  |
|       from small pool |      11    |      25    |  117538 K  |  117538 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-10-18 01:53:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-10-18 01:53:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-10-18 01:53:54 | WARNING | fairseq.trainer | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
2023-10-18 01:53:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
Traceback (most recent call last):
  File "/data/zyu401_data/anirudh/longmem/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq_cli/train.py", line 561, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq_cli/train.py", line 320, in train
    log_output = trainer.train_step(samples)
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/trainer.py", line 894, in train_step
    ) = self._aggregate_logging_outputs(
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/trainer.py", line 1375, in _aggregate_logging_outputs
    return self._fast_stat_sync_sum(
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/trainer.py", line 1438, in _fast_stat_sync_sum
    data = distributed_utils.all_reduce_dict(
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 669, in all_reduce_dict
    cpu_data = _all_reduce_dict(cpu_data)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 664, in _all_reduce_dict
    all_reduce(buf, group=group)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 503, in all_reduce
    dist.all_reduce(tensor, op=op, group=group)
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1451, in wrapper
    return func(*args, **kwargs)
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1702, in all_reduce
    work = group.allreduce([tensor], opts)
RuntimeError: Detected mismatch between collectives on ranks. Rank 0 is running collective: CollectiveFingerPrint(OpType=ALLREDUCE, TensorShape=[6], TensorDtypes=Double, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))), but Rank 1 is running collective: CollectiveFingerPrint(OpType=ALLREDUCE).

/data/zyu401_data/anirudh/longmem/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
