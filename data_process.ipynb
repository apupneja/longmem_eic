{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "from fairseq.data.encoders.gpt2_bpe import GPT2BPE\n",
    "from fairseq.data import Dictionary\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import itertools\n",
    "DEFAULT_ENCODER_JSON = \"https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\"\n",
    "DEFAULT_VOCAB_BPE = \"https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_template_dict = {\"SST-2\": \"Review: {} Sentiment: {}. \",\n",
    "                          \"sst-5\": \"Review: {} Sentiment: {}. \",\n",
    "                            \"mr\": \"Review: {} Sentiment: {}. \",\n",
    "                          \"mpqa\": \"Review: {} Sentiment: {}. \",\n",
    "                          \"subj\": \"Input: {} Type: {}. \"}\n",
    "\n",
    "def load_data(task):\n",
    "    data = {}\n",
    "    path = os.path.join(\"data/icl/original/\", task)\n",
    "    if task == \"SST-2\":\n",
    "        for split in [\"train\", \"validation\",]:\n",
    "            data[split] = []\n",
    "            file_split = \"dev\" if split == \"validation\" else split\n",
    "            \n",
    "            with open(os.path.join(path, \"{}.tsv\".format(file_split)), \"r\") as f:\n",
    "                for line in tqdm(f.readlines()):\n",
    "                    review, label = line.strip(\"\\n\").split(\"\\t\")\n",
    "                    if label == \"label\":\n",
    "                        continue\n",
    "                    label = \"positive\" if int(label) == 1 else \"negative\"\n",
    "                    data[split].append([review, label])\n",
    "    \n",
    "    if task == \"sst-5\":\n",
    "        mapping = {0: \"terrible\", 1: \"bad\", 2: \"okay\", 3: \"good\", 4: \"great\"}\n",
    "        for split in [\"train\", \"validation\", \"test\"]:\n",
    "            file_split = \"dev\" if split == \"validation\" else split\n",
    "            if os.path.exists(os.path.join(path, \"stsa.fine.{}\".format(file_split))):\n",
    "                data[split] = []\n",
    "                with open(os.path.join(path, \"stsa.fine.{}\".format(file_split)), \"r\") as f:\n",
    "                    for line in tqdm(f.readlines()):\n",
    "                        review, label = line[2:], line[0]                        \n",
    "                        label = mapping[int(label)]\n",
    "                        data[split].append([review.strip(\"\\n\"), label])\n",
    "\n",
    "    elif task in [\"mr\", \"mpqa\"]:\n",
    "        for split in [\"train\", \"validation\", \"test\"]:\n",
    "            if os.path.exists(os.path.join(path, \"{}.csv\".format(split))):\n",
    "                data[split] = []\n",
    "                with open(os.path.join(path, \"{}.csv\".format(split)), \"r\") as f:\n",
    "                    for line in tqdm(f.readlines()):\n",
    "                        review, label = line[2:], line[0]\n",
    "                        label = \"positive\" if int(label) == 1 else \"negative\"\n",
    "                        data[split].append([review.strip(\"\\n\"), label])\n",
    "        print(data['test'][0])\n",
    "\n",
    "    elif task in [\"subj\"]:\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            if os.path.exists(os.path.join(path, \"{}.csv\".format(split))):\n",
    "                data[split] = []\n",
    "                with open(os.path.join(path, \"{}.csv\".format(split)), \"r\") as f:\n",
    "                    for line in tqdm(f.readlines()):\n",
    "                        review, label = line[2:], line[0]\n",
    "                        label = \"subjective\" if int(label) == 0 else \"objective\"\n",
    "                        data[split].append([review.strip(\"\\n\").strip('\"'), label])\n",
    "        print(data['test'][0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  GPT2BPE(Namespace(gpt2_vocab_bpe=DEFAULT_VOCAB_BPE, gpt2_encoder_json=DEFAULT_ENCODER_JSON))\n",
    "dictionary = Dictionary.load(os.path.join(\"gpt2_bpe\", \"dict.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(token_number, data, task_template):\n",
    "    desired_num_tokens = token_number\n",
    "    current_num_tokens = 0\n",
    "    memory_set = []\n",
    "\n",
    "    for idx, s in enumerate(data['train']):\n",
    "        # Format the task template\n",
    "        task_template_line = task_template.format(s[0], s[1])\n",
    "\n",
    "        # Tokenize the line\n",
    "        tokenized_line = tokenizer.encode(task_template_line)\n",
    "        size = len(dictionary.encode_line(tokenized_line, add_if_not_exist=False).tolist())+1\n",
    "\n",
    "        # Check if adding the current line exceeds the desired number of tokens\n",
    "        if current_num_tokens + size <= desired_num_tokens:\n",
    "            memory_set.append(task_template_line)\n",
    "            current_num_tokens += size\n",
    "        else:\n",
    "            break  # Stop adding lines if the desired token limit is reached\n",
    "\n",
    "    return memory_set\n",
    "\n",
    "def get_imdb_data(token_number):\n",
    "    imdb_dataset = load_dataset(\"imdb\", cache_dir=\"/data/zyu401_data/anirudh/hf_data/\")\n",
    "\n",
    "    desired_num_tokens = token_number\n",
    "    current_num_tokens = 0\n",
    "    imdb_subset = []\n",
    "\n",
    "    # Iterate through the IMDb dataset and tokenize reviews\n",
    "    for example in imdb_dataset[\"train\"]:\n",
    "        review = example[\"text\"]\n",
    "        label = example[\"label\"]\n",
    "        concatenated_example = f\"Review: {review} Label: {label}\"\n",
    "        tokenized_review = tokenizer.encode(concatenated_example)\n",
    "\n",
    "        size = len(dictionary.encode_line(tokenized_review, add_if_not_exist=False).tolist())+1\n",
    "\n",
    "        # Check if adding the current review exceeds the desired number of tokens\n",
    "        if current_num_tokens + size <= desired_num_tokens:\n",
    "            imdb_subset.append(concatenated_example)\n",
    "            current_num_tokens += size\n",
    "        else:\n",
    "            print(\"EXEC FINE\")\n",
    "            break  # Stop adding reviews if the desired token limit is reached\n",
    "\n",
    "    return imdb_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6921/6921 [00:00<00:00, 832915.70it/s]\n",
      "100%|██████████| 873/873 [00:00<00:00, 1192324.13it/s]\n",
      "100%|██████████| 8544/8544 [00:00<00:00, 1273403.93it/s]\n",
      "100%|██████████| 1101/1101 [00:00<00:00, 1418718.50it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 1464362.06it/s]\n",
      "100%|██████████| 8662/8662 [00:00<00:00, 1168389.17it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1024375.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"simplistic , silly and tedious .\"', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8606/8606 [00:00<00:00, 1462864.45it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1598743.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complaining', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 1156451.21it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1313182.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart and alert , thirteen conversations about one thing is a small gem .', 'subjective']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/data/zyu401_data/anirudh/hf_data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a79e25d530643dd96f2d95fb2dfa4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXEC FINE\n",
      "7966\n"
     ]
    }
   ],
   "source": [
    "## Dataset 1\n",
    "tasks = [\"SST-2\", \"sst-5\", \"mr\", \"mpqa\",\"subj\"]\n",
    "token_number = [6500//8+1, 6500//8+1, 6500//8+1, 6500//8+1, 6500//8+1]\n",
    "\n",
    "data_comp = []\n",
    "for i, task in enumerate(tasks):\n",
    "    data = load_data(task)\n",
    "    local_data = get_data(token_number[i], data, task_template_dict[task])\n",
    "    data_comp.extend(local_data)\n",
    "\n",
    "imdb_data = get_imdb_data(32500//8+1)\n",
    "data_comp.extend(imdb_data)\n",
    "\n",
    "tokenized_lines = [tokenizer.encode(line) for line in data_comp]\n",
    "tokenized_ids = [[dictionary.bos()] + dictionary.encode_line(line, add_if_not_exist=False).tolist() for line in tokenized_lines]\n",
    "article_tokens = list(itertools.chain(*tokenized_ids))\n",
    "\n",
    "print(len(article_tokens))\n",
    "data_comp = np.array(data_comp)\n",
    "np.save('/data/zyu401_data/anirudh/d1.npy', data_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6921/6921 [00:00<00:00, 546671.02it/s]\n",
      "100%|██████████| 873/873 [00:00<00:00, 975913.48it/s]\n",
      "100%|██████████| 8544/8544 [00:00<00:00, 1166237.09it/s]\n",
      "100%|██████████| 1101/1101 [00:00<00:00, 1166143.61it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 1050834.58it/s]\n",
      "100%|██████████| 8662/8662 [00:00<00:00, 1218141.20it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1456102.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"simplistic , silly and tedious .\"', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8606/8606 [00:00<00:00, 1370029.99it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1588751.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complaining', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 58772.46it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1282465.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart and alert , thirteen conversations about one thing is a small gem .', 'subjective']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/data/zyu401_data/anirudh/hf_data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c484fc9be964da9923e76ab946c2f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXEC FINE\n",
      "85719\n"
     ]
    }
   ],
   "source": [
    "## Dataset 2\n",
    "tasks = [\"SST-2\", \"sst-5\", \"mr\", \"mpqa\",\"subj\"]\n",
    "token_number = [37500//8+1, 37500//8+1, 37500//8+1, 37500//8+1, 37500//8+1]\n",
    "\n",
    "data_comp = []\n",
    "for i, task in enumerate(tasks):\n",
    "    data = load_data(task)\n",
    "    local_data = get_data(token_number[i], data, task_template_dict[task])\n",
    "    data_comp.extend(local_data)\n",
    "\n",
    "imdb_data = get_imdb_data(62500)\n",
    "data_comp.extend(imdb_data)\n",
    "\n",
    "tokenized_lines = [tokenizer.encode(line) for line in data_comp]\n",
    "tokenized_ids = [[dictionary.bos()] + dictionary.encode_line(line, add_if_not_exist=False).tolist() for line in tokenized_lines]\n",
    "article_tokens = list(itertools.chain(*tokenized_ids))\n",
    "\n",
    "print(len(article_tokens))\n",
    "\n",
    "data_comp = np.array(data_comp)\n",
    "np.save('/data/zyu401_data/anirudh/d2.npy', data_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6921/6921 [00:00<00:00, 862591.09it/s]\n",
      "100%|██████████| 873/873 [00:00<00:00, 602737.02it/s]\n",
      "100%|██████████| 8544/8544 [00:00<00:00, 1004347.79it/s]\n",
      "100%|██████████| 1101/1101 [00:00<00:00, 1081988.92it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 1532640.85it/s]\n",
      "100%|██████████| 8662/8662 [00:00<00:00, 730713.22it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 816409.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"simplistic , silly and tedious .\"', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8606/8606 [00:00<00:00, 1329607.35it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 1584849.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complaining', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 667763.18it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 968214.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart and alert , thirteen conversations about one thing is a small gem .', 'subjective']\n",
      "124878\n"
     ]
    }
   ],
   "source": [
    "## Dataset 2\n",
    "tasks = [\"SST-2\", \"sst-5\", \"mr\", \"mpqa\",\"subj\"]\n",
    "token_number = [200000//8+1, 200000//8+1, 200000//8+1, 200000//8+1, 200000//8+1]\n",
    "\n",
    "data_comp = []\n",
    "for i, task in enumerate(tasks):\n",
    "    data = load_data(task)\n",
    "    local_data = get_data(token_number[i], data, task_template_dict[task])\n",
    "    data_comp.extend(local_data)\n",
    "\n",
    "tokenized_lines = [tokenizer.encode(line) for line in data_comp]\n",
    "tokenized_ids = [[dictionary.bos()] + dictionary.encode_line(line, add_if_not_exist=False).tolist() for line in tokenized_lines]\n",
    "article_tokens = list(itertools.chain(*tokenized_ids))\n",
    "\n",
    "print(len(article_tokens))\n",
    "\n",
    "data_comp = np.array(data_comp)\n",
    "np.save('/data/zyu401_data/anirudh/d3.npy', data_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_comp:\n",
    "    print(len(tokenizer.encode(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
