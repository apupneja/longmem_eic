2023-10-12 13:14:42 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:42 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:14:46 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:15707
2023-10-12 13:14:46 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:15707
2023-10-12 13:14:46 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:15707
2023-10-12 13:14:47 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:15707
2023-10-12 13:14:47 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:15707
2023-10-12 13:14:47 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:15707
2023-10-12 13:14:47 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:15707
2023-10-12 13:14:47 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:15707
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 6
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 7
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 3
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 1
2023-10-12 13:14:48 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 5
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 0
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 4
2023-10-12 13:14:48 | INFO | fairseq.distributed.utils | initialized host eic-gt-gpu1 as rank 2
2023-10-12 13:14:51 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15707', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_sidenet_gpt2_small', 'activation_fn': gelu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 12, 'decoder_attention_heads': 16, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': True, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'reduction_factor': 8, 'layer_reduction_factor': 2, 'reload_ptm_layer': True, 'tune_lm_head': False, 'gpt_encoder_path': 'gpt2_bpe', 'use_external_memory': True, 'retrieval_layer_index': 17, 'chunk_size': 4, 'dstore_fp16': False, 'use_gpu_to_search': True, 'move_dstore_to_mem': False, 'dstore_size': 10000000, 'k': 64, 'probe': 32, 'dstore_filename': 'data/datastore', 'long_context_attention': False, 'memory_size': 1638400, 'precompute_mem_layer': 0, 'pretrained_model_path': '/data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt', 'add_bos_token': False, 'tokens_per_sample': 1024, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': '/data/zyu401_data/anirudh/longmem_data/data-bin/longmem', 'sample_break_mode': none, 'tokens_per_sample': 1024, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'data_iteration': False, 'data_no_shuffle': True, 'seed': 42, 'batch_size': 1, 'batch_size_valid': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 100000.0, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-10-12 13:14:51 | INFO | fairseq.tasks.language_modeling | dictionary: 51200 types
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
2023-10-12 13:15:04 | INFO | fairseq.tasks.gpt | dictionary: 51200 types
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 0
put done
2023-10-12 13:15:13 | INFO | fairseq_cli.train | TransformerLanguageModelSideNet(
  (decoder): TransformerDecoderSideNet(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 1024, padding_idx=1)
    (layers): ModuleList(
      (0-7): 8 x TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): JointMultiheadAttentionWeightedSum(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9-11): 3 x TransformerDecoderSideNetLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (pretrained_model): NewGPTLanguageModel(
      (decoder): NewGPTDecoder(
        (model): NewGPTForCausalLM(
          (transformer): NewGPTModel(
            (wte): Embedding(51200, 1024)
            (drop): Dropout(p=0.1, inplace=False)
            (h): ModuleList(
              (0-23): 24 x NewGPTBlock(
                (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): NewGPTAttention(
                  (resid_dropout): Dropout(p=0.1, inplace=False)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
                )
                (mlp): NewGPTMLP(
                  (fc_in): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc_out): Linear(in_features=4096, out_features=1024, bias=True)
                  (act): NewGELUActivation()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (ln_f): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (lm_head): Linear(in_features=1024, out_features=51200, bias=True)
        )
      )
    )
    (output_projection): Linear(in_features=1024, out_features=51200, bias=False)
  )
)
2023-10-12 13:15:13 | INFO | fairseq_cli.train | task: LanguageModelingTask
2023-10-12 13:15:13 | INFO | fairseq_cli.train | model: TransformerLanguageModelSideNet
2023-10-12 13:15:13 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2023-10-12 13:15:13 | INFO | fairseq_cli.train | num. shared model params: 558,155,792 (num. trained: 151,083,024)
2023-10-12 13:15:13 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-10-12 13:15:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-10-12 13:15:17 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.pretrained_model.decoder.model.transformer.wte.weight
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.0.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.1.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.2.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.3.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.4.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.5.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.6.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.7.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.8.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.9.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.10.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.layers.11.self_attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.0.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.1.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.2.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.3.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.4.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.5.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.6.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.7.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.8.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.9.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.10.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.11.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.12.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.13.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.14.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.15.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.16.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.17.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.18.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.19.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.20.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.21.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.22.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.q_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.k_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.v_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.pretrained_model.decoder.model.transformer.h.23.attn.out_proj.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.layers.0.self_attn.k_proj.bias <- decoder.output_projection.bias
2023-10-12 13:15:17 | INFO | fairseq.trainer | detected shared parameter: decoder.pretrained_model.decoder.model.lm_head.weight <- decoder.output_projection.weight
2023-10-12 13:15:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 22.188 GB ; name = NVIDIA RTX A5000                        
2023-10-12 13:15:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-10-12 13:15:17 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-10-12 13:15:17 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 1
2023-10-12 13:15:17 | INFO | fairseq.trainer | Preparing to load checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/checkpoint_last.pt
2023-10-12 13:15:17 | INFO | fairseq.trainer | No existing checkpoint found /data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/checkpoint_last.pt
2023-10-12 13:15:17 | INFO | fairseq.trainer | loading train data for epoch 1
2023-10-12 13:18:21 | INFO | fairseq.data.data_utils | loaded 1,681,792,185 examples from: /data/zyu401_data/anirudh/longmem_data/data-bin/longmem/train
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 4
put done
[29290284]
[29290285]
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 2
put done
[14645142]
[14645143]
[0]
[1]
2023-10-12 13:22:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 915322
2023-10-12 13:22:16 | INFO | fairseq.trainer | begin training epoch 1
2023-10-12 13:22:16 | INFO | fairseq_cli.train | Start iterating over samples
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 3
put done
[21967713]
[21967714]
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 7
put done
[51257997]
[51257998]
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 6
put done
[43935426]
[43935427]
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 5
put done
[36612855]
[36612856]
Load Pre-trained GPT from /data/zyu401_data/anirudh/longmem_data/LongMem_public_checkpoints/gpt2_medium/checkpoint_last.pt
NewGPT retrieval Layer Index 17
Reload from pretrained model layer
set up external memory
chunk size 4
put index from cpu to gpu 1
put done
[7322571]
[7322572]
2023-10-12 13:22:21 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:21 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:22 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:22 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:22 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:22 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:25 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:25 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
2023-10-12 13:22:25 | INFO | faiss.loader | Loading faiss with AVX2 support.
2023-10-12 13:22:25 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-10-12 13:25:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-10-12 13:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-10-12 13:25:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-10-12 13:25:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-10-12 13:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-12 13:25:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-10-12 13:28:31 | INFO | train_inner | epoch 001:    106 / 915322 loss=4.055, ppl=16.62, wps=36277.3, ups=0.55, wpb=65536, bsz=64, num_updates=100, lr=0.0001998, gnorm=0.388, loss_scale=2, train_wall=197, gb_free=9.6, wall=793
2023-10-12 13:33:33 | INFO | train_inner | epoch 001:    206 / 915322 loss=3.892, ppl=14.84, wps=21705.9, ups=0.33, wpb=65536, bsz=64, num_updates=200, lr=0.0001996, gnorm=0.255, loss_scale=2, train_wall=302, gb_free=7.2, wall=1095
2023-10-12 13:43:03 | INFO | train_inner | epoch 001:    306 / 915322 loss=3.768, ppl=13.62, wps=11494, ups=0.18, wpb=65536, bsz=64, num_updates=300, lr=0.0001994, gnorm=0.215, loss_scale=4, train_wall=570, gb_free=7.2, wall=1666
2023-10-12 13:52:32 | INFO | train_inner | epoch 001:    406 / 915322 loss=3.826, ppl=14.18, wps=11503, ups=0.18, wpb=65536, bsz=64, num_updates=400, lr=0.0001992, gnorm=0.192, loss_scale=4, train_wall=569, gb_free=7.2, wall=2235
2023-10-12 14:02:02 | INFO | train_inner | epoch 001:    506 / 915322 loss=3.67, ppl=12.73, wps=11516.2, ups=0.18, wpb=65536, bsz=64, num_updates=500, lr=0.000199, gnorm=0.207, loss_scale=4, train_wall=569, gb_free=7.2, wall=2804
2023-10-12 14:11:36 | INFO | train_inner | epoch 001:    606 / 915322 loss=3.697, ppl=12.97, wps=11409.7, ups=0.17, wpb=65536, bsz=64, num_updates=600, lr=0.0001988, gnorm=0.21, loss_scale=8, train_wall=574, gb_free=7.2, wall=3379
2023-10-12 14:17:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-10-12 14:21:13 | INFO | train_inner | epoch 001:    707 / 915322 loss=3.64, ppl=12.47, wps=11361.3, ups=0.17, wpb=65536, bsz=64, num_updates=700, lr=0.0001986, gnorm=0.207, loss_scale=4, train_wall=576, gb_free=7.2, wall=3956
2023-10-12 14:30:45 | INFO | train_inner | epoch 001:    807 / 915322 loss=3.654, ppl=12.59, wps=11444.5, ups=0.17, wpb=65536, bsz=64, num_updates=800, lr=0.0001984, gnorm=0.207, loss_scale=4, train_wall=572, gb_free=7.2, wall=4528
2023-10-12 14:40:16 | INFO | train_inner | epoch 001:    907 / 915322 loss=3.704, ppl=13.03, wps=11482.1, ups=0.18, wpb=65536, bsz=64, num_updates=900, lr=0.0001982, gnorm=0.186, loss_scale=4, train_wall=570, gb_free=7.2, wall=5099
2023-10-12 14:49:45 | INFO | train_inner | epoch 001:   1007 / 915322 loss=3.685, ppl=12.86, wps=11524.5, ups=0.18, wpb=65536, bsz=64, num_updates=1000, lr=0.000198, gnorm=0.172, loss_scale=8, train_wall=568, gb_free=7.2, wall=5668
2023-10-12 14:49:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates
2023-10-12 14:49:45 | INFO | fairseq.trainer | Saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/checkpoint_1_1000.pt
2023-10-12 14:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/checkpoint_1_1000.pt
2023-10-12 14:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /data/zyu401_data/anirudh/longmem_data/train_ckpt/25x/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score None) (writing took 7.185983842995483 seconds)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 4)
Traceback (most recent call last):
  File "/data/zyu401_data/anirudh/longmem/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq_cli/train.py", line 561, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/nethome/zyu401/anirudh/LongMem/fairseq/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/data/zyu401_data/anirudh/longmem/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/data/zyu401_data/anirudh/longmem/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 52 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
